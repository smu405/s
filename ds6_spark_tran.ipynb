{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 데이터 변환\n",
    "\n",
    "* Last updated 202311_202011_201911_201810_201704_201611\n",
    "\n",
    "숫자, 텍스트로 구성된 RDD 또는 DataFrame을 기계학습의 입력으로 사용될 수 있도록 변환한다.\n",
    "\n",
    "* S.3 데이터 변환\n",
    "* S.4 RDD 변환: vectors (Dense, Sparse Vectors), Matrix, svm 입력 형식, TF-IDFm StandardScaler\n",
    "* S.5 DataFrame 변환: Labeled Point, 텍스트 변환, transformer/ estimator, Tokenizer, RegTokenizer, Stopwords\n",
    "    * CountVectorizerm, TF-IDF, Word2Vec, NGram, tringIndexer, 연속데이터의 변환, VectorAssembler, Pipeline\n",
    "    * 연설문을 기계학습하기 위해 변환\n",
    "    * 트윗 정서 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 Jupyter Notebook에서 SparkSession 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]=sys.executable #\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=sys.executable #\"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.3 데이터 변환\n",
    "\n",
    "원천데이터는 그대로는 사용하기 어렵고, 타겟으로 하는 모델의 입력으로 어떻게든 변환되어야 한다.\n",
    "\n",
    "이전의 데이터웨어하우스는 주로 구조화된 데이터를 사용해서 스키마도 정해있고, 사전 정의된 쿼리 및 분석을 수행한다.\n",
    "\n",
    "예를 들어, 금융기관의 경우, 금감원에 정기적으로 제출하는 보고서나 경영층이 필요한 고객군별 매출, 분기별 매출, 이익 등 필요한 쿼리를 사전에 작성하고 일괄 실행해 놓는다 (OLAP: On Line Analytical Processing 온라인 분석처리라고 한다).\n",
    "\n",
    "이를 위해서 **ETL** (Extract, Transform, Load)이라는 작업을 하는데, 소스에서 필요한 데이터를 추출, 변환하고, 다른 타켓으로 로딩하는 것으로 말한다.\n",
    "\n",
    "빅데이터의 분석은 텍스트와 같은 비구조적 데이터를 대상으로 하고 있으며, 앞서 배운 Map Reduce 알고리즘을 적용하고 있다.\n",
    "\n",
    "이를 위해 마찬가지로 ETL이 필요하다. 웹, 파일, 데이터베이스의 원천 데이터에서 읽어서, 분석 가능한 형식으로 **변환**하고 (맵리듀스 Map Reduce) 예측, 분류, 군집화, 추천 등 기계학습 모델에 로딩한다. 예를 들어, 지도학습 Supervised Learning을 하려면, DataFrame은 label, features 컬럼을, **RDD는 label과 features를 가지고 있는 Labeled Point로 구성**해야 한다.\n",
    "\n",
    "이러한 기계학습은 무작정 데이터를 우겨 넣고, 의미있는 결과가 나오기를 기다려서는 작동되지 않는다.\n",
    "입력될 데이터를 정련하고, 가공하고, 일정한 형식을 가지도록 구성하는 것이 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" contentStyleType=\"text/css\" height=\"68px\" preserveAspectRatio=\"none\" style=\"width:386px;height:68px;background:#FFFFFF;\" version=\"1.1\" viewBox=\"0 0 386 68\" width=\"386px\" zoomAndPan=\"magnify\"><defs/><g><!--entity source--><g id=\"elem_source\"><path d=\"M6,16 C6,6 39.3994,6 39.3994,6 C39.3994,6 72.7988,6 72.7988,16 L72.7988,41.2969 C72.7988,51.2969 39.3994,51.2969 39.3994,51.2969 C39.3994,51.2969 6,51.2969 6,41.2969 L6,16 \" fill=\"#F1F1F1\" style=\"stroke:#181818;stroke-width:0.5;\"/><path d=\"M6,16 C6,26 39.3994,26 39.3994,26 C39.3994,26 72.7988,26 72.7988,16 \" fill=\"none\" style=\"stroke:#181818;stroke-width:0.5;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"46.7988\" x=\"16\" y=\"42.9951\">source</text></g><!--entity transform--><g id=\"elem_transform\"><rect fill=\"#F1F1F1\" height=\"36.2969\" rx=\"2.5\" ry=\"2.5\" style=\"stroke:#181818;stroke-width:0.5;\" width=\"88.8789\" x=\"154.96\" y=\"10.5\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"68.8789\" x=\"164.96\" y=\"33.4951\">transform</text></g><!--entity target--><g id=\"elem_target\"><path d=\"M306.99,16 C306.99,6 338.3967,6 338.3967,6 C338.3967,6 369.8035,6 369.8035,16 L369.8035,41.2969 C369.8035,51.2969 338.3967,51.2969 338.3967,51.2969 C338.3967,51.2969 306.99,51.2969 306.99,41.2969 L306.99,16 \" fill=\"#F1F1F1\" style=\"stroke:#181818;stroke-width:0.5;\"/><path d=\"M306.99,16 C306.99,26 338.3967,26 338.3967,26 C338.3967,26 369.8035,26 369.8035,16 \" fill=\"none\" style=\"stroke:#181818;stroke-width:0.5;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacing\" textLength=\"42.8135\" x=\"316.99\" y=\"42.9951\">target</text></g><!--link source to transform--><g id=\"link_source_transform\"><path d=\"M72.94,28.65 C96.67,28.65 122.76,28.65 148.67,28.65 \" fill=\"none\" id=\"source-to-transform\" style=\"stroke:#181818;stroke-width:1.0;\"/><polygon fill=\"#181818\" points=\"154.67,28.65,145.67,24.65,149.67,28.65,145.67,32.65,154.67,28.65\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacing\" textLength=\"46.3442\" x=\"90.88\" y=\"21.7169\">extract</text></g><!--link transform to target--><g id=\"link_transform_target\"><path d=\"M244.15,28.65 C264.46,28.65 282.03,28.65 300.51,28.65 \" fill=\"none\" id=\"transform-to-target\" style=\"stroke:#181818;stroke-width:1.0;\"/><polygon fill=\"#181818\" points=\"306.51,28.65,297.51,24.65,301.51,28.65,297.51,32.65,306.51,28.65\" style=\"stroke:#181818;stroke-width:1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacing\" textLength=\"27.7837\" x=\"261.92\" y=\"21.7169\">load</text></g><!--SRC=[IqaiIKnAB4vLACulBKfEvIfAJIv9p4lFILKeAKhCAqxBByhbWaWfw1PbfcUKwDeXPAmKKYk0dEGIhXIOCIM9HUcf9Lnm9KYwm39M2ZdvYIa0]--></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "database source\n",
    "rectangle transform\n",
    "source -right-> transform: extract\n",
    "database target\n",
    "transform -right-> target: load\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주의: iplantuml을 사용하려면 plantuml.jar 파일을 Jupyter Notebook이 접근할 수 있는 위치에 저장해야 한다. 경로를 지정하려면:\n",
    "\n",
    "```\n",
    "os.environ[\"PLANTUML_JAR\"] = \"/path/to/plantuml.jar\"\n",
    "```\n",
    "\n",
    "또는 %%plantuml --jar /path/to/plantuml.jar 이렇게 경로를 지정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 초기에는 RDD를 사용하여 데이터를 변환하고, mllib 라이브러리를 사용하여 기계학습을 하였다.\n",
    "그 후 DataFrame이 소개되고 나서, ml 라이브러리가 사용되고 있다.\n",
    "Spark 2.0부터 mllib는 유지보수로 지원된다고 한다. 오류가 있으면 수정되지만, 새로운 기능이 추가되지는 않고 있다.\n",
    "반면에 ml 라이브러리는 새로운 기능이 추가되고 있다.\n",
    "\n",
    "따라서 Spark에서는 **```RDD mllib```** , **```DataFrame ml```** 패키지 별로 데이터 타입이나 모델이 제공되므로, 식별하여 사용한다.\n",
    "```ml``` 패키지를 사용할 경우에는 자신의 ```pyspark.ml.linalg.Vector``` 등을 사용해야 한다. ```mllib```도 마찬가지이다.\n",
    "\n",
    "패키지 | 설명 | 데이터 타입 예\n",
    "-------|-------|-----\n",
    "```mllib``` | RDD API를 제공 | ```pyspark.mllib.linalg.Vector``` 또는 ```pyspark.mllib.linalg.Matrix```\n",
    "```ml``` | DataFrame API를 제공 | ```pyspark.ml.linalg.Vector``` 또는 ```pyspark.ml.linalg.Matrix```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.4 RDD 변환\n",
    "\n",
    "기계학습을 하기 위해서는 데이터를 일정 형식으로 만들어 주어야 한다.\n",
    "**```Vector```**, **```Labeled Point```**, **```Matrix```**를 배워보자.\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "```Vector``` | ```numpy vector```와 같은 기능을 한다. **dense**와 **sparse** vector로 구분한다.\n",
    "```Labeled Point``` | 분류를 의미하는 클래스 또는 **label**과 속성 **features** 이 묶인 구조로서, 지도학습 supervised learning을 할 경우 사용된다.\n",
    "```Matrix``` | ```numpy matrix```와 같은 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.1 vectors\n",
    "\n",
    "행렬 **Vector**는 **dense**와 **sparse**로 구분할 수 있다.\n",
    "\n",
    "* 밀집벡터dense vector는 빈 값이 별로 없이 **모든** 행열이 값을 가지고 있다.\n",
    "* 희소벡터sparse vector는 빈 값이 많아서, 값이 있는 경우 그 값이 있는 **인덱스**로 표현해 배열을 축약하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dense Vectors 밀집벡터\n",
    "\n",
    "벡터는 일련의 수로 구성이 되고, 행벡터 또는 열벡터가 될 수 있다. 채워지는 값이 대부분 0이면 희소벡터 Sparse Vectors로 만들어 질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy 라이브러리를 가져오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy array를 사용하면 만들어지는 것이 dense vector이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dv = np.array([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark에서는 Vectors 명령어로 벡터를 만들 수 있다.\n",
    "단 **RDD는 mllib** 모듈을 사용해서 vectors를 만들고, 반면에 **```DataFrame```**은 **ml**를 사용한다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 벡터를 출력해보자. 그 타입도 확인하면 DenseVector이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector: [1.0,2.1,3.0]\n",
      "Type: <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "print (\"Dense vector: {}\\nType: {}\".format(dv1mllib, type(dv1mllib)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이번에는 ```pyspark.ml``` 모듈을 이용하여 Vectors를 생성해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml=Vectors.dense([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml의 dense vector: [1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"ml의 dense vector: {}\".format(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "dense vectors는 numpy array와 같은 특징을 가진다.\n",
    "인덱스로 값을 읽을 수 있다. 또한 반복문에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.1 3.0 "
     ]
    }
   ],
   "source": [
    "for e in dv1mllib:\n",
    "    print (e, end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "보통 벡터와 같이 **product**, **dot**, **norm**과 같은 벡터 연산을 할 수도 있다.\n",
    "결과 값은 numpy와 동일하다.\n",
    "\n",
    "각 요소를 곱하고 더해서 1.0 x 1.0 + 2.1 x 2.1 + 3.0 x 3.0 = 14.41이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(14.41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv1mllib.dot(dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(14.41)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(dv,dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "더하기, 빼기, 곱하기, 나누기 연산도 가능하다. 곱하기 연산을 해보자. dot와 달리 항목별로 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 4.41, 9.0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv1mllib*dv1mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sparse Vectors 희소행렬\n",
    "\n",
    "행렬에는 0 값이 많이 존재하기 때문에, 0값이 아닌 **NZ Nonzero**만 저장하면 훨씬 효율적이다.\n",
    "**sparse**는 실제 **값이 없는 요소, '0'을 제거**하여 만든 vector이다.\n",
    "Spark에서 type field (1 바이트 길이)를 통해 식별한다 (0: sparse, 1: dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 다음은 1차원 dense vector이다.\n",
    "```python\n",
    "[160, 69, 0, 0, 24]\n",
    "```\n",
    "\n",
    "sparse vectors는 값 중에 0이 포함된 경우 이를 생략하고, 값이 있는 요소 Nonzero만 남기게 된다.\n",
    "5는 컬럼 갯수, 0, 1, 4는 값이 있는 컬럼, [160.0, 69.0, 24.0]는 실제 값을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1 = Vectors.sparse(5,[0,1,4],[160.0,69.0,24.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어느 모듈, mllib 또는 ml인지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.SparseVector"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```toArray()``` 함수를 사용하면 sparse에서 dense로 벡터를 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160.,  69.,   0.,   0.,  24.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.2 Matrix\n",
    "\n",
    "로컬 Matrix 역시 밀집 dense, 희소 sparse형식을 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Matrix\n",
    "\n",
    "3행, 2열의 2차원 행렬\n",
    "```\n",
    "1 4\n",
    "2 5\n",
    "3 6\n",
    "```\n",
    "는 ```[1, 2, 3, 4, 5, 6]```로 표현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "Matrices.dense(3, 2, [1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밀집매트릭스를 배열형식으로 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4.],\n",
       "       [2., 5.],\n",
       "       [3., 6.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrices.dense(3, 2, [1,2,3,4,5,6]).toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 열기반 희소행렬\n",
    "\n",
    "밀집행렬(2차원)을 희소행렬로 변환해보자.\n",
    "\n",
    "```python\n",
    "[1 0 2]\n",
    "[0 0 3]\n",
    "[4 5 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy의 sparse vectors로 표현해보자.\n",
    "\n",
    "scipy.sparse.csc_matrix (Compressed Sparse Column matrix)는 SciPy 라이브러리에서 제공하는 희소 행렬(Sparse Matrix)의 한 유형이다.\n",
    "\n",
    "csc는 컬럼별로 읽으면 1,4,5,2,3,6 순서로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(0))\t1\n",
      "  (np.int32(2), np.int32(0))\t4\n",
      "  (np.int32(2), np.int32(1))\t5\n",
      "  (np.int32(0), np.int32(2))\t2\n",
      "  (np.int32(1), np.int32(2))\t3\n",
      "  (np.int32(2), np.int32(2))\t6\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "sparse_csc = csc_matrix([[1, 0, 2],\n",
    "                         [0, 0, 3],\n",
    "                         [4, 5, 6]])\n",
    "print(sparse_csc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csr_matrix는 row별로 읽고 1,2,3,4,5,6 순으로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(0))\t1\n",
      "  (np.int32(0), np.int32(2))\t2\n",
      "  (np.int32(1), np.int32(2))\t3\n",
      "  (np.int32(2), np.int32(0))\t4\n",
      "  (np.int32(2), np.int32(1))\t5\n",
      "  (np.int32(2), np.int32(2))\t6\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "sparse_csr = csr_matrix([[1, 0, 2],\n",
    "                         [0, 0, 3],\n",
    "                         [4, 5, 6]])\n",
    "print(sparse_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "출력설명: 위와 같은 2차원 dense vectors를 sparse vectors의 배열 방식으로 표현한다.\n",
    "우선 다음과 같이\n",
    "행, 열, 값 vector를 만든다.\n",
    "\n",
    "```python\n",
    "행 | 0 | 0 | 1 | 2 | 2 | 2\n",
    "열 | 0 | 2 | 2 | 0 | 1 | 2\n",
    "값 | 1 | 2 | 3 | 4 | 5 | 6\n",
    "```\n",
    "\n",
    "행을 보면 0번째에 '1','2' 1번째에 '3', 2번째에 '4','5','6'이므로 **0,0,1,2,2,2**\n",
    "열을 보면 0번째에 '1', 2번째 '2','3', 0번째 '4', 1번째 '5', 2번째 '6'이므로 **0,2,2,0,1,2**\n",
    "\n",
    "**행, 열, 데이터를 한 쌍**으로 읽으면 된다.\n",
    "즉 행 0, 열 0의 위치에 1, 행 0, 열 2의 위치에 2. 이런 식으로 6개의 데이터가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2차원 dense vectors를 만들어 보자.\n",
    "행의 갯수, 열의 갯수, 실제 값을 넣어주면 생성된다.\n",
    "* 6은 행 갯수\n",
    "* 4는 열 갯수\n",
    "* 다음 수는 행렬을 해체하여 연속적인 수로 나열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "dm = Matrices.dense(6,4,[1, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 5, 6, 7, 0, 0, 0, 0, 0, 0, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위를 2차원 배열로 변환해자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [2., 3., 0., 0.],\n",
       "       [0., 0., 5., 0.],\n",
       "       [0., 4., 6., 0.],\n",
       "       [0., 0., 7., 0.],\n",
       "       [0., 0., 0., 8.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(6, 4, [0, 2, 4, 7, 8], [0, 1, 1, 3, 2, 3, 4, 5], [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toSparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 설명: 밀집행렬(아래)를 희소행렬로 변환한다.\n",
    "\n",
    "```python\n",
    "[ 1.,  0.,  0.,  0.]\n",
    "[ 2.,  3.,  0.,  0.]\n",
    "[ 0.,  0.,  5.,  0.]\n",
    "[ 0.,  4.,  6.,  0.]\n",
    "[ 0.,  0.,  7.,  0.]\n",
    "[ 0.,  0.,  0.,  8.]\n",
    "```\n",
    "\n",
    "위 SparseMatrix(인자 6개)를 출력한다.\n",
    "\n",
    "* 6은 행 갯수\n",
    "* 4는 열 갯수\n",
    "* 다음은 열포인터 IA [0, 2, 4, 7, 8]\n",
    "    * **열**로 세어서 --->  **요소의 개수** 2, 2, 3, 1을 가지고 구성을 한다. 즉 열로 요소가 2, 2, 3, 1개 이다.\n",
    "    * **0**:IA[0]=0개로 시작, **2**:IA[1]=IA[0]+2개, **4**:IA[2]=IA[1]+2개 **7**:IA[3]=IA[2]+3개, **8**:IA[4]=IA[3]+1개\n",
    "* 다음은 행인덱스 JA [0, 1, 1, 3, 2, 3, 4, 5]\n",
    "    * **행**으로 세어서 (맨 뒤 실제 값 리스트의 순서를 지켜서) ---> 0 1 1 3 2 3 4 5\n",
    "    * 요소값 1은 **0**행에 있고, 나머지 요소값들 2:**1**행, 3:**1**행, 4:**3**행, 5:**2**행, 6:**3**행, 7:**4**행, 8:**5**행에 있다.\n",
    "* 다음은 소수로 실제 값 리스트 [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
    "* 마지막: 행의 개수를 포함하는 여부 (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d=sm.toDense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[9., 0.],\n",
      "             [0., 8.],\n",
      "             [0., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 아래는 행3, 열2개 행열로 [9., 0.], [0., 8.], [0., 6.] 값이다.\n",
    "* 0,1,3: **0**:IA[0]=0으로 시작, **1**:IA[1]=IA[0]+1 (1열 개수: 9뿐이 없으므르 1), **3**:IA[2]=IA[1]+2 (2열 개수: 6,8이 있으므로 2)\n",
    "* 0,2,1: 9:**0**행, 6:**2**행, 8:**1**행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.2 분산 Matrix\n",
    "\n",
    "배열은 n차원을 가질 수 있고, 2차원인 경우에는 매트릭스라고 지칭한다.\n",
    "매트릭스 역시 로컬과 분산으로 구분할 수 있다.\n",
    "앞서 로컬 매트릭스는 라이브러리 명에서 눈치를 챌 수 있는데, ```pyspark.mllib.linalg.Matrix, Matrices```를 사용한다. 반면에 분산 매트릭스는 당연히 여러 노드에 분산해서 사용할 수 있고, ```pyspark.mllib.linalg.distributed```에 존재하는 Row Matrix, Indexed Row Matrix, Coordinate Matrix, Block Matrix를 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row Matrix\n",
    "\n",
    "RowMatrix는 ```pyspark.mllib.linalg.distributed```에서 제공되는 분산벡터로서, RDD vectors로부터 생성된다.\n",
    "우선 리스트에서 RDD를 생성하고 이를 RowMatrix에 넘겨주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = [[1.0,2.0,3.0],[1.1,2.1,3.1],[1.2,2.2,3.3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my=spark.sparkContext.parallelize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [1.1, 2.1, 3.1], [1.2, 2.2, 3.3]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD Vectors를 넘겨주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rm=RowMatrix(my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.distributed.RowMatrix'>\n"
     ]
    }
   ],
   "source": [
    "print (type(rm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([1.1, 2.1, 3.1]),\n",
       " DenseVector([1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.numCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexed Row Matrix\n",
    "\n",
    "앞서 Row Matrix과 유사하지만, 파티션으로 나누어, 그러나 순서를 지켜서 저장이 된다.\n",
    "따라서 시계열 데이터와 같이 순서가 있는 데이터를 저장하기에 적합하다.\n",
    "\n",
    "순서인덱스와 벡터로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "\n",
    "irRdd = spark.sparkContext.parallelize([\n",
    "    IndexedRow(1, [3, 1, 2]),\n",
    "    IndexedRow(2, [1, 3, 2]),\n",
    "    IndexedRow(3, [5, 4, 3]),\n",
    "    IndexedRow(4, [6, 7, 4]),\n",
    "    IndexedRow(5, [8, 9, 2]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "\n",
    "irm = IndexedRowMatrix(irRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(irm.numRows())\n",
    "print(irm.numCols())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 s-1: matrix에서 데이터프레임 생성\n",
    "\n",
    "매트릭스에서 데이터프레임을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "my = Matrices.dense(3, 2, [1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밀집 매트릭스에서 바로 만들게 되면 오류가 발생한다.\n",
    "또는 아래와 같이 toArray()한 후, 인자로 사용해도 역시 오류가 발생한다.\n",
    "그 이유는 ```<class 'numpy.ndarray'>```에서 직접 만들 수 없기 때문이다.\n",
    "따라서 tolist()로 변환한 후 데이터프레임을 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = Matrices.dense(3, 2, [1,2,3,4,5,6]).toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(my, ['c1','c2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|1.0|4.0|\n",
      "|2.0|5.0|\n",
      "|3.0|6.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.4 Labeled Point\n",
    "\n",
    "Labeled point는 로컬벡터로 레이블을 가지고 있는 밀집 또는 희소 행렬을 말한다.\n",
    "레이블이 있으므로, supervised learning에 요구되는 형식이다.\n",
    "레이블은 double형식으로 저장되어야 한다. 분류에 사용되려면 예를 들어 긍정, 부정인 경우 정수 1, 0으로 하지 않고 double 형식으로 저장되어야 한다.\n",
    "\n",
    "#### label, features로 구성\n",
    "\n",
    "**분류** 및 **회귀분석**에 사용되는 데이터 타잎이다.\n",
    "**'label'**과 **'features'**로 구성된다.\n",
    "\n",
    "구분 | 지도학습을 하기 위한 label과 features의 구성\n",
    "-----|-----\n",
    "label | supervised learning에서 '구분 값'으로 사용한다. 데이터타입은 'DoubleType'으로 설정되어야 한다.\n",
    "features | **sparse**, **dense** 모두 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "label 1.0, features [1.0, 2.0, 3.0]으로 LabeledPoint를 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "sparse vectors로 features를 구성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1992.0, (10,[0,1,2],[3.0,5.5,10.0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "서로 다른 패키지의 데이터타잎 **```mllib LabeledPoint```**와 **```ml Vectors```**를 혼용하면, 형변환 오류가 발생한다.\n",
    "이러한 오류는 패키지를 혼용하지 않으면 된다.\n",
    "\n",
    "```python\n",
    "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "```\n",
    "\n",
    "**```dv1mllib```**은 앞서 **```mllib```**로부터 생성된 dense vector이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**```dv1ml```**은 앞서 **```ml```**로부터 생성된 dense vector이다.\n",
    "```mllib```에서 사용하려면, **```Vectors.fromML()```**를 사용해 ```ml```의 Vectors를 읽어서 ```mllib```로 변환하여 혼용을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-1: RDD 데이터를 LabeledPoint로 변환하기\n",
    "\n",
    "### 문제\n",
    "\n",
    "머신러닝은 사람이 경험을 통해 배우는 것과 비슷하게 **과거 데이터로부터 학습**을 한다.\n",
    "학습이란 어렵게 생각할 필요 없이, 과거 데이터에서 수학적이나 알고리즘을 활용하여 어떤 패턴을 찾아내는 것이다.\n",
    "spark에서 제공한 **데이터 파일 ```data/mllib/sample_svm_data.txt```을 읽어서 훈련데이터**를 만들어 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "데이터를 읽어 보면, 맨 처음 값은 label에 해당하고, 다음은 일련의 수로 구성된다. 이로부터 **RDD**를 생성하고, ```label```, ```features```를 구성하여 ```Labeled Point```로 만든다.\n",
    "\n",
    "```python\n",
    "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python으로 파일 읽기\n",
    "\n",
    "Spark를 다운로드하고 압축을 풀어 설치한 경우, ```%SPARK_HOME%\\data\\mllib\\sample_svm_data.txt``` 파일을 읽도록 한다.\n",
    "\n",
    "다운로드 받지 않았다면, 아파치 github을 방문해서, https://github.com/apache/spark/ 아래 data/mllib 폴더로 가보면 해당 파일을 찾을 수 있다.\n",
    "\n",
    "입출력은 ```try except``` 구문으로 오류에 대비할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-3.0.0-bin-hadoop2.7')\n",
    "_fsvm=os.path.join(os.getcwd(),'data','sample_svm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#_lines=list()\n",
    "try:\n",
    "    _f=open(_fsvm,'r')\n",
    "    _lines=_f.readlines()\n",
    "    _lines[0]\n",
    "    _f.close()\n",
    "except:\n",
    "    print(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "파일로부터 데이터를 **```readlines()```** 함수로 모두 읽어 온다.\n",
    "첫 행을 읽으면 label, features로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark에서 RDD 생성\n",
    "\n",
    "Spark는 파일을 Python을 통하지 않고, 직접 읽을 수 있다.\n",
    "원본 데이터 ```sample_svm_data.txt```는 공백으로 구분되어 있다.\n",
    "읽을 대상이 파일이므로, RDD를 사용한다. 각 행을 공백으로 분리하여 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fsvm)\\\n",
    "    .map(lambda line: [float(x) for x in line.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "각 행으로 분리되므로 2차원 리스트가 생성이 된다. 첫째 행을 읽으려면 인덱스를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 2.52078447201548,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.004684436494304,\n",
       " 2.000347299268466,\n",
       " 0.0,\n",
       " 2.228387042742021,\n",
       " 2.228387042742023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LabeledPoint 생성\n",
    "\n",
    "위 데이터에서 보듯이 첫 열은 **label**로, 그 나머지는 **features**로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "_trainRdd0=_rdd.map(lambda line:LabeledPoint(line[0], line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd0.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "공백을 분리하고, 분리된 데이터를 labeled point로 구성하는 기능을 합쳐서 실행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainRdd=spark.sparkContext.textFile(_fsvm)\\\n",
    "    .map(lambda line: [float(x) for x in line.split()])\\\n",
    "    .map(lambda p:LabeledPoint(p[0], p[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 정리하면\n",
    "\n",
    "데이터를 변환하는 과정을 함수로 만들었다.\n",
    "```createLP(line)```는 행 데이터를 받아서 LabeledPoint로 생성하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447201548,0.0,0.0,0.0,2.004684436494304,2.000347299268466,0.0,2.228387042742021,2.228387042742023,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createLP(line):\n",
    "    p = [float(x) for x in line.split()]\n",
    "    return LabeledPoint(p[0], p[1:])\n",
    "\n",
    "_rdd=spark.sparkContext.textFile(_fsvm)\n",
    "trainRdd = _rdd.map(createLP)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.6 TF\n",
    "\n",
    "지금까지는 정량데이터를 다루었지만, **텍스트**를 변환해보자.\n",
    "TF (Term Frequency)\n",
    "단어빈도를 계산하기 위해 HashingTF를 사용할 수 있다.\n",
    "단어ID로 Hash 알고리즘에 따라 무작위 번호를 생성하고, 단어빈도를 생성한다.\n",
    "Hash를 사용하지 않고 계산한 단어빈도는 당연히 동일하다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikiRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD는 mllib 라이브러리를 사용한다. 여기의 HashingTF를 사용한다.\n",
    "transform() 함수를 사용하여 RDD를 단어빈도 구조로 변환한다.\n",
    "단 **fit()은 하지 않아도** 된다는 점에 주의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.0}),\n",
       " SparseVector(1048576, {148618: 1.0, 183975: 1.0, 216207: 1.0, 261052: 1.0, 617454: 1.0, 696349: 1.0, 721336: 1.0, 816618: 1.0, 897662: 1.0}),\n",
       " SparseVector(1048576, {60386: 1.0, 177421: 1.0, 568609: 1.0, 569458: 1.0, 847171: 1.0, 850510: 1.0, 1040679: 1.0}),\n",
       " SparseVector(1048576, {261052: 4.0, 816618: 4.0}),\n",
       " SparseVector(1048576, {60386: 4.0, 594754: 4.0}),\n",
       " SparseVector(1048576, {21980: 1.0, 70882: 1.0, 274690: 1.0, 357784: 1.0, 549790: 1.0, 597434: 1.0, 804583: 1.0, 829803: 1.0, 935701: 1.0}),\n",
       " SparseVector(1048576, {154253: 1.0, 261052: 1.0, 438276: 1.0, 460085: 1.0, 585459: 1.0, 664288: 1.0, 816618: 1.0, 935701: 2.0, 948143: 1.0, 1017889: 1.0}),\n",
       " SparseVector(1048576, {270017: 1.0, 472985: 1.0, 511771: 1.0, 718483: 1.0, 820917: 1.0}),\n",
       " SparseVector(1048576, {34116: 1.0, 87407: 1.0, 276491: 1.0, 348943: 1.0, 482882: 1.0, 549350: 1.0, 721336: 1.0, 816618: 1.0, 1025622: 1.0}),\n",
       " SparseVector(1048576, {1769: 1.0, 151357: 1.0, 500659: 1.0, 547760: 1.0, 979482: 1.0})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(wikiRdd3)\n",
    "tf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.7 TF-IDF\n",
    "\n",
    "IDF는 전체에서 몇 개의 문서에 씌였는지를 반대로 계산한 값이다.\n",
    "뒤 DataFrame를 사용하여 TF-IDF를 계산하면서 자세히 설명하기로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {1026674: 1.7047}),\n",
       " SparseVector(1048576, {148618: 1.7047, 183975: 1.7047, 216207: 1.7047, 261052: 1.0116, 617454: 1.7047, 696349: 1.7047, 721336: 1.2993, 816618: 0.7885, 897662: 1.7047}),\n",
       " SparseVector(1048576, {60386: 1.2993, 177421: 1.7047, 568609: 1.7047, 569458: 1.7047, 847171: 1.7047, 850510: 1.7047, 1040679: 1.7047}),\n",
       " SparseVector(1048576, {261052: 4.0464, 816618: 3.1538}),\n",
       " SparseVector(1048576, {60386: 5.1971, 594754: 6.819}),\n",
       " SparseVector(1048576, {21980: 1.7047, 70882: 1.7047, 274690: 1.7047, 357784: 1.7047, 549790: 1.7047, 597434: 1.7047, 804583: 1.7047, 829803: 1.7047, 935701: 1.2993}),\n",
       " SparseVector(1048576, {154253: 1.7047, 261052: 1.0116, 438276: 1.7047, 460085: 1.7047, 585459: 1.7047, 664288: 1.7047, 816618: 0.7885, 935701: 2.5986, 948143: 1.7047, 1017889: 1.7047}),\n",
       " SparseVector(1048576, {270017: 1.7047, 472985: 1.7047, 511771: 1.7047, 718483: 1.7047, 820917: 1.7047}),\n",
       " SparseVector(1048576, {34116: 1.7047, 87407: 1.7047, 276491: 1.7047, 348943: 1.7047, 482882: 1.7047, 549350: 1.7047, 721336: 1.2993, 816618: 0.7885, 1025622: 1.7047}),\n",
       " SparseVector(1048576, {1769: 1.7047, 151357: 1.7047, 500659: 1.7047, 547760: 1.7047, 979482: 1.7047})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.8 StandardScaler\n",
    "\n",
    "데이터를 표준화하려면 1) 평균과 표준편차를 계산하고, 2) 측정값에서 평균을 빼고, 표준편차로 나누어 주면 된다.\n",
    "즉 zscore를 계산하는 것과 같다.\n",
    "\n",
    "$$ z = \\frac {\\bar{x_n} - \\mu} {\\sigma / \\sqrt{n}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화 할 값만 추출\n",
    "\n",
    "탭을 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '65.78', '112.99']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd.map(lambda x: x.split('\\t')).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형변환을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 65.78, 112.99]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd.map(lambda x: x.split('\\t')).map(lambda x: [str(x[0]), float(x[1]), float(x[2])]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 65.78, 112.99]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRdd.map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개의 값만 추출하여 dense vectors에 별도로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "_tRdd =tRdd\\\n",
    "    .map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .map(lambda x: Vectors.dense([x[1], x[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리스트로 저장해도 계산에 문제가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "_tRdd =tRdd\\\n",
    "    .map(lambda x: x.split('\\t'))\\\n",
    "    .map(lambda x: [str(x[0]), float(x[1]), float(x[2])])\\\n",
    "    .map(lambda x: [x[1], x[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "scaler1 = StandardScaler().fit(_tRdd)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(_tRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 계산한 zscore와 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.2458, -1.2299]),\n",
       " DenseVector([1.9011, 0.5934]),\n",
       " DenseVector([0.7388, 1.8767]),\n",
       " DenseVector([0.0919, 1.0473]),\n",
       " DenseVector([-0.1439, 1.1993])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler2.transform(_tRdd).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 DataFrame 변환\n",
    "\n",
    "DataFrame으로 만들어진 데이터를 변환해보자.\n",
    "이러한 작업이 필요한 이유는 **기계학습에 넘겨줄 입력데이터를 형식에 맞추어야** 하기 때문이다.\n",
    "데이터는 형식에 맞게 변환되고, 군집화, 회귀분석, 분류, 추천 모델 등에 입력으로 사용된다\n",
    "물론 데이터는 '일련의 수' 또는 '텍스트'로 구성된다.\n",
    "이런 데이터로부터 특징을 추출하여 **feature vectors**를 구성한다.\n",
    "지도학습을 하는 경우에는 **class 또는 label** 값이 필요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.1 Labeled Point를 label, features 컬럼으로 분해\n",
    "\n",
    "RDD LabeledPoint는 label과 vectors로 구성되어 있다.\n",
    "따라서 LabeledPoint를 DataFrame으로 읽어오면, 2개의 컬럼으로 생성된다.\n",
    "이를 label, features 컬럼으로 맞추어 주도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 레이블이 있는 Python List에서 DataFrame 생성\n",
    "\n",
    "label과 features를 가지고 가지고 있는 데이터를 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = [[1, [1.0, 2.0, 3.0]], [1, [1.1, 2.1, 3.1]], [0, [1.2, 2.2, 3.3]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 데이터를 분해해서 출력하면, label과 features를 하나씩 묶어 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n",
      "features: [1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"label: {}\\nfeatures: {}\".format(p[0][0], p[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 데이터를 읽어서 DataFrame을 생성하면, 두 개의 컬럼으로 구분해서 생성된다.\n",
    "그러나 컬럼이 자동 명명되기 때문에 ```_1, _2```가 쓰여서 만족스럽지 못하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### LabeledPoint에서 DataFrame 생성\n",
    "\n",
    "Python List를 LabeledPoint로 만들어 보자.\n",
    "**LabeledPoint는 RDD에서 사용하는 구조로서 mllib 라이브러리를 사용**해서 만들고 있다.\n",
    "**DataFrame은 LabeledPoint를 컬럼으로 가지고 있지 않는다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1, [1.0,2.0,3.0]),\n",
    "     LabeledPoint(1, [1.1,2.1,3.1]),\n",
    "     LabeledPoint(0, [1.2,2.2,3.3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 DataFrame을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "그러면 LabeledPoint는 분해되어, **label과 features를 별도 컬럼**으로 생성된다. 이런 명칭의 컬럼은 기계학습에 필요하다. **features 데이터를 모델링하여 label에 따라 분류**하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### mllib.linalg.Vectors를 사용하여 DataFrame을 생성\n",
    "\n",
    "앞서 mllib와 ml 모듈을 섞어서 사용하지 않아야 한다고 했다.\n",
    "mllib vectors를 사용해도 DataFrame을 생성하는데 문제가 없다.\n",
    "컬럼명을 ```[\"label\", \"features\"]```으로 하지 않으면, 자동명명되니 주의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD에서 DataFrame 생성\n",
    "\n",
    "rdd에서 DataFrame을 생성하면 labe, features이 당연히 생성이 되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.linalg import SparseVector # mllib ok\n",
    "from pyspark.ml.linalg import SparseVector # ml ok\n",
    "\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_df=_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: double (nullable = true)\n",
      " |-- _2: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_df.withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.2 단어 빈도\n",
    "\n",
    "정량 데이터는 합계, 평균, 표준편차 등 의미있는 통계량을 계산하거나, 이런 통계량이 집단 간에 차이가 있는지 분석한다.\n",
    "반면에 텍스트는 정량데이터와 같이 이러한 통계량의 계산이 가능하지 못하게 된다.\n",
    "텍스트에 어떤 단어가 얼마나 쓰였는지, 또한 같이 쓰이게 된 단어는 무엇인지 등 단어의 빈도에 따라 **정량화하여 과학적인 분석**을 하게 된다.\n",
    "\n",
    "#### Bag of Words 모델\n",
    "\n",
    "자연어처리 NLP에서 사용하는 모델로, 텍스트를 단어의 집합, **'bag of words'**으로 구성된다고 보며, 단어의 **순서**는 의미를 가지지 않는다.\n",
    "이런 영화리뷰가 있다고 하자.\n",
    "> \"...그 영화는 매우 강렬했다. 그냥 좋았다. 영화관에서 보는 동안 긴장을 늦출 수 없었다. 갑돌이가 분장한 악당의 케릭터가 만들어지는 과정은 흥미롭지 않을 수가 없었다. 무비의 이야기 전개는 빠르고, 무엇이 진실이고 거짓인지 판단할 수 없었다. 누가 이런 영화를 좋아 하지 않을 수가 있겠는가 이모티콘...\"\n",
    "\n",
    "이를 단어로 분리하고, 정량화 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 텍스트 변환 단계\n",
    "\n",
    "텍스트를 변환하는 단계를 보자. 순서는 변경될 수 있다.\n",
    "\n",
    "* 단계 1: 단어로 분할 Tokenization\n",
    "    * 그, 영화는, 매우, 강렬했다, 그냥, 좋았다, 영화관에서, 보는, 동안, 긴장을, 늦출, 수, 없었다, 갑돌이가, 분장한, 악당의, 케릭터가, 만들어지는, 과정은, 흥미롭지, 않을, 수가, 없었다, 무비의, 이야기, 전개는, 빠르고, 무엇이, 진실이고, 거짓인지, 판단할, 수, 없었다, 누가, 이런, 영화를, 좋아, 하지, 않을, 수가, 있겠는가, 이모티콘\n",
    "\n",
    "* 단계 2: 정리\n",
    "    - 불필요, 오타 등\n",
    "\n",
    "* 단계 3: 불용어 stopwords 제거\n",
    "    - 그, 수, 수가, 수, 이런, 하지, 수가 등\n",
    "\n",
    "* 단계 4: 어간 추출 stemming\n",
    "    영화는, 영화의는 다른 단어지만 조사를 제거하면 동일한 단어\n",
    "    - 좋았다, 좋아 단어들은 어근을 판별하면 동일한 단어이다.\n",
    "    - 영화, 무비의 단어는 이음동의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "단계 5: 계량화\n",
    "- word vector로 만든다.\n",
    "- 있다-없다, 단어빈도, TF-IDF 사용할 수 있다.<br>dense, sparse 모두 가능하다.\n",
    "```[1,1,1,1,1,0,0],[0,1,0,1,1,1,1]```\n",
    "\n",
    "```python\n",
    "[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
    "[0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
    "[1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Python을 사용한 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let it be lyrics\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문서, 문장, 단어의 계층을 먼저 이해해야 한다.\n",
    "문서는 문장으로 구성되어 있고, 문장은 단어로 구성되어 있다.\n",
    "따라서 첫째 반복문은 문서의 각 문장에 대해, 단어로 분리하고 있다.\n",
    "그 다음 반복문은 각 단어에 대해 빈도를 계산한다.\n",
    "각 단어가 키가 되는데, **키가 존재하면 빈도를 증가하고, 존재하지 않으면 새로운 키를 생성**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 단어 빈도는 dictionary d에 저장하였다.\n",
    "dictionary는 키, 빈도의 쌍으로 저장되어 있어서 ```iteritems()```으로 읽어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\t1\n",
      "I\t1\n",
      "find\t1\n",
      "myself\t1\n",
      "in\t3\n",
      "times\t1\n",
      "of\t6\n",
      "trouble\t1\n",
      "Mother\t1\n",
      "Mary\t1\n",
      "comes\t1\n",
      "to\t1\n",
      "me\t2\n",
      "Speaking\t2\n",
      "words\t3\n",
      "wisdom,\t3\n",
      "let\t3\n",
      "it\t7\n",
      "be\t7\n",
      "And\t1\n",
      "my\t1\n",
      "hour\t1\n",
      "darkness\t1\n",
      "She\t1\n",
      "is\t1\n",
      "standing\t1\n",
      "right\t1\n",
      "front\t1\n",
      "Let\t4\n",
      "Whisper\t1\n"
     ]
    }
   ],
   "source": [
    "# for k,v in d.iteritems():  # python2\n",
    "for k,v in d.items():\n",
    "    print (\"{}\\t{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.3 Spark의 transformer, estimator\n",
    "\n",
    "**RDD**를 만들고 나서도 데이터를 변환하기 위해 map-reduce와 같은 함수 또는  **transform()**, **fit()**을 사용하는 것과 같이,\n",
    "**DataFrame**도 역시 **Transformer**, **Estimator**를 사용할 수 있다.\n",
    "이러한 Spark ml 라이브러리는 Python의 scikit-learn에서 영향을 받아 기계학습 API transformer, estimator, evaluator가 유사하다.\n",
    "\n",
    "* (1) **Estimator**는 fit()함수를 제공하는 객체이다. **```Estimator.fit()```**함수는 DataFrame에 적용되는 **알고리즘**을 적용하여, 모델인 Transformer를 생성해낸다.\n",
    "* (2) **Transformer**는 transform() 함수를 통해 위 모델을 적용하여 데이터를 변환하여 DataFrame을 생성한다.\n",
    "* (3) **Evaluator**는 모델의 정확성을 측정한다. ```pyspark.ml.evaluation```의 'BinaryClassificationEvaluator', 'RegressionEvaluator',  'MulticlassClassificationEvaluator', 'MultilabelClassificationEvaluator', 'ClusteringEvaluator', 'RankingEvaluator' 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행의 집합으로 묶인 DataFrame으로 만들기 위해서, 먼저 텍스트를 2차원 배열로 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 DataFrame을 생성한다. schema는 만들어 주지 않고 컬럼명을 sent로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(doc2d, ['sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```truncate=True```는 줄여서, ```False```는 출력을 줄이지 않고 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|When I find mysel...|\n",
      "|Mother Mary comes...|\n",
      "|Speaking words of...|\n",
      "|And in my hour of...|\n",
      "|She is standing r...|\n",
      "|Speaking words of...|\n",
      "|      우리 Let it be|\n",
      "|        나 Let it be|\n",
      "|        너 Let it be|\n",
      "|           Let it be|\n",
      "|Whisper words of ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.4 Tokenizer\n",
    "\n",
    "먼저 용어를 정리해 보자.\n",
    "* corpus는 어떤 주제에 대해 쓰여지거나, 어떤 사람이 작성한 전체 '말뭉치'를 말한다. 여러 문장으로 구성된 텍스트 집합을 말한다.\n",
    "* document는 문장으로 구성된 문서를 말하지만, 한 문장으로만 구성될 수도, 여러 문장으로 만들어질 수도 있다. 예를 들어, \"why she had to go\" 같은 한 문장도 document라고 하고, \"why she had to go.. I don't know\" 역시 마찬가지이다.\n",
    "* vocabularay는 중복없는 단어 집합을 말하며, 예를 들면, \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\" 등은 단어이다.\n",
    "\n",
    "Tokenizer는 document를 단어로 분리한다.\n",
    "분리하는 기준은 whitespace로 공백, TAB, CR, New Line 등이 해당된다.\n",
    "* 입력 컬럼은 \"sent\"로,\n",
    "* 출력 컬럼은 \"words\"로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "'Tokenizer'는 ```fit```함수를 제공하지 않는다, 바로 transform하자.\n",
    "```transform()```은 앞서 만든 ```tokenizer```모델에 DataFrame을 변환하여 다른 DataFrame을 생성한다.\n",
    "그 결과는 문자열 배열로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```for```문으로 출력해보자. ```Row()``` 객체로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent='When I find myself in times of trouble', words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'])\n",
      "Row(sent='Mother Mary comes to me', words=['mother', 'mary', 'comes', 'to', 'me'])\n",
      "Row(sent='Speaking words of wisdom, let it be', words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'])\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.5 RegTokenizer\n",
    "\n",
    "Tokenizer는 white space로 분리하지만, RegexTokenizer는 단어를 분리하기 위해 **정규표현식**을 적용할 수 있다.\n",
    "정규표현식을 사용하여 분리하거나 특정 패턴을 추출할 수 있다.\n",
    "공백으로 분리할 경우 간단히 정규표현식 ```\\s``` 패턴을 적용할 수 있다.\n",
    "한글에는 ```\\w``` 패턴이 적용되지 않는다.\n",
    "* ```\\s```는 공백문자\n",
    "* ```\\w```는 숫자 및 대소문자 ```[A-Za-z0-9_]```\n",
    "* 별표 ```*```는 0 또는 그 이상, 더하기 ```+```는 1 또는 그 이상을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.6 Stopwords\n",
    "\n",
    "텍스트를 분리하고 나면, 별 의미가 없거나 쓸모가 없는 단어들이 존재한다.\n",
    "예를 들어 이, 그, 저와 같은 **한 단어** 또는 있다 등과 같은 **일부 동사**, 그래서, 그러나 등과 같은 **접속사** 등이 후보가 될 수 있다.\n",
    "이런 불필요한 단어들을 불용어 Stopwords라고 하며, 입력데이터에서 제거하도록 한다.\n",
    "영어의 경우 불용어가 식별되어 제공되고 있다\n",
    "http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "현재 stop words를 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자신의 불용어를 추가해서, 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_2522647aade3"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자신이 추가한 불용어가 있는지 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i/me/my/myself/we/our/ours/ourselves/you/your/yours/yourself/yourselves/he/him/his/himself/she/her/hers/herself/it/its/itself/they/them/their/theirs/themselves/what/which/who/whom/this/that/these/those/am/is/are/was/were/be/been/being/have/has/had/having/do/does/did/doing/a/an/the/and/but/if/or/because/as/until/while/of/at/by/for/with/about/against/between/into/through/during/before/after/above/below/to/from/up/down/in/out/on/off/over/under/again/further/then/once/here/there/when/where/why/how/all/any/both/each/few/more/most/other/some/such/no/nor/not/only/own/same/so/than/too/very/s/t/can/will/just/don/should/now/i'll/you'll/he'll/she'll/we'll/they'll/i'd/you'd/he'd/she'd/we'd/they'd/i'm/you're/he's/she's/it's/we're/they're/i've/we've/you've/they've/isn't/aren't/wasn't/weren't/haven't/hasn't/hadn't/don't/doesn't/didn't/won't/wouldn't/shan't/shouldn't/mustn't/can't/couldn't/cannot/could/here's/how's/let's/ought/that's/there's/what's/when's/where's/who's/why's/would/나/너/우리/"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e, end=\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "transformer로 불용어를 제거해보자.\n",
    "한글의 stop words '너','우리'가 제거되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.7 CountVectorizer\n",
    "\n",
    "```CountVectorizer```는 단어의 빈도 수를 계산한다.\n",
    "\n",
    "3번째 문장 \"Speaking words of wisdom, let it be\"의 word vector를 구성해 본다.\n",
    "id 값은 모든 문장에서 단어를 추출하고 나서야 부여된다.\n",
    "\n",
    "단어 (3행 \"Speaking words of wisdom, let it be\") | id | 빈도 | \n",
    "-----|-----|-----\n",
    "Speaking | 7 | 1\n",
    "words | 13 | 1\n",
    "of | stopword | 0\n",
    "wisdom | 12 | 1\n",
    "let | 3 | 1\n",
    "it | stopword | 0\n",
    "be | stopword | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 **word vector**를 표로 나타내면 아래와 같다.\n",
    "행은 문장, 열은 id이다.\n",
    "**3행은 doc2**이다. 해당하는 **단어 id의 빈도**를 적었다. 다른 행과 열은 이해를 돕기 위해 비워 놓았다.\n",
    "\n",
    "```doc``` \\ 단어 id  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |12 |13 |...\n",
    "------|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "```doc 0``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 1``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 2``` |   |   | 1 |   |   |   | 1 |   |   |   |   | 1 | 1 |...\n",
    "...   |   |   |   |   |   |   |   |   |   |   |   |   |   |..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sklearn CountVectorizer\n",
    "\n",
    "sklearn 라이브러리로 단어빈도를 세어보자.\n",
    "입력으로 단어의 집합을 넣어야 하지만, 위 문서는 2차원 리스트이다.\n",
    "\n",
    "Python의 functools 모듈에 포함된 reduce함수를 사용하여, 먼저 ```[ [ ], [ ], ..., [ ] ] ---> [ ... ]``` 2차원 리스트를 1차원으로 변경하자. 참고로 Python에는 flatmap 함수가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "doc = reduce(lambda x,y: x+y, doc2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When I find myself in times of trouble',\n",
       " 'Mother Mary comes to me',\n",
       " 'Speaking words of wisdom, let it be',\n",
       " 'And in my hour of darkness',\n",
       " 'She is standing right in front of me',\n",
       " 'Speaking words of wisdom, let it be',\n",
       " '우리 Let it be',\n",
       " '나 Let it be',\n",
       " '너 Let it be',\n",
       " 'Let it be',\n",
       " 'Whisper words of wisdom, let it be']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn에는 ```fit_transform()``` 함수가 제공되어 fit()과 transform() 두 함수가 한꺼번에 적용될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(9))\t1\n",
      "  (np.int32(0), np.int32(10))\t1\n",
      "  (np.int32(1), np.int32(5))\t1\n",
      "  (np.int32(1), np.int32(4))\t1\n",
      "  (np.int32(1), np.int32(0))\t1\n",
      "  (np.int32(2), np.int32(7))\t1\n",
      "  (np.int32(2), np.int32(13))\t1\n",
      "  (np.int32(2), np.int32(12))\t1\n",
      "  (np.int32(2), np.int32(3))\t1\n",
      "  (np.int32(3), np.int32(2))\t1\n",
      "  (np.int32(3), np.int32(1))\t1\n",
      "  (np.int32(4), np.int32(8))\t1\n",
      "  (np.int32(4), np.int32(6))\t1\n",
      "  (np.int32(5), np.int32(7))\t1\n",
      "  (np.int32(5), np.int32(13))\t1\n",
      "  (np.int32(5), np.int32(12))\t1\n",
      "  (np.int32(5), np.int32(3))\t1\n",
      "  (np.int32(6), np.int32(3))\t1\n",
      "  (np.int32(6), np.int32(14))\t1\n",
      "  (np.int32(7), np.int32(3))\t1\n",
      "  (np.int32(8), np.int32(3))\t1\n",
      "  (np.int32(9), np.int32(3))\t1\n",
      "  (np.int32(10), np.int32(13))\t1\n",
      "  (np.int32(10), np.int32(12))\t1\n",
      "  (np.int32(10), np.int32(3))\t1\n",
      "  (np.int32(10), np.int32(11))\t1\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력설명: 괄호(문서번호, 단어번호)의 빈도이다. 단어번호는 할당된다 (아래 출력 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Spark CountVectorizer\n",
    "\n",
    "CountVectorizer는 단어를 분리하고 나서, 빈도를 계산할 수 있다. 결과는 단어빈도 word vector (sparse), 즉 단어별 단어빈도 TF이다.\n",
    "자주 사용된 단어가 아니어서 제거해야할 단어, 즉 문서에 사용된 빈도 document frequency를 minDF, maxDF를 통해 설정할 수 있다.\n",
    "소수점을 사용하면, 비율, 즉 '사용된 문서 수/전체 문서 수'를 의미한다.\n",
    "\n",
    "* minDF는 단어가 나타나는 최소 문서 빈도이고, 너무 **적게 발생하는 경우 무시**한다. 1.0은 기본 값이고, 문서 1개 이하에서 나타난 단어는 무시하라는 의미이다. 즉 어떤 단어도 무시하지 말라는 의미이다.\n",
    "* maxDF는 너무 **많이 발생하는 경우 무시**, min_df와 마찬가지로 1.0이 기본 값이고, **100%보다 많이** 발생하는 경우 무시 (즉, 어떤 단어도 무시하지 말라는 의미)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```CountVectorizerModel```은 ```fit()```하고 나면 얻어진다. 다음에 사용하는 ```HashingTF```는 ```fit()```하지 않는다는 점에서 차이가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cvModel = cv.fit(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.CountVectorizer'> <class 'pyspark.ml.feature.CountVectorizerModel'>\n"
     ]
    }
   ],
   "source": [
    "print (type(cv),type(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cvDf = cvModel.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(16,[5,7,9],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(16,[6,12,13],[1....|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DataFrame 전체를 출력하면 보기 불편하므로, 이 가운데 일부 컬럼만을 선택하여 출력할 수 있다.\n",
    "```(16,[5,6,8],[1.0,1.0,1.0])```\n",
    "16은 전체 단어의 개수, 그리고 다음 5,6,8은 값이 있는 컬럼 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,7,9],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[6,12,13],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[8,11],[1.0,1...|\n",
      "|She is standing r...|[standing, right,...|(16,[4,10,14],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|      우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|        너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,15],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.select('sent','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```CountVectorizer```에서 사용된 단어 목록을 출력할 수 있다. 아래 단어의 수를 세어보면 위 sparse vector의 컬럼 개수와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let',\n",
       " 'wisdom,',\n",
       " 'words',\n",
       " 'speaking',\n",
       " 'right',\n",
       " 'find',\n",
       " 'mother',\n",
       " 'trouble',\n",
       " 'hour',\n",
       " 'times',\n",
       " 'front',\n",
       " 'darkness',\n",
       " 'mary',\n",
       " 'comes',\n",
       " 'standing',\n",
       " 'whisper']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.8 TF-IDF\n",
    "\n",
    "```TfidfTransformer```는 **TF-IDF(Term Frequency-Inverse Document Frequency)**를 계산한다.\n",
    "이를 위해서는 우선 Tokenizer를 사용하여 문장을 단어로 분리해 놓아야 한다.\n",
    "\n",
    "HashingTF를 사용하여 'word vector'를 계산한다.\n",
    "HashingTF은 hash함수에 따라 단어의 고유번호를 생성하며, 단어 수가 많아지면서 고유번호가 충돌할 수 있는 가능성이 있다는 점에 주의한다.\n",
    "그리고 IDF를 계산하고, TF-IDF를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### S.5.8.1 TF-IDF 계산\n",
    "\n",
    "'Let it be'가사 세 번째 줄 **'wisdom' 단어**의 TF-IDF를 계산해보자.\n",
    "**TF**는 단어빈도수, 즉 문서에 단어가 나타난 빈도수를 의미한다.\n",
    "단어빈도는 경우에 따라서는 문제가 될 수 있다. 예를 들어, 'a', 'the', 'of'와 같은 단어는 빈도는 높지만 별로 유용하지 못하다.\n",
    "이 경우 IDF는 유용하다. **IDF**는 자주 나타나는 단어에 대한 가중치를 줄이고, 드물게 나타나는 단어에 가중치를 높이는 방식으로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "t는 단어, 문서는 d, D는 corpus,\n",
    "\n",
    "항목 | 설명 | 예제\n",
    "-----|-----|-----\n",
    "tf(d,f) | 단어 t가 문서 d에서 나타나는 단어의 빈도 수, term frequency | $f_{t,d}$ / (number of words in d) = 1/4 = 0.25<br>(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "df | document frequency 단어가 나타난 문서 수 | 3 (wisdom이 포함된 문서는 3)\n",
    "N | number of documents 전체 문서의 수 | 11 (전체의 문서는 11개)\n",
    "idf | inverse document frequency 단어가 나타난 문서의 비율을 거꾸로 | ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1<br>0으로 나뉘는 것을 방지하기 위해 **smoothing**, 즉 1을 더한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "doc2는 speaking 1, words 1, wisdom 1, let 1이 사용되었다고 하자 (불용어 제거)\n",
    "wisdom에 대해서 TF-IDF를 계산해보자.\n",
    "\n",
    "* tf는 term frequency 즉, wisdom은 1회/문서1의 4단어\n",
    "* idf는 inverse document frequency 단어가 나타난 문서의 비율을 거꾸로, wisdom이 포함된 문서횟수 3/전체의 문서갯수 11 -> 11/3. 스무딩을 하면서 1을 더하거나 해준다.\n",
    "\n",
    "프로그래밍에서는 메모리를 적게 사용하도록 설계되어 있다. ```1/4```와 같이 정수 타잎으로 연산하면, 정수를 사용하여 연산하고 그 결과도 정수를 출력하게 된다. 이를 변환하기 위해 ```1.```의 경우에서와 같이 소수를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf: 2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print (\"idf: {}\".format(idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.2 sklearn을 사용한 TF-IDF\n",
    "\n",
    "우선 'sklearn'의 TF-IDF를 계산해보자.\n",
    "**```CountVectorizer```**는 텍스트를 단어의 빈도로 변환해주어, 문서 x 단어 표를 출력할 수 있다.\n",
    "**```CountVectorizer()```**의 인자로\n",
    "analyzer (\"word\", \"character ngram\" 등 선택),\n",
    "tokenizer (단어의 tokenizer를 지정),\n",
    "stop_words (불용어 처리 기준),\n",
    "max_features (최대 속성 개수) 등을 지정할 수 있다.\n",
    "그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```TfidfVectorizer```를 사용해서 계산하면 그 결과를 아래와 같이 볼 수 있다.\n",
    "```python\n",
    "(2,12) 2.09861228867\n",
    "```\n",
    "\n",
    "결과에서\n",
    "**'2'**는 3번째 문서번호, **'12'**는 'wisdom' 단어번호\n",
    "TF-IDF는 ```2.09861228867```이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df, min_df는 기본 값이 1.0으로 굳이 설정하지 않아도 되는데, 어떤 단어도 무시하지 말라는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(9))\t2.791759469228055\n",
      "  (np.int32(0), np.int32(10))\t2.791759469228055\n",
      "  (np.int32(1), np.int32(5))\t2.791759469228055\n",
      "  (np.int32(1), np.int32(4))\t2.791759469228055\n",
      "  (np.int32(1), np.int32(0))\t2.791759469228055\n",
      "  (np.int32(2), np.int32(7))\t2.386294361119891\n",
      "  (np.int32(2), np.int32(13))\t2.09861228866811\n",
      "  (np.int32(2), np.int32(12))\t2.09861228866811\n",
      "  (np.int32(2), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(3), np.int32(2))\t2.791759469228055\n",
      "  (np.int32(3), np.int32(1))\t2.791759469228055\n",
      "  (np.int32(4), np.int32(8))\t2.791759469228055\n",
      "  (np.int32(4), np.int32(6))\t2.791759469228055\n",
      "  (np.int32(5), np.int32(7))\t2.386294361119891\n",
      "  (np.int32(5), np.int32(13))\t2.09861228866811\n",
      "  (np.int32(5), np.int32(12))\t2.09861228866811\n",
      "  (np.int32(5), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(6), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(6), np.int32(14))\t2.791759469228055\n",
      "  (np.int32(7), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(8), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(9), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(10), np.int32(13))\t2.09861228866811\n",
      "  (np.int32(10), np.int32(12))\t2.09861228866811\n",
      "  (np.int32(10), np.int32(3))\t1.4054651081081644\n",
      "  (np.int32(10), np.int32(11))\t2.791759469228055\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79175947, 2.79175947, 2.79175947, 1.40546511, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.38629436, 2.79175947, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.09861229, 2.09861229, 2.79175947])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### S.5.8.3 Spark를 사용한 TF-IDF\n",
    "\n",
    "##### TF\n",
    "HashingTF는 단어집합을 워드벡터 word vector로 변환하는데, 해시함수를 사용해서 단어에 해당하는 일련번호를 결정한다.\n",
    "\n",
    "그러나 해싱해서 생성된 ID는 서로 충돌할 수 있기 때문에, numFeatures를 충분히 늘려주어 충돌을 최소화하도록 한다.\n",
    "\n",
    "HashingTF에서의 **```numFeatures```는 $2^n$**으로 결정한다.\n",
    "예를 들어 단어 갯수가 900이면, $2^{10}=1024$이므로 1024로 설정한다.\n",
    "기본은 $2^{18}=262,144$이다.\n",
    "그러나 충분하지 못하게 설정하면 인덱스가 부족하거나 중복된 ID로 매핑될 수 있으니 주의해야 한다.\n",
    "\n",
    "예를 들어, \n",
    "문서 ```[speaking, words, wisdom,, let]```의 경우 ```(32,[4,24,27],[1.0,1.0,2.0])```가 출력된다.\n",
    "아래의 결과와 비교하면 단어 하나가 유실된 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=32) #  mapping indices insufficient\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```HashingTF```는 ```fit()```하지 않고 ```transform()``` 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hashTF.transform()의 결과는 **벡터의 튜플**이다. 예를 들어, ```(262144,[64317,91878,152481],[1.0,1.0,1.0])```\n",
    "262144는 해시 개수 (앞서 CountVectorizer의 경우에서와 같이 전체 단어의 개수가 아니다), 그리고 다음 [64317,91878,152481]은 값이 있는 **해시 컬럼** 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------------------------------------------------+\n",
      "|nostops                        |hash                                                    |\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "|[find, times, trouble]         |(262144,[64317,91878,152481],[1.0,1.0,1.0])             |\n",
      "|[mother, mary, comes]          |(262144,[24657,63767,245426],[1.0,1.0,1.0])             |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[hour, darkness]               |(262144,[74517,98431],[1.0,1.0])                        |\n",
      "|[standing, right, front]       |(262144,[84798,218360,229166],[1.0,1.0,1.0])            |\n",
      "|[speaking, words, wisdom,, let]|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[let]                          |(262144,[173339],[1.0])                                 |\n",
      "|[whisper, words, wisdom,, let] |(262144,[151864,173339,175131,188139],[1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.select(\"nostops\", \"hash\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "앞서 hashTF의 결과는 벡터튜플이고, 이를 IDF에 입력으로 넣어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF는 estimator이다. 따라서 fit을 해주고 transform을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idfModel = idf.fit(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=['find', 'times', 'trouble'], hash=SparseVector(262144, {64317: 1.0, 91878: 1.0, 152481: 1.0}))\n",
      "Row(nostops=['mother', 'mary', 'comes'], hash=SparseVector(262144, {24657: 1.0, 63767: 1.0, 245426: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['hour', 'darkness'], hash=SparseVector(262144, {74517: 1.0, 98431: 1.0}))\n",
      "Row(nostops=['standing', 'right', 'front'], hash=SparseVector(262144, {84798: 1.0, 218360: 1.0, 229166: 1.0}))\n",
      "Row(nostops=['speaking', 'words', 'wisdom,', 'let'], hash=SparseVector(262144, {27556: 1.0, 151864: 1.0, 173339: 1.0, 175131: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n",
      "Row(nostops=['let'], hash=SparseVector(262144, {173339: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.9 Word2Vec\n",
    "\n",
    "* BoW는 단어의 발생빈도만 계산, 문맥은 고려하지 않는다.\n",
    "\n",
    "사람들이 사용하는 언어를 컴퓨터가 이해하는 것은 입출력과는 완전히 다르고 또한 어렵다.\n",
    "\n",
    "일단 컴퓨터는 뭐든 하기 위해서는 수치화되어야 한다. 앞서 배운 BoW에 따르면, 단어의 빈도를 정량화하고 있다. 단어를 하나의 토큰으로만 인식하고 그 순서는 무시하고 있지만, 단어는 순서 자체가 의미를 가지는 경우가 많다.\n",
    "\n",
    "'영화'는 동영상을 의미할 수도, 신분이 귀하고 화려화다는 의미도 될 수 있고, 또는 영국의 화폐 파운드를 의미하기도 한다.\n",
    "- 철수는 영화를 보았다.\n",
    "- 철수는 부귀 영화를 누렸다.\n",
    "\n",
    "'세다'라는 단어도 마찬가지이다. 이 단어는 숫자를 세거나, 완력이 세다는 의미를 가질 수도 있다.\n",
    "- 하나 둘 숫자를 세다.\n",
    "- 그는 가장 힘이 세다.\n",
    "\n",
    "**Bag of Words 모델은 단어 순서와 문맥을 무시**한다. Word2Vec는 이런 BoW 모델의 단점을 극복하기 위해서,\n",
    "말뭉치로부터 **단어가 주변의 단어들과 어떻게 관련**되어 있는지 서로의 **맥락 또는 연관성 Word Embedding을 신경망으로 학습**하여 Word2Vec을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 단어의 벡터란?\n",
    "\n",
    "단어가 벡터로 변환되면, 벡터연산이 가능해지고 서로 간의 거리를 측정하여, 벡터 공간 상에서 가까울수록 비슷한 단어라고 해석하게 된다. 비슷한 문맥에서 등장하는 단어들은 비슷한 벡터로 매핑되는 경향이 있다.\n",
    "\n",
    "예를 들어, 비슷한 의미를 가진 단어들의 벡터는 유사한 방향으로 향하게 되고, 벡터 간의 내적이나 코사인 유사도를 통해 단어 간의 유사성을 계산할 수 있다.\n",
    "\n",
    "벡터('king') - 벡터('man') + 벡터('woman') = 벡터('queen) 이런 연산이 가능해진다. 즉 king 단어벡터에서 man 단어백터를 빼고, woman 단어백터를 더하면, queen 단어백터를 구할 수 있다는 의미이다.\n",
    "\n",
    "따라서 Word2Vec에서 vector는 단어의 의미적 특성을 나타내며, 이를 통해 단어 간의 유사성 및 의미적 관계를 측정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 방법\n",
    "\n",
    "Word2Vec은 CBow와 Skip-gram이 있다.\n",
    "- CBow는 어떤 단어를 문맥 안의 주변 단어들을 통해 예측하는 방법이다.\n",
    "CBow의 예를 들면, '철수는 재미있는 ___ 를 보았다'의 문장에서 ___ 를 예측하는 것이다.\n",
    "- Skip-gram은 반대로 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습\n",
    "\n",
    " 학습은 다음과 같이 진행된다.\n",
    "\n",
    "```vectorSize```는 단어벡터를 몇 개로 구성할 것인지, ```minCount```는 최소 단어빈도를 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tigerDf=spark.createDataFrame([[\"호랑이는 매우 용맹하다\"], [\"호랑이는 무섭다고 하더라\"], [\"호랑이는 사납고 날쌔다\"]], ['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tigerTokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tigerDf=tigerTokenizer.transform(_tigerDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=word2Vec.fit(tigerDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vDf = model.transform(tigerDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([0.0463, 0.0502, -0.0446]))\n",
      "Row(w2v=DenseVector([0.0694, 0.0022, -0.09]))\n",
      "Row(w2v=DenseVector([-0.0761, -0.0479, -0.0277]))\n"
     ]
    }
   ],
   "source": [
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어는 3차원 벡터 ([-0.1629, -0.1131, 0.0764] 등)로 출력되고 있다. 실제 Word2Vec에서는 보통 50, 100, 200, 300차원으로 사용된다. 차원이 높을수록 단어 표현력이 좋아지지만, 계산 비용이 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단의 벡터를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------------------------------------------+\n",
      "|word    |vector                                                          |\n",
      "+--------+----------------------------------------------------------------+\n",
      "|무섭다고|[0.09494997560977936,0.15659883618354797,-0.16218486428260803]  |\n",
      "|용맹하다|[0.0942535549402237,0.12480875849723816,-0.13816510140895844]   |\n",
      "|매우    |[0.014006072655320168,0.13399997353553772,0.15489862859249115]  |\n",
      "|사납고  |[-0.16290713846683502,-0.1131218895316124,0.0764860212802887]   |\n",
      "|하더라  |[0.08260229974985123,-0.04195655509829521,0.04253583028912544]  |\n",
      "|날쌔다  |[-0.09601441025733948,0.07757269591093063,-0.008975493721663952]|\n",
      "|호랑이는|[0.03057572990655899,-0.10813723504543304,-0.15047048032283783] |\n",
      "+--------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|    word|         similarity|\n",
      "+--------+-------------------+\n",
      "|무섭다고| 0.9970972537994385|\n",
      "|호랑이는|0.25962263345718384|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"용맹하다\", 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|    word|         similarity|\n",
      "+--------+-------------------+\n",
      "|용맹하다|0.25962263345718384|\n",
      "|무섭다고|0.22578667104244232|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"호랑이는\", 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.10 NGram\n",
    "\n",
    "텍스트를 대상으로 하면, n-gram은 연속된 n개의 토큰으로 구성된 순열을 말한다.\n",
    "unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGram은 Transformer이다. fit없이 바로 transform을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ngramDf = ngram.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|                sent|               words|                ngrams|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|When I find mysel...|[when, i, find, m...|  [when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|  [mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|  [and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|  [she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|      우리 Let it be| [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|        나 Let it be|   [나, let, it, be]| [나 let, let it, i...|\n",
      "|        너 Let it be|   [너, let, it, be]| [너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|       [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|  [whisper words, w...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'], ngrams=['when i', 'i find', 'find myself', 'myself in', 'in times', 'times of', 'of trouble'])\n",
      "Row(words=['mother', 'mary', 'comes', 'to', 'me'], ngrams=['mother mary', 'mary comes', 'comes to', 'to me'])\n",
      "Row(words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'], ngrams=['speaking words', 'words of', 'of wisdom,', 'wisdom, let', 'let it', 'it be'])\n"
     ]
    }
   ],
   "source": [
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.5.11 StringIndexer\n",
    "\n",
    "문자열 컬럼을 인덱스 컬럼으로 변환한다. **빈도가 제일 높은 순서**로 ```0.0```부터 인덱스 값이 주어진다. 인덱스는 double 형을 가지게 된다.\n",
    "**없는 레이블에 대해서는 예외**가 발생할 수 있으므로 (default), ```setHandleInvalid(\"skip\")``` 함수로 'skip', 'keep', 'error' 등으로 설정할 수 있다.\n",
    "\n",
    "구분 | 설명 | 예\n",
    "-----|-----|-----\n",
    "nominal | 명목 또는 구분 값 cateogry  | 사자, 호랑이, 사람\n",
    "ordinal | 명목값과 다른 점은 순서가 있다. | 키 low, med, high\n",
    "interval | 일정한 간격이 있다. | 150-165, 165-180, 180-195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 텍스트에 대해서는 적당한 명목변수가 없으므로, 문장 전체에 대해 인덱스로 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer는 Estimator이므로, fit을 먼저 해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=labelIndexer.fit(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "siDf=model.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      5.0|\n",
      "|Mother Mary comes...|      3.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      1.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|      우리 Let it be|      9.0|\n",
      "|        나 Let it be|      7.0|\n",
      "|        너 Let it be|      8.0|\n",
      "|           Let it be|      2.0|\n",
      "|Whisper words of ...|      6.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "#### 왜 One-Hot Encoding을 해야 하나\n",
    "\n",
    "더미 변수(dummy variable)는 통계학이나 머신러닝에서 명목변수를 수치형 변수 0 또는 1로 변환하기 위해 사용한다. One-Hot Encoding도 마찬가지이다.\n",
    "\n",
    "의사결정트리 기계학습의 경우에는 명목변수를 그대로 사용할 수 있다. 예를 들어, 휴가를 가는 곳이 '산' 또는 '바다'인지로 분기해 나가기 때문이다.\n",
    "회귀분석을 하면서 명목변수를 만나면, 그대로 사용한다면 어떤 의미가 있을까? '산' 또는 '바다'를 문자열로 입력해서는 크다 작다를 분별하거나 어떤 연산이 불가능하기 때문에, 한 걸음 나아가 숫자화 해서 이진변수로 나타내서 사용하는데, 이 경우 더미 변수라고 부른다.\n",
    "\n",
    "\n",
    "\n",
    "보통 기계학습도 명목변수를 그대로 사용하지 못하고, 정량화해서 숫자로 나타내야 한다. 명목변수는 어떻게든 숫자로 나타내야 하고, one-hot encoding이 방법이다.\n",
    "\n",
    "#### 단순히 숫자로 변환해도 충분하지 않다\n",
    "\n",
    "그렇다면 StringIndexer로 충분하다고 판단할 수 있다. 아니다!\n",
    "\n",
    "앞서 StringIndexer는 0 < 1 < 2 ... 라는 순서가 있는 것으로 보여질 수 있다.\n",
    "\n",
    "그렇다고 '산'은 1 '바다'는 0으로 표현하면 곤란하다 (또는 반대로 해도 마찬가지이다). 순서가 생겨서 어느 하나가 크거나 작거나 하게 된다.\n",
    "이진화해서 True, False로 표현해야 한다. 더미변수A는 '산'이면 1, 아니면 0, 더미변수B는 '바다'이면 1, 아니면 1 이렇게 표현한다.\n",
    "\n",
    "사자, 호랑이, 사람에게 인덱스 0, 1, 2가 배정되었다고 하자.\n",
    "그렇다고 해서 사자가 호랑이보다 순서가 앞선다는 의미는 아니다.\n",
    "실제로는 그런 순서가 있지 않다.\n",
    "\n",
    "#### One-hot Encoding은 명목변수 -> 정수 인덱스 -> 이진벡터 순서로 한다.\n",
    "\n",
    "먼저, 명목변수를 StringIndexer로 정수 인덱스로 변환한다.\n",
    "\n",
    "그 다음, One-Hot Encoding은 명목변수인덱스를 이진벡터로 변환하여 (이진변수는 dummy 변수라고도 한다), 서로 순서가 없도록 한다.\n",
    "\n",
    "명목변수가 A, B, C의 3가지 값을 가진다고 하자.\n",
    "그러면 2자리의 이진수로는 $2^2$, 4가지 조합이 가능하다.\n",
    "\n",
    "명목 값 | 이진 벡터 | Sparse Vector\n",
    "-----|-----|-----\n",
    "A | 10 | (2,[0],[1.0])\n",
    "B | 01 | (2,[1],[1.0])\n",
    "C | 00 | (2,[],[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, \"B\"),\n",
    "    (2, \"C\"),\n",
    "    (3, \"A\"),\n",
    "    (4, \"B\"),\n",
    "    (5, \"C\"),\n",
    "    (6, \"A\")\n",
    "], [\"id\", \"grade\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grade는 문자열이므로 우선 index로, 다음은 벡터로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"grade\", outputCol=\"gradeIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 문자열에 대한 인덱스를 벡터로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"gradeIndex\", outputCol=\"gradeVec\")\n",
    "encoded = encoder.fit(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "| id|grade|gradeIndex|     gradeVec|\n",
      "+---+-----+----------+-------------+\n",
      "|  1|    B|       1.0|(2,[1],[1.0])|\n",
      "|  2|    C|       2.0|    (2,[],[])|\n",
      "|  3|    A|       0.0|(2,[0],[1.0])|\n",
      "|  4|    B|       1.0|(2,[1],[1.0])|\n",
      "|  5|    C|       2.0|    (2,[],[])|\n",
      "|  6|    A|       0.0|(2,[0],[1.0])|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.transform(indexed).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명목변수가 여러개일 경우에는 다음과 같이 코드를 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, \"B\", \"Push\"),\n",
    "    (2, \"C\", \"Touch\"),\n",
    "    (3, \"A\", \"Wheel\"),\n",
    "    (4, \"B\", \"Wheel\"),\n",
    "    (5, \"C\", \"Touch\"),\n",
    "    (6, \"A\", \"Push\")\n",
    "], [\"id\", \"grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in df.columns[1:] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2: df.columns는 모든 컬럼명의 리스트를 뜻한다. ```[1:]```0번째 컬럼을 제외하고 2, 3을 포함하게 되고, 반복문을 사용하므로 코드를 줄일 수 있게 된다.bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer를 출력하면, 앞서 만든 StringIndexer를 모두 포함하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_518a49aa0b1c, StringIndexer_8c698fb16628]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 방식으로 반복문으로 여럿 OneHotEncoder를 포함하도록 encoder를 만들고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_c0e724c4b5b0, OneHotEncoder_0c8de26cc0d9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.12 연속데이터의 변환\n",
    "\n",
    "몸무게(inches), 키(pounds) 데이터를 분석해보자.\n",
    "이 데이터는 정량, 연속 데이터이다. \n",
    "출처는 https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\n",
    "\n",
    "```python\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "binDf = binarizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qdDf = discretizer.fit(binDf).transform(binDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.13 VectorAssembler\n",
    "\n",
    "열을 묶어서 Vector열로 만든다. features 컬럼을 생성할 경우에 사용한다.\n",
    "단 문자열 string은 묶을 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vaDf = va.transform(qdDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.14 Pipeline\n",
    "\n",
    "**Pipeline**은 여러 Estimator를 묶은 Estimator를 반환한다. Pipeline은 여러 작업을 묶어, 순서대로 단계적으로 Estimator를 적용하기 위해 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (0, \"a b c d e spark\", 1.0),\n",
    "        (1, \"b d\", 0.0),\n",
    "        (2, \"spark f g h\", 1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0),\n",
    "        (4, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer, HashingTF는 Transformer이고, LogisticRegression은 Estimator이다.\n",
    "\n",
    "파이프라인은 필요한 경우 fit, transform을 먼저 실행하게 된다.\n",
    "Tokenizer, HashingTF는 Transformer이므로 단어로 분리하고, 분리된 단어의 빈도를 계산해서 (즉 transform해서) 컬럼을 설정한다.\n",
    "다음 LogisticRegression은 fit()부터 먼저 실행해서 lr을 생성하고 tranform을 한다.\n",
    "\n",
    "Pipeline은 Estimator이므로, fit() 함수가 먼저 실행되고, PipelineModel (이것은 Transformer)에 따라 변환 transform을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(262144,[74920,89...|\n",
      "|  0.0|(262144,[89530,14...|\n",
      "|  1.0|(262144,[36803,17...|\n",
      "|  0.0|(262144,[132966,1...|\n",
      "|  0.0|(262144,[1074,389...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('label', 'features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 연설문을 기계학습하기 위해 변환\n",
    "\n",
    "2019.10.21일 '제74주년 경찰의 날 기념식 축사' 전문을 변환하세요.\n",
    "전문은 http://www.korea.kr/archive/speechView.do?newsId=132031636 에서 읽을 수 있고, 해당 사이트에서 텍스트만 파일로 저장해서 사용한다.\n",
    "\n",
    "* 1) DataFrame 생성\n",
    "* 2) 단어로 분리해서, 출력\n",
    "* 3) 정리 strip, replace\n",
    "* 3) 불용어 구성, 출력 - 축사 전문에서 한글자로 된 단어를 찾아내 스스로 구성\n",
    "* 4) 불용어 제거하고, 출력\n",
    "* 5) TF-IDF를 계산하고, 출력\n",
    "* 6) TF-IDF 컬럼을 features로 구성, 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#spark.createDataFrame(os.path.join(\"data\", \"20191021_policeAddress.txt\"))\n",
    "#df=spark.read.text(os.path.join(\"data\", \"20191021_policeAddress.txt\"))\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "police=spark.read\\\n",
    "    .options(header=\"true\", delimiter=\" \", inferSchema=\"true\")\\\n",
    "    .schema(\n",
    "        StructType([\n",
    "            StructField(\"sent\",StringType()),\n",
    "            ])\n",
    "    )\\\n",
    "    .text(os.path.join(\"data\", \"20191021_policeAddress.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 줄 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "police.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "_police=police.filter(\"sent != ' '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_police.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문장이 잘리지 않고 온전하게 출력되려면 'False'로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                              sent|\n",
      "+----------------------------------+\n",
      "|  존경하는 국민 여러분, 경찰관 ...|\n",
      "| 국민의 안전을 위해 밤낮없이 애...|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_police.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 단어로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"tokens\")\n",
    "tokDf = tokenizer.transform(police)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------------------------+\n",
      "|                             sent|                        tokens|\n",
      "+---------------------------------+------------------------------+\n",
      "| 존경하는 국민 여러분, 경찰관 ...| [존경하는, 국민, 여러분,, ...|\n",
      "|                                 |                            []|\n",
      "|국민의 안전을 위해 밤낮없이 애...|[국민의, 안전을, 위해, 밤낮...|\n",
      "+---------------------------------+------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.\n",
      " \n",
      "국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\").take(3):\n",
    "    print (r[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 정리\n",
    "\n",
    "텍스트는 정리가 필요하다. 예를 들어, '여러분,'은 컴마로 인해 '여러분'과 다른 단어로 인식된다.\n",
    "'날’입니다.'도 '날입니다.'와 다른 단어로 취급된다.\n",
    "\n",
    "#### 컴마, 따옴표, 마침표 등 제거\n",
    "\n",
    "'여러분,', '‘경찰의', '날’입니다.' 에서 발견할 수 있듯이, 컴마, 따옴표, 마침표 등을 제거해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'여러분'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'여러분,'.rstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'경찰의'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'‘경찰의'.lstrip('‘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'날’입니다'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'날’입니다.'.rstrip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'날입니다.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'날’입니다.'.replace(\"’\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList=['존경하는', '국민', '여러분,', '경찰관', '여러분,', '일흔네', '‘경찰의', '날’입니다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '국민', '여러분', '경찰관', '여러분', '일흔네', '경찰의', '날입니다']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned=list()\n",
    "for w in wordList:\n",
    "    cleaned.append(w.lstrip('‘').rstrip(\"’\").rstrip(',').rstrip('.').replace(\"’\",\"\").replace(\"”\",\"\"))\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 숫자 제거\n",
    "\n",
    "숫자가 하나 이상 있는 경우 정규식 패턴 ```\\d+```를 적용한다.\n",
    "숫자로 시작하는 문자열인 경우 ```regex.match(w)```가 아니면 컴마등을 정리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Seoul1', 'Seoul']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('\\d+')\n",
    "cleaned=list()\n",
    "\n",
    "wordList=[\"1\", \"123\", \"15만\", \"2015년에\", \"15.1%\", \"74.5점\", \"8,572명을\", \"Seoul1\", \"Seoul\"]\n",
    "for w in wordList:\n",
    "    if not regex.match(w):\n",
    "        cleaned.append(w)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수: 컴마, 따옴표, 마침표, 숫자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def trim(wordList):\n",
    "    regex = re.compile('\\d+')\n",
    "    cleaned=list()\n",
    "    for w in wordList:\n",
    "        if not regex.match(w):\n",
    "            cleaned.append(w.lstrip('‘').rstrip(\"’\").rstrip(',').rstrip('.').replace(\"’\",\"\").replace(\"”\",\"\"))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 함수가 올바르게 실행되는지 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seoul1', 'Seoul']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[\"1\", \"123\", \"15만\", \"2015년에\", \"15.1%\", \"74.5점\", \"8,572명을\", \"Seoul1\", \"Seoul\"]\n",
    "trim(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 udf에 넣어서 컬럼을 생성해보자.\n",
    "udf함수 결과는 **문자열 배열 ArrayType(StringType)**로 맞추어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "trimUdf=f.udf(trim, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsDf = tokDf.withColumn('words', trimUdf(f.col('tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 보듯이, 15만과 같이 숫자가 포함된 문자열이 제거되었다는 것을 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sent                                                                                                                                                                              |tokens                                                                                                                                                                                                    |words                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.                                                                                                                 |[존경하는, 국민, 여러분,, 경찰관, 여러분,, 일흔네, 돌, ‘경찰의, 날’입니다.]                                                                                                                               |[존경하는, 국민, 여러분, 경찰관, 여러분, 일흔네, 돌, 경찰의, 날입니다]                                                                                                                           |\n",
      "|                                                                                                                                                                                  |[]                                                                                                                                                                                                        |[]                                                                                                                                                                                               |\n",
      "|국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.|[국민의, 안전을, 위해, 밤낮없이, 애쓰시는, 전국의, 15만, 경찰관, 여러분께, 먼저, 감사를, 드립니다., 전몰·순직, 경찰관들의, 고귀한, 희생에, 경의를, 표합니다., 유가족, 여러분께, 위로의, 마음을, 전합니다.]|[국민의, 안전을, 위해, 밤낮없이, 애쓰시는, 전국의, 경찰관, 여러분께, 먼저, 감사를, 드립니다, 전몰·순직, 경찰관들의, 고귀한, 희생에, 경의를, 표합니다, 유가족, 여러분께, 위로의, 마음을, 전합니다]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDf.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 불용어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_d31f9c57b0fd"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "#stop = StopWordsRemover(inputCol=\"tokens\", outputCol=\"nostops\")\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")\n",
    "stop.setStopWords([u\"돌\", u\"너\", u\"우리\", u'있습니다', u'더', u'합니다', u'그', u'드립니다', u'것입니다'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_mystopwords=[u\"나\", u\"너\", u\"우리\"]\n",
    "stopwords=list()\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------------------------+------------------------------+------------------------------+\n",
      "|                             sent|                        tokens|                         words|                       nostops|\n",
      "+---------------------------------+------------------------------+------------------------------+------------------------------+\n",
      "| 존경하는 국민 여러분, 경찰관 ...| [존경하는, 국민, 여러분,, ...|[존경하는, 국민, 여러분, 경...|[존경하는, 국민, 여러분, 경...|\n",
      "|                                 |                            []|                            []|                            []|\n",
      "|국민의 안전을 위해 밤낮없이 애...|[국민의, 안전을, 위해, 밤낮...|[국민의, 안전을, 위해, 밤낮...|[국민의, 안전을, 위해, 밤낮...|\n",
      "+---------------------------------+------------------------------+------------------------------+------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stopDf=stop.transform(tokDf)\n",
    "stopDf=stop.transform(wordsDf)\n",
    "stopDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['존경하는', '국민', '여러분', '경찰관', '여러분', '일흔네', '경찰의', '날입니다']\n",
      "[]\n",
      "['국민의', '안전을', '위해', '밤낮없이', '애쓰시는', '전국의', '경찰관', '여러분께', '먼저', '감사를', '전몰·순직', '경찰관들의', '고귀한', '희생에', '경의를', '표합니다', '유가족', '여러분께', '위로의', '마음을', '전합니다']\n"
     ]
    }
   ],
   "source": [
    "for r in stopDf.select(\"nostops\").take(3):\n",
    "    for e in r:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 단어빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어를 제거한 단어들을 가진 컬럼을 RDD로 변환해서 출력해보자.\n",
    "리스트의 Row로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nostops=['존경하는', '국민', '여러분', '경찰관', '여러분', '일흔네', '경찰의', '날입니다']),\n",
       " Row(nostops=[]),\n",
       " Row(nostops=['국민의', '안전을', '위해', '밤낮없이', '애쓰시는', '전국의', '경찰관', '여러분께', '먼저', '감사를', '전몰·순직', '경찰관들의', '고귀한', '희생에', '경의를', '표합니다', '유가족', '여러분께', '위로의', '마음을', '전합니다'])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopDf.select(\"nostops\").rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap()을 하면, Row가 제거된다. 그래도 2차원 리스트로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['존경하는', '국민', '여러분', '경찰관', '여러분', '일흔네', '경찰의', '날입니다'], []]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopDf.select(\"nostops\").rdd.flatMap(lambda x:x).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap()을 한차례 더하면, 2차원 리스트마저 해제되어 1차원 리스트가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경하는', '국민', '여러분']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopDf.select(\"nostops\").rdd.flatMap(lambda x:x).flatMap(lambda x:x)\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 RDD에서 해보았던대로 단어빈도를 계산하고, 빈도순으로 정렬하여 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, '경찰은'),\n",
       " (7, '국민의'),\n",
       " (6, '여러분'),\n",
       " (6, '경찰의'),\n",
       " (5, '경찰관'),\n",
       " (4, '경찰'),\n",
       " (4, '우리의'),\n",
       " (3, '여러분께'),\n",
       " (3, '역대'),\n",
       " (3, '가장'),\n",
       " (3, '함께'),\n",
       " (2, '안전을'),\n",
       " (2, '위해'),\n",
       " (2, '먼저'),\n",
       " (2, '감사를'),\n",
       " (2, '받으신'),\n",
       " (2, '비롯한'),\n",
       " (2, '또한'),\n",
       " (2, '외국'),\n",
       " (2, '경찰을')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopDf.select(\"nostops\")\\\n",
    "    .rdd\\\n",
    "    .flatMap(lambda x:x).flatMap(lambda x:x)\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경찰은, 경찰의, 경찰 이런 단어들은 이음동의로 처리하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, '경찰'),\n",
       " (7, '국민의'),\n",
       " (6, '여러분'),\n",
       " (5, '경찰관'),\n",
       " (4, '우리의'),\n",
       " (3, '여러분께'),\n",
       " (3, '역대'),\n",
       " (3, '가장'),\n",
       " (3, '함께'),\n",
       " (2, '안전을'),\n",
       " (2, '위해'),\n",
       " (2, '먼저'),\n",
       " (2, '감사를'),\n",
       " (2, '받으신'),\n",
       " (2, '비롯한'),\n",
       " (2, '또한'),\n",
       " (2, '외국'),\n",
       " (2, '경찰을'),\n",
       " (2, '경찰헌장은'),\n",
       " (2, '겨레를')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopDf.select(\"nostops\")\\\n",
    "    .rdd\\\n",
    "    .flatMap(lambda x:x).flatMap(lambda x:x)\\\n",
    "    .map(lambda x: x.replace(\"경찰은\",\"경찰\"))\\\n",
    "    .map(lambda x: x.replace(\"경찰의\",\"경찰\"))\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TF-IDF 계산, features로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=32) #  mapping indices insufficient\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")\n",
    "\n",
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|nostops                                                                                                                                                                                                                                                   |hash                                                                                                                                                                                                                                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[존경하는, 국민, 여러분, 경찰관, 여러분, 일흔네, 경찰의, 날입니다]                                                                                                                                                                                        |(262144,[162,62257,80697,80732,89874,127225,160086],[1.0,1.0,1.0,1.0,2.0,1.0,1.0])                                                                                                                                                                                                  |\n",
      "|[]                                                                                                                                                                                                                                                        |(262144,[],[])                                                                                                                                                                                                                                                                      |\n",
      "|[국민의, 안전을, 위해, 밤낮없이, 애쓰시는, 전국의, 경찰관, 여러분께, 먼저, 감사를, 전몰·순직, 경찰관들의, 고귀한, 희생에, 경의를, 표합니다, 유가족, 여러분께, 위로의, 마음을, 전합니다]                                                                   |(262144,[5341,30732,43098,49855,63600,75300,77757,89318,96799,123553,160081,160086,167255,178931,208192,217323,224666,247023,257249,261393],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "|[]                                                                                                                                                                                                                                                        |(262144,[],[])                                                                                                                                                                                                                                                                      |\n",
      "|[오늘, 홍조근정훈장을, 받으신, 중앙경찰학교장, 이은정, 치안감님, 근정포장을, 받으신, 광주남부경찰서, 김동현, 경감님을, 비롯한, 수상자, 여러분께, 각별한, 축하와, 감사를, 또한, 경찰, 영웅으로, 추서되신, 차일혁, 최중락님께, 국민의, 사랑을, 전해드립니다]|(262144,[29823,32228,36822,49855,61014,61103,72971,82902,84159,89227,91126,118725,153204,167255,169211,185989,206065,217323,220284,235673,237148,248593,249689,251574,254458],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfDf.select(\"nostops\", \"hash\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "#word2Vec = Word2Vec(vectorSize=20, minCount=1, inputCol=\"nostops\", outputCol=\"w2v\")\n",
    "#model = word2Vec.fit(stopDf)\n",
    "#w2vDf = model.transform(stopDf)\n",
    "word2Vec = Word2Vec(vectorSize=20, minCount=1, inputCol=\"tokens\", outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|word       |vector                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|공정해야   |[-0.01956624910235405,4.3965294025838375E-4,0.02093738690018654,-0.02469729632139206,-0.0029533396009355783,-0.013268505223095417,0.014036831445991993,0.015425316989421844,-0.007554206997156143,-0.010221980512142181,0.01579464226961136,0.005100483540445566,-0.0070682791993021965,0.01378161646425724,-0.011855483055114746,-0.017141193151474,0.024288376793265343,-0.02176894247531891,0.0012321865651756525,0.01664634235203266]            |\n",
      "|자치경찰제 |[-0.0023411624133586884,-0.02290630154311657,-0.022723712027072906,0.010539853014051914,0.009328084997832775,0.010304925963282585,-0.0072167133912444115,0.014063694514334202,-0.007251224014908075,-0.019550971686840057,-0.022707294672727585,0.009587050415575504,-0.0036982186138629913,-0.006747324950993061,-0.006122529972344637,-0.002275155857205391,-0.02392960712313652,-0.0024442244321107864,0.007628464139997959,-0.020418955013155937]|\n",
      "|공권력을   |[-0.02462926134467125,0.008254586718976498,0.0021805132273584604,0.004449242260307074,-0.0068113552406430244,7.719086715951562E-5,0.020371511578559875,0.004845248069614172,0.006193369161337614,-0.01958371326327324,-0.023182766512036324,0.008837563917040825,0.004199927672743797,0.023293951526284218,-0.012475602328777313,-0.011701175011694431,-0.021298261359333992,2.9580132104456425E-4,-0.009890198707580566,0.01527214702218771]        |\n",
      "|흘렸습니다.|[-0.021057672798633575,-0.0125816585496068,4.1893700836226344E-4,-7.56933179218322E-4,0.020840231329202652,0.012451408430933952,-0.023384105414152145,0.009410099126398563,-0.008355128578841686,-0.0011506255250424147,0.008969446644186974,-0.01135278306901455,-0.00968469213694334,0.0026358349714428186,0.022347405552864075,0.02441575936973095,0.013212746009230614,-0.006101911887526512,-0.013286322355270386,0.020274845883250237]         |\n",
      "|만들고     |[-0.009184147231280804,-0.004762627184391022,-0.023159625008702278,0.021564967930316925,0.008916742168366909,0.0017421947559341788,-0.015107473358511925,-0.0058917878195643425,0.007869087159633636,0.008789648301899433,-0.0028742437716573477,-0.003788797650486231,-0.00806998647749424,-0.024558110162615776,-0.0036999185103923082,-0.01813952624797821,-0.01822650618851185,-0.023571711033582687,0.001330035156570375,-0.011234426870942116] |\n",
      "+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      word|        similarity|\n",
      "+----------+------------------+\n",
      "|    제약을|0.6079672574996948|\n",
      "|    가시기|0.5796914100646973|\n",
      "|    공정한|0.5436513423919678|\n",
      "|  다짐으로|0.4804166853427887|\n",
      "|검찰개혁과|0.4793410003185272|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"검경\", 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|준수해야|0.6232707500457764|\n",
      "|    좋게|0.5994613170623779|\n",
      "|    비해|0.5407519936561584|\n",
      "| 늘렸고,|0.5286233425140381|\n",
      "|체감으로|0.5016434788703918|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"치안이\", 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|       word|        similarity|\n",
      "+-----------+------------------+\n",
      "|   엄정하고|0.6329967975616455|\n",
      "|  전몰·순직|0.5268281698226929|\n",
      "|     최고를|0.5212573409080505|\n",
      "|     안보를|0.4808694124221802|\n",
      "|마땅합니다.|0.4756418764591217|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"공권력이\", 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 문제: 기계학습에 변환 데이터를 입력, 실행하기\n",
    "\n",
    "기계학습 Machine learning은 컴퓨터가 스스로 학습을 해서 문제를 해결한다.\n",
    "\n",
    "지도학습은 일정한 형식의 입력데이터가 필요하다.\n",
    "- label 컬럼: 분류 값을 가진다. 예를 들어, 재미있는 영화=1, 재미없는 영화=0의 이진분류\n",
    "- features 컬럼: 속성 값으로 예를 들어, 재미있는 영화는 주인공, 투자비, 시나리오 등의 특징이 있다.\n",
    "\n",
    "LIBSVM은 기계학습 모델인 svm을 위한 입력데이터 형식이다.\n",
    "0은 label, 나머지는 index:value 쌍으로 구성한다.\n",
    "\n",
    "```python\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "```\n",
    "\n",
    "* 예\n",
    "```python\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataFrame 읽기\n",
    "\n",
    "파일의 경로를 설정한다. ```sample_libsvm_data.txt``` 파일을 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-3.0.0-bin-hadoop2.7')\n",
    "fsvm=os.path.join(os.getcwd(),'data','sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DataFrameReader의 포맷 format(\"libsvm\")과 options을 설정해서 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "스키마를 출력하면, 의도한 바와 같이 label, features 컬럼과 그 타입이 double, vector로 설정되어 있다.\n",
    "\n",
    "지도학습에 필요한 label (double), features (vector) 컬럼이 있으니 다행이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "데이터의 label, features 컬럼은 sparse vectors 형식으로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "_df=dfsvm.withColumn('labelStr', dfsvm.label.cast(IntegerType()).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+\n",
      "|label|            features|labelStr|\n",
      "+-----+--------------------+--------+\n",
      "|  0.0|(692,[127,128,129...|       0|\n",
      "|  1.0|(692,[158,159,160...|       1|\n",
      "|  1.0|(692,[124,125,126...|       1|\n",
      "|  1.0|(692,[152,153,154...|       1|\n",
      "|  1.0|(692,[151,152,153...|       1|\n",
      "+-----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLUtils를 사용하여 RDD 읽기\n",
    "\n",
    "또는 ```MLUtils.loadLibSVMFile```로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(spark.sparkContext, fsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0})]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, test 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#dfsvm = spark.read.format(\"libsvm\").load(os.path.join(os.getcwd(),'data','sample_libsvm_data.txt'))\n",
    "train, test = dfsvm.randomSplit([0.6,0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "label은 현재 0,1 이진분류 값을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   43|\n",
      "|  1.0|   57|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 선형 SVM (Linear Support Vector Machine)\n",
    "\n",
    "우선 데이터 명칭에 어울리는 SVM을 시도해보자.\n",
    "\n",
    "SVM은 데이터를 이진, 다중 분류하는 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "훈련데이터에 대해 모델링하고, 그 모델을 실제 test에 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lsvcModel = lsvc.fit(train)\n",
    "#print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "#print(\"Intercept: \" + str(lsvcModel.intercept))\n",
    "\n",
    "testDf = lsvcModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.select('label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n",
    "\n",
    "이진, 다중분류가 가능하다.\n",
    "\n",
    "Binomial logistic regression의 이진분류를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "# lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\") # 다중분류\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "#print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|[0.79131099251271...|[0.68811275675687...|       0.0|\n",
      "|  0.0|[0.96805045224787...|[0.72473074107468...|       0.0|\n",
      "|  0.0|[0.78812640324048...|[0.68742889151848...|       0.0|\n",
      "|  0.0|[0.82313549866571...|[0.69490151379221...|       0.0|\n",
      "|  0.0|[0.66486774510368...|[0.66035301066603...|       0.0|\n",
      "|  0.0|[0.66217249492003...|[0.65974824031487...|       0.0|\n",
      "|  0.0|[0.93934922460813...|[0.71896818507846...|       0.0|\n",
      "|  0.0|[0.58292384885578...|[0.64173990605996...|       0.0|\n",
      "|  0.0|[0.80142759058619...|[0.69027977374768...|       0.0|\n",
      "|  0.0|[0.85941701244153...|[0.70253883714673...|       0.0|\n",
      "|  0.0|[0.55874069026810...|[0.63616111026927...|       0.0|\n",
      "|  0.0|[0.92721583055183...|[0.71651009665097...|       0.0|\n",
      "|  0.0|[0.68558502970096...|[0.66498407604905...|       0.0|\n",
      "|  0.0|[0.91599812607805...|[0.71422599507308...|       0.0|\n",
      "|  0.0|[0.91894424272898...|[0.71482693905013...|       0.0|\n",
      "|  0.0|[0.68120343764327...|[0.66400723926908...|       0.0|\n",
      "|  0.0|[0.90404219453940...|[0.71177946600621...|       0.0|\n",
      "|  0.0|[0.87322791683480...|[0.70541691615571...|       0.0|\n",
      "|  0.0|[0.85097588781765...|[0.70077181713738...|       0.0|\n",
      "|  0.0|[0.90456328733427...|[0.71188635611207...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.select('label','rawPrediction','probability','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "조건부 베이지안 확률에 따라 분류하는 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb=NaiveBayes(featuresCol='features', labelCol='label', modelType='multinomial', predictionCol='prediction')\n",
    "#nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train)\n",
    "\n",
    "predictions=model.transform(test)\n",
    "\n",
    "predictions.select('label', 'prediction').show()\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### decision tree\n",
    "\n",
    "조건에 따라 분기하는 의사결정 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "dfsvm = spark.read.format(\"libsvm\").load(\"data/sample_libsvm_data.txt\")\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(dfsvm)\n",
    "\n",
    "# maxCategories > 4보다 크면 연속값\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.0714286 \n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_c8e82dc10b99, depth=1, numNodes=3, numClasses=2, numFeatures=692\n"
     ]
    }
   ],
   "source": [
    "(train, test) = dfsvm.randomSplit([0.7, 0.3])\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

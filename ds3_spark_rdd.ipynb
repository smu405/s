{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 3: Spark RDD\n",
    "\n",
    "* Last updated 202410_202310_202009_201910_201812_201712_201705_201611\n",
    "\n",
    "Spark를 시작하기 위해서는 그 진입점 SparkSession를 생성해야 한다. 그로부터 RDD를 파일, JSON 등에서 읽어서 생성할 수 있다. 낯선 단어 RDD는 비구조적인 데이터에 보다 적합하고, 많이 쓰이는 Map-Reduce 등 RDD API를 사용하여 데이터를 변환하고, 분석할 수 있다. 텍스트를 단어 빈도, word vector로 변환해보기로 한다.\n",
    "\n",
    "* 3.1 Spark 시작하기: 완전설치한 경우, PIP설치한 경우로 구분하여 설명\n",
    "* 3.2 Spark 설정\n",
    "* 3.3 데이터 구조: RDD, 데이터프레임\n",
    "* 3.4 RDD 소개\n",
    "* 3.5 RDD 생성: 파일, json 등에서 생성하기\n",
    "* 3.6 RDD API, Pair RDD\n",
    "* 3.7 spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession은 Apache Spark 2.0부터 도입된 애플리케이션의 기본 진입점으로, 이전에 사용되던 개별적인 SparkContext, SQLContext, HiveContext 등을 하나로 통합한 객체입니다. 이를 통해 사용자는 데이터 처리와 분석에 필요한 모든 API를 단일 진입점에서 편리하게 사용할 수 있습니다.\n",
    "\n",
    "1. SparkSession의 역할\n",
    "클러스터와의 인터페이스: Spark 클러스터에 접속하는 클라이언트 역할을 하며, CPU, 메모리와 같은 자원을 할당받아 작업을 수행합니다.\n",
    "데이터 처리 및 분석: DataFrame API를 사용하거나, SQL 쿼리를 실행하고, RDD 변환과 같은 작업을 제공합니다.\n",
    "자원 관리: 클러스터 매니저(YARN, Mesos 등)로부터 자원을 배분받아 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Spark 시작하기\n",
    "\n",
    "### 3.1.1 SparkSession이 뭐지\n",
    "\n",
    "SparkSession은 Apache Spark 버전 2.0부터 도입된 애플리케이션의 진입점으로, 이전에 사용되던 SparkContext, SQLContext, HiveContext 등을 하나로 통합한 객체이다.\n",
    "이를 통해 다음 작업을 위한 API를 단일 진입점에서 편리하게 사용할 수 있다.\n",
    "\n",
    "- 클러스터와의 인터페이스: Spark 클러스터에 접속하는 클라이언트 역할을 한다. 클러스터 매니저(YARN, Mesos, Kubernetes 등)와 통신해 작업을 스케줄링한다.\n",
    "- 데이터 처리 및 분석: DataFrame API를 사용하거나, SQL 쿼리를 실행하고, RDD 변환과 같은 작업을 제공한다.\n",
    "- 자원 관리: 클러스터 매니저(YARN, Mesos 등)로부터 CPU, 메모리와 같은 자원을 배분받아 작업을 수행한다. SparkSession을 사용하면 클러스터 매니저와 효율적으로 통합되어 자원 할당이 자동화된다.\n",
    "\n",
    "참고로 Spark 2.0 이전에는 아래 코드에서 보듯이 SparkContext를 먼저 만들고 이를 통해 다른 SQLContext, HiveContext를 사용했다. 이런 방식은 2.x에서도 호환되므로 그대로 사용할 수도 있지만, **SparkSession으로 통합**되었다.\n",
    "\n",
    "```python\n",
    "# Spark 버전 1.x에서는 SparkSession이 진입점이 아니었다\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf)  #SparkContext를 직접 생성한다.\n",
    "sqlContext = SQLContext(sc)           #SparkContext를 넣어서 SQLContext를 생성\n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Context는 다음과 같이 RDD, DataFrame, 라이브데이터, 데이터베이스에 대해 제공되고 있다.\n",
    "\n",
    "Context 구분 | 설명 | 사용 예\n",
    "----------|----------|----------\n",
    "SparkContext | RDD를 사용하는 Context | 버전 2.0부터는 SparkSession.sparkContext를 사용\n",
    "StreamingContext | 트위터, Flume, TCP 소켓 등 실시간 발생하는 데이터 처리 | pyspark.streaming.StreamingContext(sparkConf, Seconds(2)) 2초마다 스트리밍 데이터를 처리\n",
    "SQLContext | Spark SQL, DataFrame | 버전2부터는 spark.sql (1.x에서는 SQLContext(SparkContext))\n",
    "HiveContext | HiveQL, DataFrame | 버전2부터는 spark.sql (1.x에스는 HiveContext(SparkContext))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 3.1.2  Spark 완전설치는 경로 설정 후 SparkSession 생성하자.\n",
    "\n",
    "완전설치한 경우에는 앞서 작성한 코드를 재사용해서 spark 객체를 생성하면 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 단독형 PIP pypark의 설치는 SPARK_HOME/라이브러리 설정 필요없다\n",
    "\n",
    "pyspark를 설치하는 경우에는 Spark가 설치된 경로 SPARK_HOME, Python 라이브러리 경로 PYTHONPATH를 별도로 설정할 필요가 없이 SparkSession을 별도로 생성하면 된다.\n",
    "\n",
    "혹시 다른 방식, 예를 들면 아파치 스파크가 제공하는 pyspark 프롬프트 또는 Databricks의 Spark Cloud를 사용하는 경우는 SparkSession을 미리 생성해 주기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk 21.0.1 2023-10-17\n",
      "OpenJDK Runtime Environment (build 21.0.1+12-29)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.1+12-29, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "설치하고 나면 아래 출력이 발견된다. 그렇지 않으면 오류일 가능성이 크다.\n",
    "\n",
    "Successfully built pyspark\n",
    "Installing collected packages: py4j, pyspark\n",
    "Successfully installed py4j-0.10.9.7 pyspark-3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 경로 설정\n",
    "\n",
    "혹시 모르니, 특히 자신의 PC에 복수의 파이썬 2.x, 3.x 버전이 설치되어 있는 경우, 어느 버전을 사용해야 하는지 환경변수를 설정해서 실행 경로를 지정해주도록 한다.\n",
    "\n",
    "현재 사용 중인 파이썬 인터프리터의 실행 가능한 파일 경로를 출력하고 이를 적용하면 안전하다 (파이썬 가상 환경(가상환경)을 사용하고 있는 경우에도 적용된다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\program files\\\\python39\\\\python.exe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### SparkSession 생성\n",
    "\n",
    "이제 Spark의 진입점 ```SparkSession``` 객체를 생성하기 위해 pyspark.sql 모듈의 SparkSession.builder, 즉 ```pyspark.sql.SparkSession.builder```를 사용한다.\n",
    "\n",
    "필요한 설정은 SparkSession이 만들어지기 전에 해 두는 편이 좋다. 여기서는 설정을 별도로 하지 않고 비워 놓았다.\n",
    "\n",
    "spark를 생성하기 전 환경변수를 설정해주는 편이 좋다고 했는데 왜 그럴까? \n",
    "```getOrCreate()```가 그런 의미이다. 이 함수는 이미 SparkSession 객체가 있으면 기존에 있는 spark 인스턴스를 가져와 쓰고, 아니면 새로운 객체를 생성하여 반환하기 때문이다.\n",
    "\n",
    "Spark를 실행하기 전 **필수적**으로 설정하는 항목은 **master**, **appName**이다.\n",
    "\n",
    "> 환경 설정을 변경하고자 할 때의 문제점\n",
    "> \n",
    "> 기존에 만들어진 SparkSession이 있다면, 새로운 설정이 적용되지 않는다. Jupyter Notebook에서는 커널을 재시작하지 않으면 메모리에 기존 SparkSession이 남아있을 수 있다. 기존의 spark를 메모리에서 제거한 후 설정을 해야 효과가 있기 때문에, 커널 재시작을 통해 SparkSession 관련 변수를 초기화하는 것이 정확한 설정 적용을 보장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=sys.executable          # 현재 사용 중인 Python 실행 파일 경로 설정 (리눅스 \"/usr/bin/python3\")\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=sys.executable   # Driver에서도 동일한 Python 경로 설정\n",
    "#os.environ['HADOOP_HOME']=os.getcwd() # 현재 디렉터리를 HADOOP_HOME으로 설정\n",
    "#os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin') # PATH에 Hadoop 바이너리 추가\n",
    "\n",
    "myConf=pyspark.SparkConf() # 기본 설정 객체 생성, 여기에 필요한 설정 정의\n",
    "#myConf=pyspark.SparkConf().set(\"spark.driver.bindAddress\", \"127.0.0.1\") 드라이버 바인딩 주소 설정\n",
    "#myConf=pyspark.SparkConf().set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.13:10.1.1\") \n",
    "\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 설명\n",
    "\n",
    "- 코드 os.environ을 사용하기 위한 ```import os```\n",
    "- 코드 sys.path를 사용하기 위한 ```import sys```\n",
    "- 코드 \"PYSPARK_PYTHON\", \"PYSPARK_DRIVER_PYTHON\" 경로를 설정한다.\n",
    "    * PYSPARK_PYTHON는 workers/executors가 사용하는 버전, ```sys.executable```을 적어준다.\n",
    "    * PYSPARK_DRIVER_PYTHON: driver가 사용하는 버전\n",
    "- 코드 os.environ['HADOOP_HOME']:  HDFS와 연동 등 필요한 경우를 위해 winutils.exe, hadoop.dll을 HADOOP_HOME\\bin에 저장하고 설정한다.\n",
    "- 코드 myConf=pyspark.SparkConf(): SparkConf 객체를 생성하고, 곧 이어 코드 ```.config(conf=myConf)```에 전달하면서 myConf 설정(여기서는 아무런 설정도 수행하지 않았으니, 생략해도 된다).\n",
    "- 코드 ```SparkSession.builder``` 생성자를 호출하여 SparkSession을 생성한다.\n",
    "- 코드 ```.master``` 함수를 사용하여 클러스터 매니저를 지정, ```local```은 Spark를 로컬에서 실행한다는 의미이다.\n",
    "    (1) 분산의 경우 master URL 또는 (2) 로컬인 경우 ```local[]```라고 적어준다. 즉 local의 수는 CPU core의 수를 의미한다. 예를 들어 ```local[*]```는 가능한 최대한의  core를 사용한다는 의미이다. 예를 들어, local[5]라고 하면, core의 수가 2개라고 하더라도 데이터는 5개의 partitions로 나누어져 주어진다.\n",
    "    * ```local```은 Spark를 로컬에서 실행한다는 의미이다.\n",
    "    * ```local[n]```는 worker의 쓰레드를 n개로 한다는 의미. CPU core의 개수에 맞추어 설정하자.\n",
    "    * ```local[*]``` 는 가능하면 가용한 모든 쓰레드를 사용한다는 의미 (Runtime.getRuntime.availableProcessors()로 그 수를 알 수 있다)\n",
    "- 코드 ```.appName``` 함수를 사용하여 애플리케이션 이름을 설정하고,\n",
    "- 코드 .config(conf=myConf) 설정된 SparkConf 사용\n",
    "- 코드 ```.getOrCreate()``` 함수를 사용하면 (1) 이미 생성된 SparkSession이 있는 경우에는 해당 객체를 반환하고, (2) 없는 경우에는 새로운 SparkSession을 생성한다. 불필요한 중복 생성을 피하기 위해 자주 적용되는 기법이다. 이 함수는 **singleton 패턴**으로 한 번에 하나의 세션만이 존재하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하면 단말 또는 명령프롬프트에 로그가 출력이 되고 있는데, 이 메시지를 읽고 상태를 파악하는 것은 좋은 습관이며 꼭 해야 한다.\n",
    "\n",
    "(1) PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON를 설정해 주어야 한다.\n",
    "(2) winutils.exe가 없다는 'WARN' (오류가 아니라서 다행이다), 메시지 출력에서 요청하는 HADOOP_HOME을 생성하고 그 파일을 저장해 놓자.\n",
    "\n",
    "```\n",
    "The system cannot find the path specified.\n",
    "Missing Python executable 'c:\\program files\\python39\\python.exe', defaulting to 'C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\bin\\..' for SPARK_HOME environment variable. Please install Python or specify the correct Python executable in PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON environment variable to detect SPARK_HOME safely.\n",
    "\n",
    "23/08/31 15:25:47 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 오류\n",
    "\n",
    "주피터 노트북을 열어놓고 작업하다 컴퓨터를 대기모드로 두면서 세션이 끊기거나 하는 등의 이유로, 멀쩡히 작동하던 SparkSession을 다시 생성하려면 오류가 발생할 수 있다. 그러면 주피터 노트북의 Kernel을 재시작하면 해결된다.\n",
    "\n",
    "- ConnectionRefusedError: [WinError 10061] 네트워크 연결을 시도하는 동안 발생한 오류, Kernel을 재시작해보자.\n",
    "- ```Java gateway process exited before sending its port number``` Python과 Java 간의 통신을 위해 사용하는 Java Gateway 프로세스가 시작되었으나, 해당 프로세스가 포트 번호를 전송하기 전에 비정상적으로 종료되었다는 의미이다. Spark 버전에 따라 지원되는 Java 버전이 다를 수 있고, 그 때 발생할 수 있다. 코드에서는 Spark 2.2를 사용하고 있으나, 현재 훨씬 늦게 발표된 자바 20을 사용하고 있다면 당연히 발생할 수 있는 오류이다. Apache Spark 3.4.1을 사용하려면 Java 8, Java 11 중 하나를 선택하고, Python 3.6 이상의 버전을 사용하는 것이 권장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"21.0.1\" 2023-10-17\n",
      "OpenJDK Runtime Environment (build 21.0.1+12-29)\n",
      "OpenJDK 64-Bit Server VM (build 21.0.1+12-29, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javac 21.0.1\n"
     ]
    }
   ],
   "source": [
    "!javac -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Code\\\\git\\\\bb\\\\jsl\\\\pyds\\\\env3.9\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Code\\\\git\\\\bb\\\\jsl\\\\pyds\\\\env3.9\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYSPARK_DRIVER_PYTHON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\Code\\\\git\\\\bb\\\\jsl\\\\pyds\\\\env3.9\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYSPARK_PYTHON']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 오류: spark-warehouse 설정\n",
    "\n",
    "혹시 오류가 발생, 생성되지 못할 수 있다. 그렇다면 현재 작업 디렉토리에 쓰기 권한이 없거나 해서 디렉토리가 생성되지 못할 수 있다. 디렉토리를 생성할 수 있는 권한을 확인하자.\n",
    "\n",
    "\"spark.sql.warehouse.dir\"는 **SparkSQL에서 사용되는 데이터 저장 및 메타데이터 관리**를 위한 내부적으로 사용되는 디렉토리이다.\n",
    "\n",
    "- spark-warehouse는 Spark를 실행하면 **현재 작업 디렉토리 아래 자동으로 생성**된다.\n",
    "\n",
    "- 또는 ```/conf/hive-site.xml``` 파일에 spark.sql.warehouse.dir를 설정해도 된다.\n",
    "\n",
    "- 또는 다음과 같이 SparkSession을 생성하기 전에 해당 설정을 구성해야 한다. (참고: hdfs 파일형식으로 ```file:///```을 추가)\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"PJT_DIR/myspark-warehouse\")\\\n",
    "    .getOrCreate()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "spark-warehouse 디렉토리는 conf.get() 함수로 알아볼 수 있다. 다음에서 보듯이, ```usr.dir``` 밑에 spark-warehous가 만들어졌다는 의미이다.\n",
    "Java Property에 따르면 ```user.dir```는 사용자 작업디렉토리, user working directory이다. ```user.home```은 홈 디렉토리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/C:/Users/admin/Code/git/bb/jsl/pyds/spark-warehouse'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 명령어 작성하면서 점연산자 이해하기\n",
    "\n",
    "SparkSession을 생성하는 명령어를 보면, 체인처럼 점명령어가 여럿 연결되어 있다.\n",
    "\n",
    "명령어는 복사, 붙여넣기 식으로 하게되면 이해하지 못한다.\n",
    "교재에 나와 있는 명령어를 이해하고, 안보고 혼자 힘으로 해야 한다.\n",
    "코딩을 하면서 지금 무엇을 하는 것인지, 왜 이것을 하는지 알면서 해야 한다.\n",
    "\n",
    "예를 들어:\n",
    "* spark.sparkContext하면, spark를 타이핑하고, spark가 무엇인지 생각함. spark는 SparkSession을 말함.\n",
    "* spark하고 점을 누르고 ```spark.```\n",
    "* 점 다음에는 <TAB>키, 그러면 지원하는 명령어가 드롭다운으로 나옴.\n",
    "* spark는 sparkContext를 지원한다. sparkContext는 RDD를 사용할 때 쓰는 명령어\n",
    "* 그리고 spark.sparkContext. 그리고 파일을 읽으니까, textFile 이런 식으로\n",
    "\n",
    "이런 식으로 무엇을 하는지, 왜 이렇게 하는지 이해하면서 프로그래밍 해야 한다.\n",
    "그냥 쓱 명령어를 눈으로 보거나, 복붙하면 향상이 느릴 수 밖에 없다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.3 데이터 구조\n",
    "\n",
    "Spark에서는 RDD, Dataframe, DataSet 세 가지 데이터구조를 제공하고 있다. 이 가운데 **RDD API에 대한 지원은 축소**되고 있다.\n",
    "* **RDD**는 Spark 1.0부터 사용된 기본 구조이고, 이를 기반으로 다른 데이터구조가 만들어졌다. RDD는 데이터가 **비구조적**인 경우 사용하기 적합하다. 모델schema를 정하지 않고 사용할 수 있다.\n",
    "* **Dataframe**은 버전 1.3에서 제공되어 많이 쓰이고 있다. 행x열의 테이블과 비슷하고, DataFrame과 DataSet은 데이터가 schema와 데이터타입을 가진 **구조적**인 경우 사용한다.\n",
    "* **DataSet**은 그 후 1.6부터 제공하고 있다.\n",
    "\n",
    "데이터구조 | 언제 Spark에 도입 | 설명\n",
    "---------|---------|---------\n",
    "**RDD** | 1.0 | **비구조적**, schema 정의가 필요없다, low-level\n",
    "**Dataframe** | 1.3 | **구조적**, schema가 있고, 행열이 있는 SQL 테이블과 유사한 구조이다. Dataset[Row]와 같은 의미로, 타입을 강제하지 않는다.\n",
    "**Dataset** | 1.6 | 자바의 Generic과 같이 Dataset[T]으로 '타잎'을 강제하는 형식이다. Scala와 Java에서 주로 사용되며, Python에서는 DataSet을 명시적으로 사용할 수 없다\n",
    "\n",
    "\n",
    "* Spark의 RDD, DataFrame 모두 **immutable**이라 일단 생성되고 나면 원본을 수정할 수 없다.\n",
    "* Spark의 데이터는 모두 **lazy**, 실제 transformation을 action까지 연기한다. **변환할 때마다 실제 변환이 일어나면 그 결과가 메모리에 저장되는 비효율성**을 막기 위해, **action이 실행되는 경우, 계산이 이루어지고, 실제 메모리를 사용**한다. RDD의 경우, action이 실행될 때마다 재계산이 이루어지는 것을 막기 위해 persist (or cache)함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.4 RDD 소개\n",
    "\n",
    "RDD(Resilient Distributed Dataset)는 Apache Spark에서 데이터를 표현하는 기본 구조이다.\n",
    "\n",
    "그 줄임말에서 알 수 있듯이 **장애 복구가 가능한 분산 DataSet**이며, 다음과 같은 특성이 있다.\n",
    "\n",
    "* 불변성: 생성후 변경할 수 없다.\n",
    "* 병렬처리: RDD는 여러 파티션으로 나누어져 있으며, 각 파티션은 클러스터의 여러 노드에서 병렬로 처리됩니다. 파티션은 데이터를 분산 및 병렬 처리하기 위한 기본 단위이며, 사용자는 파티션 수를 제어하거나 파티션을 사용자 정의할 수 있습니다.\n",
    "* 장애 복구 - \n",
    "RDD 변환 연산의 로그에 따라 필요한 변환 연산을 재실행하여 누락된 데이터 파티션을 다시 생성할 수 있고, 이를 통해 작업 중단으로 인해 손실된 데이터가 복원될 수 있다.\n",
    "* 타입 지정하는 데이터 구조\n",
    "* 지연 연산: 변환 연산은 선언만 되고 실제로 실행되지 않고 지연된다.\n",
    "\n",
    "RDD는 Python List, 파일, hdfs 등 다양한 자료에서 생성할 수 있고, 생성된 자료는 수정할 수 없는 read-only이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 RDD 생성\n",
    "\n",
    "RDD는 **sparkContext**로부터 만들어 진다.\n",
    "* 1) 이미 만들어진 배열과 같에서 읽어서 생성한다. 이 경우 parallelize() 함수를 사용하게 된다.\n",
    "* 2) 또는 파일, 데이터베이스 등 외부에서 읽어서 생성할 수도 있다. textFile() 함수를 사용한다.\n",
    "\n",
    "생성 방법 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "내부에서 읽기 | Pytho list에서 생성 | parallelize()\n",
    "외부에서 읽기 | 파일, HDFS, HBase 등 | ```textFile(\"mydir/\")```<br>```textFile(\"mydir/*.txt\")```<br>```textFile(\"mydir/*.gz\")```<br>```Hadoop InputFormat```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### List에서 RDD 생성하기\n",
    "\n",
    "```sparkContext.parallelize()``` 함수를 사용하여 Python list에서 RDD를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD를 화면에 출력하기 위해서는, collect() 또는 take(출력개수) 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(3) #Py4JJavaError가 나면 PSPARK_PYTHON, PYSPARK_DRIVER_PYTHON 경로 설정해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Py4JJavaError\n",
    "\n",
    "Py4JJavaError는 PySpark와 Python 간의 상호작용에서 발생하는 오류로, 환경 변수 설정 문제나 Spark의 초기화 과정에서 발생할 수 있다.\n",
    "\n",
    "해결방안: PYSPARK_PYTHON 및 PYSPARK_DRIVER_PYTHON 환경 변수를 정확히 설정하고, 커널을 재시작한다. Jupyter Notebook에서 환경 변수나 경로 설정을 변경했을 때, 또는 새로운 라이브러리를 설치한 경우, 변경을 적용하려면 Jupyter Notebook의 커널을 재시작해야 한다. 그렇지 않으면, 이전 설정이 계속 유지되고 새로운 변경 사항이 반영되지 않을 수 있다. 그리고 pyspark.sql.SparkSession을 재생성하면 된다.\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take() 또는 collect() 출력하면서 오류가 발생하는 경우에는 os.environ['PYSPARK_PYTHON'], os.environ['PYSPARK_DRIVER_PYTHON']을 설정하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([0, 2, 3, 4, 6], 2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: collect() 함수는 모든 파티션의 데이터를 수집하여 로컬의 리스트로 반환하는 액션(Action) 함수이다. 강조하자면 분산환경의 데이터를 로컬로 내려온다는 점이다. 대규모 데이터에 대해 collect()하면 메모리 부하가 걸릴 수 있으니, take() 함수를 사용하여 일부만 가져오는 편이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 파티션 partition\n",
    "\n",
    "> 파티션이란 논리적인 데이터 분할, 즉 일정한 크기로 잘라놓은 데이터 뭉치이다.\n",
    "따라서 RDD는 파티션으로 구성되어 있다고 할 수 있다.\n",
    "파티션의 수는 원하는 숫자만큼 구성할 수 있다.\n",
    "HDFS를 사용하면 블록마다 (HDFS의 블록크기는 보통 128MB) 파티션을 만든다.\n",
    "이렇게 데이터를 분할해 놓으면 동시에 여러 파티션을 나누어 여러 노드/스레드에서 병렬처리가 가능해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6, 7]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1,2,3,4,5,6,7], 2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: ```parallelize([1,2,3,4,5,6,7], 2)```와 같이 파티션을 2개로 분할하고, glom()은 파티션을 배열로 그룹화하는 함수이다. collect()하면 ```[[1, 2, 3], [4, 5, 6, 7]]``` 파티션 2개에 대해서 만들어진 배열 2개를 볼 수 있다. 이와 같이 glom()은 대량의 데이터에 대해 메모리 부하를 줄이기 위해 파티션 단위로 나누어 처리할 경우 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일에서 RDD 생성하기\n",
    "\n",
    "파일에서 직접 RDD를 생성해 본다.\n",
    "현재 작업 디렉토리 아래에 **'data/' 디렉토리**를 만들고 아래 파일을 생성한다.\n",
    "파일 내용은 wikipedia에서 Apache spark를 검색한 후 첫 문단을 복사해서 가져 왔다.\n",
    "일부러 3째줄은 한글, 4째 줄은 같은 단어를 반복해서 추가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일에서 RDD를 생성하기 위해서는 앞서와 같이 SparkContext를 사용한다.\n",
    "파일명을 textFile() 함수 인자로 넣어서 만들어 주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```first()```는 첫 데이터만 조회하는 action함수이다.\n",
    "first()은 take(1)과 동일한 결과를 출력하는데, 그 이유는 first()는 내부적으로 take(1) 함수를 사용하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wikipedia'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RDD와 Spark Dataframe를 만드는 함수는 서로 다르다\n",
    "\n",
    "DataFrame은 다음 장에서 배우게 되겠지만, file에서 읽는 방식이 RDD와 Dataframe이 서로 다르다.\n",
    "RDD는 sparkContext.textFile(), Dataframe은 read.text()을 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "SparkSession.sparkContext.textFile() | **'SparkContext'를 사용하므로 RDD를 생성**한다.\n",
    "SparkSession.read.text() | **DataFrame을 생성**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "print (myDf.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "읽고 생성된 변수 myDf의 데이터타입을 type()으로 확인하면 DataFrame이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print (type(myDf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### csv에서 RDD 생성하기\n",
    "\n",
    "csv 파일은 컴마로 구분된 데이터를 저장하고 있다. 이 파일을 읽어서 RDD를 생성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "파일에서 읽어 RDD를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```take()```를 하면 그 결과는 리스트가 된다.\n",
    "아래에서 보듯이 파일의 각 라인이 묶여서 리스트의 한 요소로 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "\n",
    "print(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 2개씩 문자열로 묶여져 있다. 이들을 더해보자. '35, 2'를 더해서 37을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37,\n",
       " 67,\n",
       " 50,\n",
       " 46,\n",
       " 22,\n",
       " 33,\n",
       " 47,\n",
       " 44,\n",
       " 31,\n",
       " 49,\n",
       " 18,\n",
       " 33,\n",
       " 34,\n",
       " 49,\n",
       " 33,\n",
       " 23,\n",
       " 15,\n",
       " 68,\n",
       " 56,\n",
       " 27]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.map(lambda x: x.split(\",\"))\\\n",
    "    .map(lambda x: int(x[0])+int(x[1]))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: '35, 2' ---split() 하면---> '35' '2'로 분리하고 리스트 ```['35' '2']```에 담긴다.\n",
    "- L2: '35' '2' ---정수 형변환---> 35 2이 되고 비로서 연산이 가능해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 파일에서 RDD 생성\n",
    "\n",
    "다음 링크에서 파일을 읽어서 RDD를 생성하고, 5줄을 화면출력하세요.\n",
    "\n",
    "* 1) 경기도 의정부시 인구현황 (파일명: ```경기도 의정부시_인구현황_20230731```)\n",
    "https://www.data.go.kr/data/15009613/fileData.do\n",
    "\n",
    "* 2) 제주특별자치도 서귀포시 내 연도별 65세이상 인구수 및 고령화비율, 노령화지수 현황 (파일명: ```제주특별자치도 서귀포시_고령화비율및노령화지수현황_20200623```)\n",
    "https://www.data.go.kr/data/15051545/fileData.do\n",
    "\n",
    "* 파일을 읽을 경우, 문자가 한글인지, 영어인지 어떻게 인코딩되었는지 주의해야 한다. 결과가 깨져보인다면, 왜 그런지 이유를 적어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession 생성\n",
    "\n",
    "문제를 풀기 위해, SparkSession을 생성하고 있다. 주의! 한 번 만들어지면 메모리에서 해제되기까지는 존속하니, 세션 동안 SparkSession은 1회만 생성하면 된다. 이미 생성되어 있다면 또 할 필요가 없다는 뜻이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "#os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "popRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"경기도 의정부시_인구현황_20240930.csv\"), use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['�������,�α���(��),�α���(��),�α���(��),������(��),������(��),������(��),����,�����,������α�,���������,�����μ���,�μ���ȭ��ȣ,�����ͱ�������',\n",
       " '������1��,39567,20025,19542,8.60,4.35,4.25,102.47,23371,1.69,��\\u2d75 �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30',\n",
       " '������2��,29644,14758,14886,6.44,3.21,3.23,99.14,16051,1.85,��\\u2d75 �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30',\n",
       " 'ȣ��1��,34059,16442,17617,7.40,3.57,3.83,93.33,15178,2.24,��\\u2d75 �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30',\n",
       " 'ȣ��2��,32529,15643,16886,7.07,3.40,3.67,92.64,13272,2.45,��\\u2d75 �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�������,�α���(��),�α���(��),�α���(��),������(��),������(��),������(��),����,�����,������α�,���������,�����μ���,�μ���ȭ��ȣ,�����ͱ�������\n",
      "������1��,39567,20025,19542,8.60,4.35,4.25,102.47,23371,1.69,��⵵ �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30\n",
      "������2��,29644,14758,14886,6.44,3.21,3.23,99.14,16051,1.85,��⵵ �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30\n",
      "ȣ��1��,34059,16442,17617,7.40,3.57,3.83,93.33,15178,2.24,��⵵ �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30\n",
      "ȣ��2��,32529,15643,16886,7.07,3.40,3.67,92.64,13272,2.45,��⵵ �����ν�û,�ο����ǰ�,031-828-2466,2024-09-30\n"
     ]
    }
   ],
   "source": [
    "for i in popRdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "화면에 출력하면 한글이 깨져있다. ```use_unicode=True```설정을 주었는데도 그렇다.\n",
    "다운로드 받으면서 한글이 깨져 있기 때문에 그렇다. 다운로드 받은 파일을 수정해서 출력하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agedRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"제주특별자치도 서귀포시_고령화비율및노령화지수현황_20240419.csv\"), use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�⵵,��,�������� �α���,65���̻� �α��� ,14������ �α���,���ȭ����,���ȭ����,�����ͱ�������\n",
      "2008,12,153120,22241,26792,14.53,83.01,2024-04-19\n",
      "2009,12,152285,23031,25504,15.12,90.30,2024-04-19\n",
      "2010,12,153716,23990,24633,15.61,97.39,2024-04-19\n",
      "2011,12,153366,24839,23686,16.20,104.87,2024-04-19\n"
     ]
    }
   ],
   "source": [
    "for i in agedRdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "헤더를 제외하고 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008,12,153120,22241,26792,14.53,83.01,2024-04-19\n",
      "2009,12,152285,23031,25504,15.12,90.30,2024-04-19\n",
      "2010,12,153716,23990,24633,15.61,97.39,2024-04-19\n",
      "2011,12,153366,24839,23686,16.20,104.87,2024-04-19\n"
     ]
    }
   ],
   "source": [
    "for i in agedRdd.take(5)[1:]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(컬럼2 - 컬럼3 - 컬럼4) 연산을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2008', '12', '153120', '22241', '26792', '14.53', '83.01', '2024-04-19', 104087]\n",
      "['2009', '12', '152285', '23031', '25504', '15.12', '90.30', '2024-04-19', 103750]\n",
      "['2010', '12', '153716', '23990', '24633', '15.61', '97.39', '2024-04-19', 105093]\n",
      "['2011', '12', '153366', '24839', '23686', '16.20', '104.87', '2024-04-19', 104841]\n"
     ]
    }
   ],
   "source": [
    "for i in agedRdd.take(5)[1:]:\n",
    "    j=i.split(\",\")\n",
    "    j.append(int(j[2])-int(j[3])-int(j[4]))\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: ```[1:]```은 행1 (두번째 행이다) 다음부터 출력한다.\n",
    "- L2: ```['2008', '12', '153120', '22241', '26792', '14.52', '83.01', '2023-03-24']```가 된다.\n",
    "- L3: 문자열을 정수형변환하고 연산한다 (첫번째 행의 경우 153120 - 22241 - 26792 = 104087). 그 결과를 리스트 끝에 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binaryFiles\n",
    "\n",
    "binaryFiles()는 이진파일을 읽는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "popRddBin = spark.sparkContext.binaryFiles(os.path.join(\"data\",\"경기도 의정부시_인구현황_20240930.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "_my = popRddBin.map(lambda x :x[1].decode('euc-kr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['행정기관,인구수(계),인구수(남),인구수(여),구성비(계),구성비(남),구성비(여),성비,세대수,세대당인구,관리기관명,관리부서명,부서전화번호,데이터기준일자\\r\\n의정부1동,39567,20025,19542,8.60,4.35,4.25,102.47,23371,1.69,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n의정부2동,29644,14758,14886,6.44,3.21,3.23,99.14,16051,1.85,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n호원1동,34059,16442,17617,7.40,3.57,3.83,93.33,15178,2.24,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n호원2동,32529,15643,16886,7.07,3.40,3.67,92.64,13272,2.45,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n장암동,18741,8923,9818,4.07,1.94,2.13,90.88,8235,2.28,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n신곡1동,39747,19449,20298,8.64,4.23,4.41,95.82,17136,2.32,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n신곡2동,44657,21531,23126,9.70,4.68,5.02,93.10,18826,2.37,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n송산1동,30633,14946,15687,6.66,3.25,3.41,95.28,13469,2.27,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n송산2동,31499,15582,15917,6.84,3.39,3.46,97.90,13099,2.40,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n송산3동,44981,21785,23196,9.77,4.73,5.04,93.92,17938,2.51,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n자금동,25369,12403,12966,5.51,2.69,2.82,95.66,11739,2.16,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n가능동,23839,11916,11923,5.18,2.59,2.59,99.94,11966,1.99,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n흥선동,18905,9653,9252,4.11,2.10,2.01,104.33,9500,1.99,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n녹양동,19555,9681,9874,4.25,2.10,2.15,98.05,9081,2.15,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n고산동,26559,13295,13264,5.77,2.89,2.88,100.23,10674,2.49,경기도 의정부시청,민원여권과,031-828-2466,2024-09-30\\r\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_my.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD binaryFiles로 읽으니, 파일의 전체내용을 하나의 값으로 읽을 뿐만 아니라,\n",
    "2차원 배열로 읽어도 행렬의 구조가 없어서 이해하기 어렵다.\n",
    "인덱스를 변경해서 데이터를 읽어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---00:  행정기관,인구수(계),인구수(남),인구수(여),구성비(계),구성비(남),구성비(여),성비,세대수,세대당인구,관리기관명,관리부서명,부서전화번호,데이터기준일자\n",
      "---01:  의정부1동,39567,20025,19542,8.60,4.35,4.25,102.47,23371,1.69,경기도\n"
     ]
    }
   ],
   "source": [
    "popList = _my.map(lambda x: x.split()).take(3)\n",
    "print(\"---00: \", popList[0][0])\n",
    "print(\"---01: \", popList[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 더 알아보기: 한글 인코딩\n",
    "\n",
    "> 한글 문자를 인코딩하기 위해 사용되는 방식은 몇 가지 있다.\n",
    "\n",
    "> * euc-kr (Extended Unix Code-KR): 유닉스/리눅스 시스템에서 주로 사용되는 방식이고, ASCII 문자를 1바이트로, 한글문자를 2바이트로 표현한다.\n",
    "> * cp949 (Code Page 949): 윈도우에서 주로 사용되는 방식이고, \"euc-kr\"과 유사하지만, 일부 문자 코드가 다를 수 있다.\n",
    "> * UTF-8 (Unicode Transformation Format - 8 bit): 국제적으로 사용되는 방식, 1~4바이트의 가변크기를 사용하고 크기에 따라 UTF-16, UTF-32가 있다.\n",
    "\n",
    "> 어떤 방식을 사용하느냐는 주로 시스템 설정과 사용하는 어플리케이션에 따라 다르다. 보통 국제적인 UTF-8을 적용하는 것이 좋지만, 특히 시간이 좀 지난 경우 \"euc-kr\"이나 \"cp949\"도 적용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "데이터를 읽어서 DataFrame을 만드려면 read() 함수를 사용한다.\n",
    "폰트를 변경하거나 ```option(\"charset\", \"euc-kr\")```, 헤더를 읽을지 ```option(\"header\", \"true\")``` 설정을 변경할 수 있어 RDD보다 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "popDf = spark\\\n",
    "            .read.option(\"charset\", \"euc-kr\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(os.path.join(\"data\",\"경기도 의정부시_인구현황_20240930.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+-----------------+----------+------------+--------------+\n",
      "| 행정기관|인구수(계)|인구수(남)|인구수(여)|구성비(계)|구성비(남)|구성비(여)|  성비|세대수|세대당인구|       관리기관명|관리부서명|부서전화번호|데이터기준일자|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+-----------------+----------+------------+--------------+\n",
      "|의정부1동|     39567|     20025|     19542|      8.60|      4.35|      4.25|102.47| 23371|      1.69|경기도 의정부시청|민원여권과|031-828-2466|    2024-09-30|\n",
      "|의정부2동|     29644|     14758|     14886|      6.44|      3.21|      3.23| 99.14| 16051|      1.85|경기도 의정부시청|민원여권과|031-828-2466|    2024-09-30|\n",
      "|  호원1동|     34059|     16442|     17617|      7.40|      3.57|      3.83| 93.33| 15178|      2.24|경기도 의정부시청|민원여권과|031-828-2466|    2024-09-30|\n",
      "|  호원2동|     32529|     15643|     16886|      7.07|      3.40|      3.67| 92.64| 13272|      2.45|경기도 의정부시청|민원여권과|031-828-2466|    2024-09-30|\n",
      "|   장암동|     18741|      8923|      9818|      4.07|      1.94|      2.13| 90.88|  8235|      2.28|경기도 의정부시청|민원여권과|031-828-2466|    2024-09-30|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+-----------------+----------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "popDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agedDf = spark\\\n",
    "            .read.option(\"charset\", \"euc-kr\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(os.path.join(\"data\",\"제주특별자치도 서귀포시_고령화비율및노령화지수현황_20240419.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|년도| 월|서귀포시 인구수|65세이상 인구수 |14세이하 인구수|고령화비율|노령화지수|데이터기준일자|\n",
      "+----+---+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|2008| 12|         153120|           22241|          26792|     14.53|     83.01|    2024-04-19|\n",
      "|2009| 12|         152285|           23031|          25504|     15.12|     90.30|    2024-04-19|\n",
      "|2010| 12|         153716|           23990|          24633|     15.61|     97.39|    2024-04-19|\n",
      "|2011| 12|         153366|           24839|          23686|     16.20|    104.87|    2024-04-19|\n",
      "|2012| 12|         154057|           25826|          22861|     16.76|    112.97|    2024-04-19|\n",
      "+----+---+---------------+----------------+---------------+----------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agedDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark-submit 실행\n",
    "\n",
    "위 프로그램을 ```.py```로 저장하고, ```spark-submit``` 명령으로 배치 실행을 할 수 있다.\n",
    "\n",
    "```#!``` 코드를 실행할 프로그램의 경로, 아래에서는 ```/usr/bin/env python3```이라고 명시할 수 있다.\n",
    "리눅스에서는 ```#``` 샵 (Sharp), ```!```는 뱅 (bang), 합쳐서 ```shebang```이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds3_popCsvRead.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds3_popCsvRead.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    print (\"---------RESULT-----------\")\n",
    "    popDf = spark\\\n",
    "                .read.option(\"charset\", \"euc-kr\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(os.path.join(\"data\",\"경기도 의정부시_인구현황_20240930.csv\"))\n",
    "    popDf.show(5)\n",
    "    agedDf = spark\\\n",
    "                .read.option(\"charset\", \"euc-kr\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(os.path.join(\"data\",\"제주특별자치도 서귀포시_고령화비율및노령화지수현황_20240419.csv\"))\n",
    "    agedDf.show(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "    #os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python3.9 경로 설정이 되어 있으면 spark-submit도 실행하는데 문제가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "!spark-submit src/ds3_popCsvRead.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---------RESULT-----------\n",
    "+---------+----------+----------+----------+----------+----------+----------+-------+------+----------+-----------------+----------+------------+--------------+\n",
    "| 행정기관|인구수(계)|인구수(남)|인구수(여)|구성비(계)|구성비(남)|구성비(여)|   성비|세대수|세대당인구|       관리기관명|관리부서명|부서전화번호|데이터기준일자|\n",
    "+---------+----------+----------+----------+----------+----------+----------+-------+------+----------+-----------------+----------+------------+--------------+\n",
    "|의정부1동|    37515 |    19039 |    18476 |     8.07 |     4.10 |     3.98 |103.05 |22508 |     1.67 |경기도 의정부시청|민원여권과|031-828-2466|    2023-07-31|\n",
    "|의정부2동|    29759 |    14831 |    14928 |     6.40 |     3.19 |     3.21 | 99.35 |15985 |     1.86 |경기도 의정부시청|민원여권과|031-828-2466|    2023-07-31|\n",
    "|  호원1동|    34709 |    16793 |    17916 |     7.47 |     3.61 |     3.85 | 93.73 |15313 |     2.27 |경기도 의정부시청|민원여권과|031-828-2466|    2023-07-31|\n",
    "|  호원2동|    33443 |    16109 |    17334 |     7.20 |     3.47 |     3.73 | 92.93 |13407 |     2.49 |경기도 의정부시청|민원여권과|031-828-2466|    2023-07-31|\n",
    "|   장암동|    19231 |     9182 |    10049 |     4.14 |     1.98 |     2.16 | 91.37 | 8332 |     2.31 |경기도 의정부시청|민원여권과|031-828-2466|    2023-07-31|\n",
    "+---------+----------+----------+----------+----------+----------+----------+-------+------+----------+-----------------+----------+------------+--------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "+----+---+---------------+----------------+---------------+----------+----------+--------------+\n",
    "|년도| 월|서귀포시 인구수|65세이상 인구수 |14세이하 인구수|고령화비율|노령화지수|데이터기준일자|\n",
    "+----+---+---------------+----------------+---------------+----------+----------+--------------+\n",
    "|2008| 12|         153120|           22241|          26792|     14.52|     83.01|    2023-03-24|\n",
    "|2009| 12|         152285|           23031|          25504|     15.12|      90.3|    2023-03-24|\n",
    "|2010| 12|         153716|           23990|          24633|      15.6|     97.38|    2023-03-24|\n",
    "|2011| 12|         153366|           24839|          23686|      16.2|    104.86|    2023-03-24|\n",
    "|2012| 12|         154057|           25826|          22861|     16.76|    112.97|    2023-03-24|\n",
    "+----+---+---------------+----------------+---------------+----------+----------+--------------+\n",
    "only showing top 5 rows\n",
    "```\n",
    "\n",
    "로그가 출력되면서 결과가 출력되고 있다.\n",
    "작업이 진행되면서 임시파일이 만들어지는데, 완료되면서 그 파일을 지우면서 오류가 발생하고 있다 (ERROR ShutdownHookManager: Exception while deleting Spark temp dir), 심각한 오류가 아니므로 그냥 둔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S.6 RDD API\n",
    "\n",
    "앞서 RDD를 생성하여 보았다.\n",
    "이제는 RDD라는 데이터구조에서 데이터를 읽고 변환하고 분석하여 보자.\n",
    "RDD는 **데이터 변환Transformations**, **연산Actions**으로 구분할 수 있다.\n",
    "다음에 배우게 될 **Dataframe**의 **Transformer**, **Estimator**와 비교될 수 있다.\n",
    "\n",
    "### 변환 **transformations**\n",
    "\n",
    "RDD를 변형하고 가공하는 함수이고, 변환 결과는 RDD 또는 seq(RDD)로 만들어진다.\n",
    "\n",
    "변환함수는 **lazy연산**을 한다. 실제 변환은 action이 수행되는 시점, 출력이 필요할 때까지 늦추어져서 이루어진다.\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```map(fn)``` | 요소별로 함수 fn을 적용하여 새로운 RDD를 생성해서 돌려줌 | ```.map(lambda x: x.split(' ')```\n",
    "```filter(fn)``` | 요소별로 선별하여 함수 fn을 적용해서 결과 RDD 돌려줌 | ```.filter(lambda x: \"Spark\" in x)```\n",
    "```flatMap(fn)``` | 요소별로 함수 fn을 적용하고, flat해서 결과 RDD 돌려줌 | ```.flatMap(lambda x: x.split(' '))```\n",
    "```groupByKey()``` | key를 그룹해서 iterator를 돌려줌. |\n",
    "```reduceByKey(fn)``` | 키-값 쌍의 paired RDD에서 키를 기준으로 집단화하고, 각 집단 내에서 함수 fn를 사용하여 값을 줄인다\n",
    "```sortBy(fn)``` | 요소를 함수 fn을 기반으로 정렬하여 새로운 RDD를 생성 |\n",
    "```mapPartitions(fn)``` | 각 파티션에 함수 fn을 적용하여 새로운 RDD를 생성. 파티션 단위로 동작하므로 파티션 간에 데이터를 공유하고 연산을 수행하고, 파티션 수는 유지 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### **actions**\n",
    "\n",
    "RDD를 값으로 변환하는데, 보통 Python 리스트가 생성된다.\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```reduce(fn)``` | 요소별로 fn을 사용해서 줄여서 로컬의 list를 돌려줌 | ```reduce(lambda x,y:x+y)```\n",
    "```collect()``` | 모든 파티션의 요소를 수집하여 로컬의 list로 돌려줌 |\n",
    "```collectAsMap()``` | 키-값 쌍으로 이루어진 RDD를 딕셔너리로 변환하여 반환 | \n",
    "```count()``` | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "```take(n)``` | ```collect()```는 전체이지만, n개만 돌려줌 | ```take(1)```\n",
    "```countByKey()``` | key별 갯수를 세는 함수, dictionary로 반환 | ```countByKey().items()```\n",
    "```foreach(fn)``` | 각 데이터 항목에 함수fn을 적용 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  S.6.1 우선 Python 함수로 해보기\n",
    "\n",
    "#### map()함수 흉내내기\n",
    "\n",
    "map()을 사용하지 않고도 그런 작업이 물론 가능하다. 입력 데이터 각 각에 대해 변환을 적용하면 되기 때문이다.\n",
    "\n",
    "섭씨 데이터를 하나씩 화씨로 변환하는 **c2f()**함수를 만들어 보자. 나중에 **map()함수를 적용하면 for문이 없어도** 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print (c2f([39.2, 36.5, 37.3, 37.8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 결과의 저장에 필요한 list\n",
    "- L3~5: 한 항목씩 읽어 **for문으로 처리하고, 리스트f를 만들어 반환**한다.\n",
    "- L5: 리스트에 항목 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### map() 함수를 사용해 보기\n",
    "\n",
    "Python은 map(), reduce(), filter() 함수를 이미 가지고 있다.\n",
    "Python map()을 사용해 보자. **for문이 사라진다**는 점에 유의한다.\n",
    "반복문이 코드에서 보이지 않지만, map() 내부에서는 실행된다는 점에 유의하자.\n",
    "**map() 함수의 인자는 2개** 이다.\n",
    "\n",
    "* (1) 첫째 인자는 **처리 함수**이고, **함수의 return은 반드시 있어야** 한다,\n",
    "* (2) 두 번째 인자는 **처리하려는 데이터**이다.\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | **map(fn,data)**\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | **filter(fn, data)**\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | **reduce(fn, data)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, [39.2, 36.5, 37.3, 37.8])\n",
    "print (list(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* map lambda 함수의 사용\n",
    "\n",
    "map-reduce 함수에서 자주 사용되므로 잘 이해해야 한다.\n",
    "\n",
    "lambda는 함수를 정의하는 명령어로서 무명 함수이므로 별도의 선언이 필요 없다.\n",
    "\n",
    "함수의 인자를 받아서, 한 줄의 표현식으로 처리하고, 그 결과는 **'return'을 사용하지 않아도 반환**된다. 이런 까닭에 줄 수가 줄어든다.\n",
    "\n",
    "```python\n",
    "lambda 함수인자 : 표현식\n",
    "```\n",
    "\n",
    "단순히 인자에 2를 곱하는 함수 코드가 있다고 하자.\n",
    "\n",
    "```\n",
    "def f(x):\n",
    "    return x * 2\n",
    "```\n",
    "\n",
    "이런 코드에 **lambda함수**를 적용해보자. lamdba함수 'y'를 정의하면 다음과 같다. 같은 기능을 lambda를 사용하지 않은 위 코드와 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y = lambda x : x * 2\n",
    "\n",
    "print (y(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 인자를 2개 가질 수도 있다.\n",
    "x, y인자를 받아 그 합산을 반환하는 lambda 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = lambda x, y : x + y\n",
    "\n",
    "print(x(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map(lambda함수, 입력데이터) 이런 식으로 lambda를 map에 넣어서 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "f=map(lambda c:(float(9)/5)*c + 32, celsius)\n",
    "\n",
    "print(list(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문자열에 map()을 사용해 보자. 단어로 분리되기를 기대하지만, 철자로 분리되고 있다.\n",
    "\n",
    "```str.split(구분자)``` 함수에 구분자를 생략하면 whitespace로 분리한다. whitespace는 공백이나 탭 등의 기호를 말하는 것으로, 문장을 분리해서 단어로 분리할 경우에 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x:x.split(), \"Hello World\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1: 문자열 \"Hello World\"의 하나씩 문자가 map으로 입력된다. 각 철자를 map()함수의 인자로 처리해서 split()한다. 기대했던 \"Hello\", \"World\"로 나누어지지 않는다. 단 \"Hello World\".split()은 단어로 분리해서 \"Hello\" \"World\"를 출력한다. 문자열 전체 \"Hello World\"가 입력되고, 단어로 분리되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 문자열의 리스트를 map해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World'], ['Good', 'Morining']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\", \"Good Morining\"]\n",
    "list(map(lambda x:x.split(), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1: 리스트의 요소 하나씩 map을 하기 때문에. \"Hello World\".split()하므로 단어로 분리된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### filter()\n",
    "\n",
    "filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print (list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### reduce()\n",
    "\n",
    "reduce() 역시 **함수와 데이터 2개의 인자**를 받는다.\n",
    "\n",
    "데이터에 대해 함수를 반복적으로 적용하여 결과 값을 만들게 된다, 즉 [ func(func(s1, s2),s3), ... , sn) ]와 같이 수행한다.\n",
    "\n",
    "아래 예는 1부터 101까지 **두 수 x,y 인자를 반복해서 더한다**는 것이다. x는 부분합계로 y를 계속 저장해 나가는 역할을 하며, 최종 합계에 이르게 된다.\n",
    "\n",
    "단계 | x<p>부분 합계 | y | 함수적용\n",
    "-----|-----|-----|-----\n",
    "1 | 0 | 1 | func(0,1) <--- x는 초기화, y에는 첫번째 값 1을 넣는다\n",
    "2 | 1 | 2 | func(1,2) <-- func(0 + 1, 2) 연산을 수행하고 결과를 x의 값으로 사용하고, y는 다음 수 2를 가져온다.\n",
    "3 | 3 | 3 | func(3,3) <-- func(0 + 1 + 2, 3)\n",
    "4 | 6 | 4 | func(6,4) <-- func(0 + 1 + 2 + 3, 4)\n",
    "...|...|...|...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda x, y: x + y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6.2 RDD 사용하기\n",
    "\n",
    "Python으로도 이런 map, reduce 등을 할 수 있지만 빅데이터에는 적용되기에는 무리이다. 비교적 작은 데이터, 단일 머신에서 적용가능하기 때문이다.\n",
    "\n",
    "이제 RDD로 전환해서 이들 함수를 적용해서, 앞서 설명한 transformation, action 함수를 사용해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### map\n",
    "\n",
    "map()을 사용해서 각 요소를 **제곱**해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[62] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "\n",
    "print (squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 정수 리스트를 입력해서, RDD를 만든다.\n",
    "- L2: RDD를 변환하기 위해 map() 함수를 호출한다. 리스트의 요소들을 하나씩 가져와서 제곱을 한다. map()은 transformation 함수라서, 실제 값은 action 함수가 적용될 때까지 연기되어 계산된다.\n",
    "- L4: map() 함수의 실행한 결과는 RDD이고, 출력하면 PythonRDD라고 표시된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환의 실제 결과를 보려면 collect()를 사용해서 출력해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print (squared.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열을 정수로 변환\n",
    "\n",
    "위 csv 파일을 읽어서 myRdd4를 생성하였다.\n",
    "\n",
    "파일에서 가져오는 데이터는 정수나 소수의 숫자데이터라 하더라도 문자열로 읽힌다.\n",
    "\n",
    "myRDD를 take(5) 함수로 5개 읽어보면, 문자 타입이라는 것을 금세 알 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35, 2', '40, 27', '12, 38', '15, 31', '21, 1']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자열 리스트 ['35, 2', '40, 27', '12, 38', '15, 31', '21, 1'] ---> 리스트의 정수리스트 [[35, 2], [40, 27], [12, 38], [15, 31], [21, 1]] 형식으로 변환하려고 한다.\n",
    "\n",
    "이 RDD의 문자열 요소를 정수로 변환하기 위해서는, map()함수가 적합하다.\n",
    "단 리스트의 구성은 그대로 유지하도록 하자.\n",
    "\n",
    "map() 함수에는 아래와 같은 로직이 숨어있다.\n",
    "\n",
    "```python\n",
    "모든 줄을 반복:\n",
    "    한 줄line을 읽는다. ('35, 2' 첫 반복에서 문자열)\n",
    "    줄line을 컴마(,)로 분리한다. ('35', ' 2' 컴마로 분리했으므로 2앞에 공백이 없어지지 않고 남아있다)\n",
    "    줄line을 리스트로 만든다. (['35', ' 2'] split()은 문자열을 분리해서 리스트로 만든다. 형변환하지 않으면 문자열이 유지된다)\n",
    "```\n",
    "\n",
    "map() 함수에 lambda를 넣어서 해보자.\n",
    "line을 받아서 split(,) 즉 컴마로 분리하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열을 정수로 변환\n",
    "\n",
    "결과에서 보듯이, 리스트 안의 숫자들이 따옴표로 되어 있다.\n",
    "이는 숫자가 아니라 문자라는 의미이다. 문자는 + 연산을 하면 아래와 같이 합성이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'352'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'35'+'2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뭔가 연산을 하려면, 숫자를 정수로 형변환을 해야 한다.\n",
    "우선 리스트의 첫 째 요소 ['35', '2']를 Python으로 형변환 해보자.\n",
    "이 경우 반복문으로 하나씩 읽어서 문자를 int()로 형변환을 해야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 반복문으로 해보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 2]\n"
     ]
    }
   ],
   "source": [
    "x=['35', ' 2']\n",
    "y=list()\n",
    "for i in x:\n",
    "    y.append(int(i))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- L1: 문자열 리스트이다. 2앞에 공백이 있다.\n",
    "- L2: 결과를 저장하기 위한 리스트\n",
    "- L4: 정수로 변환하고, 리스트에 append() 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 줄여서 아래와 같이 처리하는데, 프로그램이 줄 수가 줄어들어 간략해 지면서 Python의 특징이 잘 드러나고 있다.\n",
    "이를 Pythonoic하다라고 한다.\n",
    "\n",
    "유의해야 할 코드에 쓰인 단축 표현 (Pythonic한 표현)\n",
    "- for문과 실행문을 한 줄로 단축 표현\n",
    "- for문의 결과를 ```[]``` 괄호로 감싸서 리스트로 결과를 만드는 단축 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 2]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(i) for i in ['35', ' 2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 되는 것을 확인했으니, map() 함수에 넣어서 정수로 만들자. 반복문이 사라지는 것을 재확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 2], [40, 27], [12, 38], [15, 31], [21, 1]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd6 = myRdd5.map(lambda x: [int(i) for i in x])\n",
    "myRdd6.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어 분리\n",
    "\n",
    "map 함수를 사용해서 문서를 문장으로 분리해 보자.\n",
    "문서파일이 10개 문장을 포함하고 있으므로, count()는 10개를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences=myRdd2.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력을 해보면 곧 알게 되겠지만, split()한 결과는 문장일까? 단어일까? 그 개수를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용자 함수를 이용해서 단어 분리\n",
    "\n",
    "사용자 함수, 즉 사용자가 만든 mySplit() 함수를 사용해 map()을 수행하고 있다.\n",
    "이와 같이 lambda를 사용하지 않고, 사용자함수로 map()을 사용할 수 있다.\n",
    "lambda는 한 줄의 명령문만 가지게 되므로, 여러 명령문으로 함수를 만드는 경우 사용자함수를 만들어 유용하게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mySplit(x):\n",
    "    return x.split()\n",
    "\n",
    "sentences2=myRdd2.map(mySplit)\n",
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L4: map()의 인자에 함수를 적고 있다. 요소 하나씩 가져오고 ---> mySplit()을 호출하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take() 함수를 사용해 수집해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1:  take()의 인자 3을 넣어 갯수를 정하고 있다. 리스트에 담겨져 있으므로, 이를 풀어서 출력하기 위해서는 for문을 사용해야 한다.\n",
    "리스트에 담겨져 있는 데이터를 살펴보면, 단어로 분리되어 있는 것을 알 수 있다. 앞서 문장? 단어? 라는 질문의 답을 이제 해보자. map()은 파일의 한 줄씩 문장을 읽는다. 그 문장을 ```.split()```해서 단어로 분리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 중첩 for 문으로 단어 하나씩 출력\n",
    "\n",
    "출력하려는 대상은 리스트의 단어 리스트, 2차원 배열이다. 이를 출력하려면 for문 하나가 아니라 중첩이 필요하다. 기억하자 리스트의 차원과 반복문의 회차는 같아야 작동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.collect():\n",
    "    for word in line:\n",
    "        print (word, end=\" \")\n",
    "    print (\"\\n-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1~2: sentences를 수집 collect()하고 중첩반복한다. **for문을 중첩**하는 이유는, 앞서 본 경우와 같이 리스트에는 **문장**이 분리된 **단어**가 요소로 담겨져 있으므로(2차원), 단어를 출력하기 위해서이다.\n",
    "- L2: 문장에 대한, 즉 단어를 하나씩 처리하기 위한 반복.\n",
    "- L3: Python3에서는 print()가 함수로 취급된다. 단어를 출력할 경우, 그 사이에 공백을 넣고 싶으면 end= ' ' 라고 적어주어야 한다. 따옴표는 한 개 '' 또는 \"\" 어느 것으로 해도 기능의 차이는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자 개수\n",
    "\n",
    "각 문장의 철자 갯수를 세어 보자.\n",
    "철자는 len()함수를 사용하면 된다.\n",
    "첫 문장 'Wikipedia'는 **9**, 다음 문장 'Apache Spark is an open source cluster computing framework.'는 마침표를 포함하여 **59**자를 출력하고 있다.\n",
    "아래는 58을 출력하는데, 자세히 살펴보면 마침표가 빠져있어서 그렇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Apache Spark is an open source cluster computing framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 문장의 철자 개수를 세어보고, ```.collect()``` 함수로 결과를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 교체\n",
    "\n",
    "이번에는 리스트에서 RDD를 만들어 간단한 문자처리 기능으로 **대소문자 변환**이나 **교체**를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is', 'a line']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"this\",\"This\"))\n",
    "repRdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 대소문자 변환\n",
    "\n",
    "첫 글자를 대문자로 만들어서 출력해 보자.\n",
    "다음 's'.upper()는 철자 's'를 대문자로 출력하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print (wordsRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print (upperRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 리스트의 0번째는 'this' 'a'가 해당되고, 이 단어들을 대문자로 변환하고 있다. 이들 단어에 해당되지 않으면 제거된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리스트의 모든 단어를 대문자로 바꾸려면 for문을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2RDD =wordsRdd.map(lambda x: [i.upper() for i in x])\n",
    "print (upper2RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### reduce\n",
    "\n",
    "앞서 RDD를 사용하지 않고, Python reduce()한 예와 비교해 보자.\n",
    "reduce()는 lamdba함수를 사용해서 **입력 데이터를 하나씩 서로 더해서 x+y** 결과 값을 만들어 낸다.\n",
    "\n",
    "```python\n",
    "subtotal = 0으로 초기화,\n",
    "subtotal = subtotal + 1 (y는 1이므로)\n",
    "subtotal = subtotal + 2 (y는 하나 증가해서 2이므로)\n",
    "...\n",
    "subtotal = subtotal + 100 (y는 하나씩 증가해서 최종 값 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100.reduce(lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fold\n",
    "\n",
    "fold() 함수는 초기값을 설정하는 것을 제외하고는 reduce()와 비슷한 기능을 수행한다.\n",
    "\n",
    "먼저 파티션을 펼쳐서 (unfold), 초기값을 파티션별로 적용한다. 그리고 전체를 펼쳐서 초기값을 또 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,11),2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기값을 0으로 위 reduce() 연산과 동일하게, 연산하면 5050 결과가 얻어진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,11)).fold(0, lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파티션 수 1 * 초기값 10 + (초기값 10 + 55) = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,11),1).fold(10, lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파티션 수 2 * 초기값 10 + (초기값 10 + 55) = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,11),2).fold(10, lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단순 통계 기능\n",
    "\n",
    "텍스트데이터와 달리 정량데이터로부터 sum, min, max, 표준편차 등 서술통계를 계산해 낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  5050\n",
      "min:  1\n",
      "max:  100\n",
      "count:  100\n",
      "standard deviation: 28.86607004772212\n",
      "variance:  833.25\n"
     ]
    }
   ],
   "source": [
    "print (\"sum: \", myRdd100.sum())\n",
    "print (\"min: \", myRdd100.min())\n",
    "print (\"max: \", myRdd100.max())\n",
    "print (\"count: \", myRdd100.count())\n",
    "print (\"standard deviation:\", myRdd100.stdev())\n",
    "print (\"variance: \", myRdd100.variance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### filter()\n",
    "\n",
    "filter() 함수로 조건에 맞는 문장만 분리해 보자.\n",
    "\"Spark\" 단어가 포함된 문장이 조건이된다.\n",
    "count() 함수로 그 갯수를 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n"
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print (\"How many lines having 'Spark': \", myRdd_spark.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "한글을 filter할 수 있다. Python2는 u를 붙여주어야 하고, 반면에 Python3은 유니코드를 기본으로 지원하기 때문에 앞에 u를 붙여주지 않아도 된다 (u는 유니코드를 의미한다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print (myRdd_unicode.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* filter()를 사용해서 **stopwords** 제거하기\n",
    "\n",
    "문장 안에 stopwords를 포함한 경우는 제거되지 않는다.\n",
    "따라서 flatMap()을 하고 단어에 대해 불용어를 제거해야 한다.\n",
    "불용어는 단어빈도를 계산하면서 제거하고 싶은 단어를 말한다.\n",
    "불용어는 빈도를 세어도 의미가 없는 대명사 (이, 그, 저...) 또는 한 글자 단어 (등...)이 될 수 있다.\n",
    "한글은 유니코드로 처리해야 한다.\n",
    "영어는 대소문자를 모두 처리하기 위해 여기서는 소문자로 만들어 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split())\\\n",
    "                    .filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "stopwords를 제거한 문장을 출력해 보자.\n",
    "collect() 함수는 문장을 수집하여, list로 만들어 준다.\n",
    "list를 하나씩 반복문 for를 사용하여 출력한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Apache Spark open source cluster computing framework. 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. Apache Spark Apache Spark Apache Spark Apache Spark 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 Originally developed University of California, Berkeley's AMPLab, Spark codebase was later donated to Apache Software Foundation, which has maintained it since. Spark provides interface programming entire clusters with implicit data parallelism and fault-tolerance. "
     ]
    }
   ],
   "source": [
    "for words in myRdd_stop.collect():\n",
    "    print (words, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### foreach()\n",
    "\n",
    "```foreach()```는 action이지만, 다른 action 함수들과 달리 **반환 값이 없다**, 그래서 출력이 없다.\n",
    "각 요소에 대해 적용한다는 역할에 대해 유사한 기능을 하는 ```map()``` 함수가 있다.\n",
    "```map()``` 함수는 각 요소에 대해 계산을 하고, 그 값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "다음은 foreach()는 f() 함수를 호출하여 실행하고 있다. 그 결과 각 요소를 print() 출력하고 있다.\n",
    "\n",
    "Spark 로그가 출력되는 명령창을 살펴보면, foreach()의 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figures/11_foreachPrint.png \"foreach print\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### pipeline\n",
    "\n",
    "파이프라인은 **transformation**(예: map()), **action**(예: collect()) 함수를 **연이어 적용**하는 방식을 말한다.\n",
    "파이프라인이 아니라면 함수를 하나씩 끝나고, 결과를 받은 후 다음 함수를 단계별로 적용하게 된다.\n",
    "보다 효율적인 처리를 위해 함수들을 파이프라인같이 붙여서 중간결과를 별도로 산출하지 않고 연이어 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2list=wordsRdd.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print (upper2list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 주의! x가 리스트이어야 하고, 그래야 연산이 가능하다. 대문자로 map 변환하고, 리스트로 collect 수집한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map(), collect()를 파이프라인으로 처리하여, 단어의 수를 len()함수로 세고 있다.\n",
    "위와 같이 연결해서 작성할 수 있지만, 한 줄씩 작성하는 편이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = wordsRdd\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print (wordsLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 파일에 쓰기\n",
    "\n",
    "메모리에 떠 있는 RDD를 로컬 파일에 저장해 놓고, 나중에 읽어서 작업을 할 수도 있다.\n",
    "\n",
    "윈도우에서는 이러한 IO 작업을 하기 위해서, ```%HADOOP_HOME%\\BIN\\WINUTILS.EXE```가 실행될 수 있도록 설정되어 있어야 한다.\n",
    "\n",
    "물론 제어판의 환경변수에서 설정할 수 있지만, os.environ을 사용하면 세션이 유지될 동안만 적용되지만 간편한 방법이니까 해보자. 단 환경을 재시작해야 설정이 적용되므로, \"Restart Kernel\"을 해서 초기화를 한 후 하자.\n",
    "\n",
    "> 더 알아보기: winutils\n",
    "기본적으로 winutils.exe는 Hadoop이 Windows에서 동작할 수 있도록 도와주는 도구이다.\n",
    "- 로컬 모드에서 기본 PySpark 사용: winutils.exe가 필요하지 않을 수 있다.\n",
    "- Hadoop과 통합되거나 고급 기능 사용: winutils.exe가 필요할 가능성이 크다.\n",
    "\n",
    "winutils.exe는 일반적으로 HADOOP_HOME\\bin 디렉토리에 있어야 하며, Spark는 HADOOP_HOME 환경 변수를 통해 이 경로를 참조하기 때문다. 단순히 winutils.exe를 환경 변수 PATH에 추가하는 것만으로는 충분하지 않다. HADOOP_HOME 경로를 설정해두고, 그 경로의 bin 디렉토리에 winutils.exe 파일을 두는 것이 중요하다.\n",
    "\n",
    "설정이 안되어 있으면, spark 서버를 띄우면서 또는 파일에 쓰려고 하면 \"WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException\" 이런 오류가 발생하므로 충분히 눈치챌 수 있다. 그렇다면 지금이라도 필요한 파일 ```winutils.exe, hadoop.dll```을 내려받고 설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> HADOOP_HOME: C:\\Users\\jsl\\Code\\201711111\n",
      "=> PATH: C:\\Program Files\\Python39\\Scripts\\;C:\\Program Files\\Python39\\;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Geth;C:\\Program Files\\nodejs\\;C:\\Program Files\\solidity-windows;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\Scripts;C:\\Users\\jsl\\.windows-build-tools\\python27\\;C:\\Program Files\\nodejs\\node_modules\\npm\\node_modules\\npm-lifecycle\\node-gyp-bin;C:\\Users\\jsl\\AppData\\Roaming\\npm\\node_modules\\windows-build-tools\\node_modules\\.bin;C:\\Users\\jsl\\AppData\\Roaming\\npm\\node_modules\\.bin;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Java\\openjdk-20.0.2_windows-x64_bin\\jdk-20.0.2\\bin;C:\\Users\\jsl\\Code\\201711111\\apache-tomcat-10.1.8\\bin;C:\\Users\\jsl\\Code\\201711111\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME']=\"C:\\\\Users\\\\jsl\\\\Code\\\\201711111\" #os.getcwd()\n",
    "os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "print(\"=> HADOOP_HOME: {}\".format(os.environ['HADOOP_HOME']))\n",
    "print(\"=> PATH: {}\".format(os.environ['PATH']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L3: '프로젝트 디렉토리'를 ```HADOOP_HOME```으로 설정한다. 다른 디렉토리로 설정해도 물론 가능하다.\n",
    "- L4: Spark버전, Hadoop 버전에 맞는 ```winutils.exe, hadoop.dll```를 다운로드 받아서 bin폴더에 저장하고 설정한다. 운영체제에 따라 디렉토리 구분자가 다르므로, os.path.join() 함수를 사용하자.\n",
    "- L6: ```HADOOP_HOME``` 설정되었는지 확인한다.\n",
    "- L7: 경로에 bin이 추가되었는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize([['THIS', 'IS'], ['A', 'LINE']]).saveAsTextFile(\"data/ds_spark_wiki_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 리스트에서 RDD가 생성되고, 그 RDD는 **디렉토리**가 만들어지고 그 안에 파일로 쓰여지게 된다. 한 번 실행되면 디렉토리가 생성되므로, 여러 번 실행하면 FileAlreadyExistsException 오류가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 생성되었는지 확인해보자. 명령창에서 확인해도 되지만 (윈도우에서는 dir, 리눅스에서는 ```ls```), Python에서 직접 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000.crc', '._SUCCESS.crc', 'part-00000', '_SUCCESS']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/ds_spark_wiki_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방금 작성된 파일을 읽어서 내용을 읽어보는 것도 물론 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd=spark.sparkContext.textFile(os.path.join(\"data\", \"ds_spark_wiki_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['THIS', 'IS']\", \"['A', 'LINE']\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```coalesce()``` 함수를 사용하면, partition을 늘리거나, 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd.map(lambda x: \"\".join(x)).coalesce(1).saveAsTextFile(\"data/ds_spark_wiki_txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: RDD를 1개의 partition으로 조정하고, 파일로 내려받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로젝트 디렉토리 아래로 가서, data\\ds_spark_wiki_txt\\part-00000 파일을 열어보자 (메모장으로 열어도 좋다). 작성된 내용이 적혀 있다.\n",
    "\n",
    "!cat data/ds_spark_wiki_txt/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### groupBy\n",
    "\n",
    "groupBy()는 RDD를 변환transformation 함수이다. Paired나 Unpaired 어느 경우에도 사용할 수 있지만, 주로 **unpaired RDD**에 많이 쓰인다.\n",
    "\n",
    "Unpaired 데이터는 key, value 쌍으로 구성이 되지 않은 데이터를 말한다. 텍스트를 보면, 그냥 문자열이고 키가 존재하지 않아 Unpaired 형식이다. 또 다른 예는 이름, 전공인데 얼핏보면 이름이 키로 보이지만 문자열 데이터이다.\n",
    "\n",
    "```python\n",
    "'lim', 'computer'\n",
    "'cho', 'computer'\n",
    "'lim', 'business'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "아래와 같은 텍스트는 unpaired인데, 이를 groupBy 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia',\n",
       " 'Apache Spark is an open source cluster computing framework.',\n",
       " '아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.',\n",
       " 'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " '아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크',\n",
       " \"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " 'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " 'which has maintained it since.',\n",
       " 'Spark provides an interface for programming entire clusters with',\n",
       " 'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터의 집단화 groupBy\n",
    "\n",
    "데이터를 집단화하기 위해서 groupBy() 함수가 필요하고, 그 결과는 그룹명은 key, 구성원은 value로 저장된다.\n",
    "\n",
    "\"아파치\" 단어의 포함여부에 따라 집단화해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: <pyspark.resultiterable.ResultIterable object at 0x000001DEA99A2A90>\n",
      "True: <pyspark.resultiterable.ResultIterable object at 0x000001DEA9A942B0>\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:\"아파치\" in x)\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    print (\"{}: {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2: \"아파치\"를 포함하면 True, 아니면 False로 groupBy() 집단화한다.\n",
    "- L4: collect()한 리스트에서 하나씩 k,v 쌍으로 반복문 실행한다.\n",
    "- 출력: key는 예상대로 True, False로 집단화되었으나 value는 iterator로 생성되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력의 ResultIterable 내부를 보려면, 반복문으로 해체하여야 하고, 외부의 반복문과 중첩하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: Wikipedia\n",
      "False: Apache Spark is an open source cluster computing framework.\n",
      "False: Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "False: Originally developed at the University of California, Berkeley's AMPLab,\n",
      "False: the Spark codebase was later donated to the Apache Software Foundation,\n",
      "False: which has maintained it since.\n",
      "False: Spark provides an interface for programming entire clusters with\n",
      "False: implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "True: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "True: 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "#myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print (\"{}: {}\".format(k, eachValue))\n",
    "    print (\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.6.3 Paired RDD에 적용하는 함수들\n",
    "\n",
    "Paired RDD는 **key,value 쌍**으로 구성된 RDD를 말한다.\n",
    "이렇게 구성된 RDD에 대해서 키에 대해 연산을 하는 **byKey()** 또는 값에 대해 **byValue()** 함수를 사용할 수 있다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "byKey 연산 | 동일한 키에 대해 연산<br>- 단계 1: key-value를 계산한다. 각 key의 빈도를 계산하여  '(key,1)' 형식으로 만든다.<br>- 단계 2: byKey를 적용한다. 동일한 key의 value를 더해준다.\n",
    "byValue 연산 | 그룹으로 구분하고 나면, 값이 복수 개가 된다. 복수의 값에 대해 개수를 셀지, 합계를 계산할지 mapValues() 함수에서 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paired RDD에 대한 변환함수는 다음과 같다.\n",
    "\n",
    "Paired RDD의 transformation 함수  | 설명\n",
    "-----|-----\n",
    "groupByKey() | 같은 key를 grouping, partition에서 **먼저 reduce하지 않고, 전체로 계산**한다.\n",
    "reduceByKey() | 같은 key의 value를 합계, partition에서 **먼저 reduce하고, 전체로 계산**한다. grouping + aggregation.<p>즉 **reduceByKey = groupByKey().reduce()**\n",
    "combineByKey() | 키별로 합계, 개수 (key, (sum, count))를 계산. createCombiner, mergeValue, mergeCombiners.\n",
    "aggregateByKey() | reduceByKey()와 유사한 기능을 수행한다. 키별로 합계, 개수, 평균을 계산.\n",
    "mapValues() | Paired RDD는 key,value가 있기 마련이고, **value에 적용하는 함수**이다.\n",
    "keys() | 키를 출력\n",
    "values() | value를 출력\n",
    "sortByKey() | 키별로 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paired RDD에 대한 action 함수는 다음과 같다.\n",
    "    \n",
    "Paired RDD의 action 함수 | 설명\n",
    "-----|-----\n",
    "countByKey() | 키별로 개수를 셈\n",
    "collectAsMap() | 결과를 map으로 만든다.\n",
    "lookup(key) | key의 값을 출력\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* (key, value)로 구성된 Paired 데이터\n",
    "\n",
    "Paired 데이터는 튜플 또는 리스트로 구성해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tuple=((\"Seoul\",1),(\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),\n",
    "           (\"Seoul\",1),(\"Busan\",1),(\"Bundang\",1),(\"Bundang\",1),(\"Bundang\", 1),\n",
    "           (\"Seoul\",1),(\"Seoul\",1),(\"Busan\",1),(\"Busan\",1),(\"Seoul\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list=[[\"Seoul\",1],[\"Seoul\",1],[\"Seoul\",1],[\"Busan\",1],[\"Busan\",1],[\"Busan\",1],\n",
    "           [\"Seoul\",1],[\"Busan\",1],[\"Bundang\",1],[\"Bundang\",1],[\"Bundang\",1],\n",
    "           [\"Seoul\",1],[\"Seoul\",1],[\"Busan\",1],[\"Busan\",1],[\"Seoul\",1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리스트는 수정가능하고 반면에 튜플은 수정불가하다는 차이가 있다.\n",
    "\n",
    "이로 인해 메모리의 재할당 및 해제 작업이 발생할 수 있어, 아래에서 보는 것처럼 메모리 크기가 튜플이 리스트에 비해 약간 효율적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory size of tuples (bytes):  160\n",
      "memory size of lists (bytes):  184\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"memory size of tuples (bytes): \", sys.getsizeof(_tuple))\n",
    "print(\"memory size of lists (bytes): \",sys.getsizeof(_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 SparkContext parallelize() 함수를 사용하여 리스트에서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 데이터에 groupBy()를 적용해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ResultIterable\n",
    "\n",
    "첫 요소 x[0]는 key이고, 두 개의 key1, key2가 있다. key를 groupBy()로 구분하여 묶어 보자.\n",
    "groupByKey()의 결과는 **```ResultIterable```**이고, 이 객체는 그대로 볼 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul', <pyspark.resultiterable.ResultIterable at 0x22c6413fbb0>),\n",
       " ('Busan', <pyspark.resultiterable.ResultIterable at 0x22c6413fc40>),\n",
       " ('Bundang', <pyspark.resultiterable.ResultIterable at 0x22c6413fca0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* mapValues\n",
    "\n",
    "ResultIterable을 리스트로 변환하여 값을 보기로 한다.\n",
    "\n",
    "이 경우 **```mapValues()```** 함수를 사용하는데, 함수명을 살펴보면 value에 대해 map을 적용한다는 뜻이다.\n",
    "\n",
    "이 함수를 사용하여, **values 값**들의 list()를 적용하고 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1]]),\n",
       " ('Busan',\n",
       "  [['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1]]),\n",
       " ('Bundang', [['Bundang', 1], ['Bundang', 1], ['Bundang', 1]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또는 보다 간단하게 다음과 같이 ```list```만을 해주어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul',\n",
       "  [['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1],\n",
       "   ['Seoul', 1]]),\n",
       " ('Busan',\n",
       "  [['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1],\n",
       "   ['Busan', 1]]),\n",
       " ('Bundang', [['Bundang', 1], ['Bundang', 1], ['Bundang', 1]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* partition\n",
    "\n",
    "데이터를 분할해서 클러스터의 노드에 배분된 논리적인 데이터 조각을 말한다.\n",
    "현재 partition은 1개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "아래와 같이 RDD를 생성할 때, partition을 늘려서 RDD를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_testRdd=spark.sparkContext.parallelize(_list, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'f'를 사용하면 변수를 그대로 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, jsl 2020.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2020\n",
    "name = 'jsl'\n",
    "f\"Hello, {name} {year}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partition을 2개로 늘려 놓았으므로 아래와 같이 각 partition에 분배된 데이터를 확인해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions 0 -> [['Seoul', 1], ['Seoul', 1], ['Seoul', 1], ['Busan', 1], ['Busan', 1], ['Busan', 1], ['Seoul', 1], ['Busan', 1]]\n",
      "Partitions 1 -> [['Bundang', 1], ['Bundang', 1], ['Bundang', 1], ['Seoul', 1], ['Seoul', 1], ['Busan', 1], ['Busan', 1], ['Seoul', 1]]\n"
     ]
    }
   ],
   "source": [
    "partitions = _testRdd.glom().collect()\n",
    "for num, partition in enumerate(partitions):\n",
    "    print(f'Partitions {num} -> {partition}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: glom()은 파티션이 있으면 그대로 내려서, collect() 수집한다.\n",
    "- L2: partitions는 순회 가능한(iterable) 리스트이다. 반복문에서 입력으로 사용하면, 각 요소와 해당 요소의 인덱스를 함께 출력한다. 출력할 때 인덱스 (코드에서 num, 요소(코드에서 partition)을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* key 출력\n",
    "\n",
    "Pair RDD는 key-value로 구성된다. key만 출력해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seoul',\n",
       " 'Seoul',\n",
       " 'Seoul',\n",
       " 'Busan',\n",
       " 'Busan',\n",
       " 'Busan',\n",
       " 'Seoul',\n",
       " 'Busan',\n",
       " 'Bundang',\n",
       " 'Bundang',\n",
       " 'Bundang',\n",
       " 'Seoul',\n",
       " 'Seoul',\n",
       " 'Busan',\n",
       " 'Busan',\n",
       " 'Seoul']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### groupByKey, reduceByKey, mapValues\n",
    "\n",
    "Partition1, 2, 3으로 데이터가 분할되어 있다고 가정하자.\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,1)<br> | (key1,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> | (key2,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              |\n",
    "\n",
    "* reduceByKey: key에 대해 집단화하고 reduce를 적용한다. 로컬에서 미리 reduce되기 때문에 메모리 사용량이 적다. 그 결과는 (key1,6) (key2,5)\n",
    "reduceByKey는 partition별로 작업을 먼저 수행한다. 아래에서 보는 것처럼 P1, P2, P3 파티션 별로 먼저 실행하고 난 후 합산된다.\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,3)<br>(key2,2) | (key1,1)<br>(key2,1) | (key1,2)<br>(key2,2)\n",
    "\n",
    "* groupByKey: 키를 기준으로 집단화하므로, 메모리 사용량이 높아질 수 있다. (key1,[1,1,1,1,1,1]) (key2,[1,1,1,1,1])\n",
    "반면에 groupByKey는 partition별로 수행되지 않고, **메모리에 값을 모두 저장**해 놓는다.\n",
    "\n",
    "key1 | key2\n",
    "-----|-----\n",
    "(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1) | (key2,1)<br>(key2,1)<br>(key2,1)<br>(key2,1)<br>(key2,1)\n",
    "\n",
    "* 어느 것을 써야 할까?\n",
    "\n",
    "**groupByKey보다는 reduceByKey 또는 combineByKey를 사용하는 것을 추천**하고 있다.\n",
    "groupByKey는 메모리를 많이 사용하고, reduceByKey는 Partition별로 작업이 동시에 병렬적으로 수행될 수 있으므로 보다 빠르기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Busan', 6), ('Seoul', 7), ('Bundang', 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "groupByKey()함수는 Paired RDD, 즉 키-밸류의 쌍으로 되어 있는 데이터에 적용되기 때문에 인자를 주지 않아도 된다. 그 결과는 **```ResultIterable```**이다. 결과를 바로 볼 수 없다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Busan', <pyspark.resultiterable.ResultIterable at 0x22c546be670>),\n",
       " ('Seoul', <pyspark.resultiterable.ResultIterable at 0x22c64164790>),\n",
       " ('Bundang', <pyspark.resultiterable.ResultIterable at 0x22c64164040>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **```mapValues()```**함수에 **list() 함수**를 적어주고 collect()하면 그 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Busan', [1, 1, 1, 1, 1, 1]),\n",
       " ('Seoul', [1, 1, 1, 1, 1, 1, 1]),\n",
       " ('Bundang', [1, 1, 1])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().mapValues(list).collect() # list is a function, that is, list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이해를 돕기 위해 **```mapValues()```**에 lambda함수를 사용해 1씩 더해본다.\n",
    "키는 변동이 없고, 값만 1씩 증가하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul', 2),\n",
       " ('Seoul', 2),\n",
       " ('Seoul', 2),\n",
       " ('Busan', 2),\n",
       " ('Busan', 2),\n",
       " ('Busan', 2),\n",
       " ('Seoul', 2),\n",
       " ('Busan', 2),\n",
       " ('Bundang', 2),\n",
       " ('Bundang', 2),\n",
       " ('Bundang', 2),\n",
       " ('Seoul', 2),\n",
       " ('Seoul', 2),\n",
       " ('Busan', 2),\n",
       " ('Busan', 2),\n",
       " ('Seoul', 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단어빈도 예제\n",
    "\n",
    "앞서 위키데이터에서 생성한 myRdd2를 사용해서, 단어를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', <pyspark.resultiterable.ResultIterable at 0x22c641791f0>),\n",
       " ('Apache', <pyspark.resultiterable.ResultIterable at 0x22c641792e0>),\n",
       " ('Spark', <pyspark.resultiterable.ResultIterable at 0x22c64179340>)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2: flatMap()은 RDD를 전체로 flatten해서 공백으로 분리한다.\n",
    "- L3: 단어빈도를 계산하기 위해, map()은 단어별 빈도 (x,1)를 만듦\n",
    "- L4: groupByKey()는 key를 묶어준다. 따라서 **```ResultIterable```** iterator를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "groupBy()를 하고 나면 결과는 ```ResultIterable```이다. 단어빈도를 집계해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1),\n",
       " ('Apache', 6),\n",
       " ('Spark', 7),\n",
       " ('is', 1),\n",
       " ('an', 2),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework.', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-L5: ```mapValues(sum)```는 value에 적용되는 함수로서, 합계를 계산한다.\n",
    "    * 내장함수 sum()은 value의 합계를 내는 sum(value)을 의미\n",
    "    * 또는 스스로 사용자 함수를 정의해서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum() 함수 대신 len() 함수를 사용해도 결과는 동일하다.\n",
    "키별로 1개씩 값을 가지게 되므로, 길이를 구해도 sum과 동일한 결과를 출력하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AMPLab,', 1),\n",
       " ('Apache', 6),\n",
       " (\"Berkeley's\", 1),\n",
       " ('California,', 1),\n",
       " ('Foundation,', 1),\n",
       " ('Originally', 1),\n",
       " ('Software', 1),\n",
       " ('Spark', 7),\n",
       " ('University', 1),\n",
       " ('Wikipedia', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return len(x)\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(f)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L6: 값에 대해 f함수 (길이를 계산)를 적용한다.\n",
    "- L7: 키로 정렬한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "groupByKey() 결과를 collect() 또는 take()하면 그 결과는 리스트가 된다.\n",
    "좀 더 읽기 좋게 출력하려면, Python 반복문으로 리스트를 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어:AMPLab,\t\t빈도:1\n",
      "단어:Apache\t\t빈도:6\n",
      "단어:Berkeley's\t\t빈도:1\n",
      "단어:California,\t\t빈도:1\n",
      "단어:Foundation,\t\t빈도:1\n",
      "단어:Originally\t\t빈도:1\n",
      "단어:Software\t\t빈도:1\n",
      "단어:Spark\t\t빈도:7\n",
      "단어:University\t\t빈도:1\n",
      "단어:Wikipedia\t\t빈도:1\n"
     ]
    }
   ],
   "source": [
    "wc=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)\n",
    "\n",
    "for k,v in wc:\n",
    "    print (f\"단어:{k}\\t\\t빈도:{v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```reduceByKey()```는 ```groupByKey()```와 달리 키별로 빈도를 합산하기 때문에 ```mapValues()```가 필요없다.\n",
    "```reduce()```는 **함수**를 사용해서 **인자를 2개 받아서 1개로 병합**하는 기능을 수행한다.\n",
    "```reduceByKey()```은 (K, V) 쌍으로 병합해서 (K, V)를 반환한다는 점에 유의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1),\n",
       " ('Apache', 6),\n",
       " ('Spark', 7),\n",
       " ('is', 1),\n",
       " ('an', 2),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework.', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### countByKey\n",
    "\n",
    "**```countByKey()```**는 Key별로 계산을 하는 action 함수이고, 그 결과는 **dictionary**로 출력된다.\n",
    "**```coutByKey().items()```**하면 리스트로 변환될 수 있다.\n",
    "\n",
    "```countByKey()```는 dictionary로 병합하는 반면, reduceByKey()는 (K,V)로 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Wikipedia': 1,\n",
       "             'Apache': 6,\n",
       "             'Spark': 7,\n",
       "             'is': 1,\n",
       "             'an': 2,\n",
       "             'open': 1,\n",
       "             'source': 1,\n",
       "             'cluster': 1,\n",
       "             'computing': 1,\n",
       "             'framework.': 1,\n",
       "             '아파치': 5,\n",
       "             '스파크는': 1,\n",
       "             '오픈': 1,\n",
       "             '소스': 1,\n",
       "             '클러스터': 1,\n",
       "             '컴퓨팅': 1,\n",
       "             '프레임워크이다.': 1,\n",
       "             '스파크': 4,\n",
       "             'Originally': 1,\n",
       "             'developed': 1,\n",
       "             'at': 1,\n",
       "             'the': 3,\n",
       "             'University': 1,\n",
       "             'of': 1,\n",
       "             'California,': 1,\n",
       "             \"Berkeley's\": 1,\n",
       "             'AMPLab,': 1,\n",
       "             'codebase': 1,\n",
       "             'was': 1,\n",
       "             'later': 1,\n",
       "             'donated': 1,\n",
       "             'to': 1,\n",
       "             'Software': 1,\n",
       "             'Foundation,': 1,\n",
       "             'which': 1,\n",
       "             'has': 1,\n",
       "             'maintained': 1,\n",
       "             'it': 1,\n",
       "             'since.': 1,\n",
       "             'provides': 1,\n",
       "             'interface': 1,\n",
       "             'for': 1,\n",
       "             'programming': 1,\n",
       "             'entire': 1,\n",
       "             'clusters': 1,\n",
       "             'with': 1,\n",
       "             'implicit': 1,\n",
       "             'data': 1,\n",
       "             'parallelism': 1,\n",
       "             'and': 1,\n",
       "             'fault-tolerance.': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .countByKey() # .items() to be added to get a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: RDD를 사용하여 단어빈도를 계산하고, 그래프 그리기.\n",
    "\n",
    "텍스트 파일을 읽어서, 단어빈도를 계산하는 프로그램을 작성하자.\n",
    "단어빈도는 내림차순으로 출력해서 상위 15개를 출력한다.\n",
    "\n",
    "단어빈도는 꼭 Spark로 해야만 되는 것은 아니다.\n",
    "(1) Python으로도 해보고, (2) pyspark으로 RDD를 생성하여, 단어빈도를 계산해보자.\n",
    "\n",
    "영어 불용어는 Python NLTK, Gensim, sklearn 등 라이브러리에서 목록을 제공하고 있지만, 한글 불용어는 아직 제공되고 있지 못하여, 임의로 만들어 쓰도록 하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_bigdata_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_bigdata_wiki.txt\n",
    "Big data\n",
    "활용사례 및 의의[편집]\n",
    "정치 및 사회[편집]\n",
    "2008년 미국 대통령 선거[편집]\n",
    "2008년 미국 대통령 선거에서 버락 오바마 미국 대통령 후보는 다양한 형태의 유권자 데이터베이스를 확보하여 이를 분석, 활용한 '유권자 맞춤형 선거 전략'을 전개했다. 당시 오바마 캠프는 인종, 종교, 나이, 가구형태, 소비수준과 같은 기본 인적 사항으로 유권자를 분류하는 것을 넘어서서 과거 투표 여부, 구독하는 잡지, 마시는 음료 등 유권자 성향까지 전화나 개별 방문을 또는 소셜 미디어를 통해 유권자 정보를 수집하였다. 수집된 데이터는 오바마 캠프 본부로 전송되어 유권자 데이터베이스를 온라인으로 통합관리하는 ‘보트빌더(VoteBuilder.com)’시스템의 도움으로 유권자 성향 분석, 미결정 유권자 선별 , 유권자에 대한 예측을 해나갔다. 이를 바탕으로‘유권자 지도’를 작성한 뒤 ‘유권자 맞춤형 선거 전략’을 전개하는 등 오바마 캠프는 비용 대비 효과적인 선거를 치를 수 있었다.\n",
    "\n",
    "대한민국 제19대 총선[편집]\n",
    "중앙선거관리위원회는 대한민국 제19대 총선부터 소셜 네트워크 등 인터넷 상의 선거 운동을 상시 허용하였다.[15] 이에 소셜 미디어 상에서 선거 관련 데이터는 증폭되었으며, 2010년 대한민국 제5회 지방 선거 및 2011년 대한민국 재보궐선거에서 소셜 네트워크 서비스의 중요성을 확인한 정당들 또한 SNS 역량 지수를 공천 심사에 반영하는 등[16] 소셜 네트워크 활용에 주목했다. 이 가운데 여론 조사 기관들은 기존 여론조사 방식으로 예측한 2010년 제5회 지방 선거 및 2011년 재보궐선거의 여론조사 결과와 실제 투표 결과와의 큰 차이를 보완하고자 빅 데이터 기술을 활용한 SNS 여론 분석을 시행했다. 그러나 SNS 이용자의 대다수가 수도권 20~30대에 쏠려 있기에[17], 빅 데이터를 이용한 대한민국 제19대 총선에 대한 SNS 분석은 수도권으로 한정되어 일치하는 한계를 드러내기도 하였다.\n",
    "\n",
    "경제 및 경영[편집]\n",
    "아마존닷컴의 추천 상품 표시 / 구글 및 페이스북의 맞춤형 광고[편집]\n",
    "아마존닷컴은 모든 고객들의 구매 내역을 데이터베이스에 기록하고, 이 기록을 분석해 소비자의 소비 취향과 관심사를 파악한다.[18] 이런 빅 데이터의 활용을 통해 아마존은 고객별로 '추천 상품(레코멘데이션)'을 표시한다. 고객 한사람 한사람의 취미나 독서 경향을 찾아 그와 일치한다고 생각되는 상품을 메일, 홈 페이지상에서 중점적으로 고객 한사람 한사람에게 자동적으로 제시하는 것이다.[19] 아마존닷컴의 추천 상품 표시와 같은 방식으로 구글 및 페이스북도 이용자의 검색 조건, 나아가 사진과 동영상 같은 비정형 데이터 사용을 즉각 처리하여 이용자에게 맞춤형 광고를 제공하는 등 빅데이터의 활용을 증대시키고 있다.\n",
    "\n",
    "문화[편집]\n",
    "MLB (메이저 리그 베이스볼)의 머니볼 이론 및 데이터 야구[편집]\n",
    "머니볼 이론이란 경기 데이터를 철저하게 분석해 오직 데이터를 기반으로 적재적소에 선수들을 배치해 승률을 높인다는 게임 이론이다.[20] 이는 미국 메이저 리그 베이스볼 오클랜드 어슬레틱스의 구단장 빌리 빈이 리그 전체 25위에 해당하는 낮은 구단 지원금 속에서도 최소비용으로 최대효과를 거둔 상황에서 유래되었다. 빌리 빈은 하치해 최하위에 그치던 팀을 4년 연속 포스트시즌에 진출시키고 메이저 리그 최초로 20연승이라는 신기록을 세우도록 탈바꿈 시켰다. 미국 월스트리트 저널은 미국 경제에 큰 영향을 끼치는 파워 엘리트 30인에 워렌 버핏, 앨런 그린스펀과 함께 빌리 빈을 선정[21] 하는 등 머니볼 이론은 경영, 금융 분야에서도 주목받았다. 최근 들어서 과학기술 및 카메라 기술의 발달로 더욱 정교한 데이터의 수집이 가능해졌으며 투구의 궤적 및 투수의 그립, 타구 방향, 야수의 움직임까지 잡아낼 수 있게 되었다. 이처럼 기존의 정형 데이터뿐만 아닌 비정형 데이터의 수집과 분석, 활용을 통해 최근 야구경기에서 빅 데이터의 중요성은 더욱 커지고 있다.\n",
    "\n",
    "선수의 인기만을 쫓는 것이 아니라 팀별 승률이나 선수의 성적을 나타내는 수치와 야구를 관전한다면 그 재미는 배가된다. '출루율'은 타율로 인정되지 않는 볼넷을 포함하여 타자가 성공적으로 베이스를 밟은 횟수의 비율, '장타율'은 타수마다 밟은 총 베이스를 계산해서 타격력이 얼마나 강한지를 나타내는 비율이다.\n",
    "\n",
    "출루율과 장타율 못지 않게 '타수'는 한두 경기에서 낸 성적이 아닌, 수천 번의 타석에 들어 좋은 성적을 만들어낸 선수를 선별하기 위한 기초 통계자료이다. 이처럼 한 선수의 타율에서 팀의 역대 시리즈 전적까지 모든 것을 숫자로 표현할 수 있다고 해서 야구를 '통계의 스포츠'라고 부르기도 한다. 야구뿐만 아니라 생활 곳곳에서 활용되는 통계는 복잡한 상황과 설명을 간단한 숫자로 바꿔주는 매우 강력한 도구이다.[22]\n",
    "\n",
    "'프로파일링'과 '빅데이터' 기법을 활용한 프로그램 MBC <프로파일링>[편집]\n",
    "방송에는 19세 소년의 살인 심리를 파헤친 '용인살인사건의 재구성', 강남 3구 초등학교 85곳의 학업성취도평가 성적과 주변 아파트 매매가의 상관관계를 빅데이터(디지털 환경에서 발생한 방대한 규모의 데이터)를 통해 분석한 '강남, 부자일수록 공부를 잘할까'[23]\n",
    "\n",
    "2014년 FIFA 월드컵 독일 우승과 '빅데이터'[편집]\n",
    "브라질에서 개최된 2014년 FIFA 월드컵에서 독일은 준결승에서 개최국인 브라질을 7:1로 꺾고, 결승에서 아르헨티나와 연장전까지 가는 접전 끝에 1:0으로 승리를 거두었다. 무패행진으로 우승을 차지한 독일 국가대표팀의 우승의 배경에는 '빅데이터'가 있었다.\n",
    "\n",
    "독일 국가대표팀은 SAP와 협업하여 훈련과 실전 경기에 'SAP 매치 인사이트'를 도입했다. SAP 매치 인사이트란 선수들에게 부착된 센서를 통해 운동량, 순간속도, 심박수, 슈팅동작 등 방대한 비정형 데이터를 수집, 분석한 결과를 감독과 코치의 태블릿PC로 전송하여 그들이 데이터를 기반으로 전술을 짜도록 도와주는 솔루션이다. 기존에 감독의 경험이나 주관적 판단으로 결정되는 전략과는 달리, SAP 매치 인사이트를 통해 이루어지는 분석은 선수들에 대한 분석 뿐만 아니라 상대팀 전력, 강점, 약점 등 종합적인 분석을 통해 좀 더 과학적인 전략을 수립할 수 있다. 정보 수집에 쓰이는 센서 1개가 1분에 만들어내는 데이터는 총 12000여개로 독일 국가대표팀은 선수당 4개(골키퍼는 양 손목을 포함해 6개)의 센서를 부착했고, 90분 경기동안 한 선수당 약 432만개, 팀 전체로 약 4968만개의 데이터를 수집했다고 한다.월드컵8강 獨 전차군단 비밀병기는 '빅데이터'\n",
    "\n",
    "과학기술 및 활용[편집]\n",
    "통계학[편집]\n",
    "데이터 마이닝이란 기존 데이터베이스 관리도구의 데이터 수집, 저장, 관리, 분석의 역량을 넘어서는 대량의 정형 또는 비정형 데이터 집합 및 이러한 데이터로부터 가치를 추출하고 결과를 분석하는 기술로, 수집되는 ‘빅 데이터’를 보완하고 마케팅, 시청률조사, 경영 등으로부터 체계화돼 분류, 예측, 연관분석 등의 데이터 마이닝을 거쳐 통계학적으로 결과를 도출해 내고 있다.[24][25]\n",
    "\n",
    "대한민국에서는 2000년부터 정보통신부의 산하단체로 사단법인 한국BI데이터마이닝학회가 설립되어 데이터 마이닝에 관한 학술과 기술을 발전, 보급, 응용하고 있다. ‎또한 국내ㆍ외 통계분야에서 서서히 빅 데이터 활용에 대한 관심과 필요성이 커지고 있는 가운데 국가통계 업무를 계획하고 방대한 통계자료를 처리하는 국가기관인 통계청이 빅 데이터를 연구하고 활용방안을 모색하기 위한 '빅 데이터 연구회'를 발족하였다.[26] 하지만 업계에 따르면, 미국과 영국, 일본 등 선진국들은 이미 빅 데이터를 다각적으로 분석해 조직의 전략방향을 제시하는 데이터과학자 양성에 사활을 걸고 있다. 그러나 한국은 정부와 일부 기업이 데이터과학자 양성을 위한 프로그램을 진행 중에 있어 아직 걸음마 단계인 것으로 알려져 있다.[27]\n",
    "\n",
    "생물정보학[편집]\n",
    "최근 생물학에서 DNA, RNA, 단백질 서열 및 유전자들의 발현과 조절에 대한 데이터의 양이 급격히 증가했고 이에 따라 이 빅 데이터를 활용한 생명의 이해에 관한 논의가 진행되고 있다.\n",
    "\n",
    "보건의료[편집]\n",
    "국민건강보험공단은 가입자의 자격·보험료, 진료·투약내용, 건강검진 결과 및 생활습관 정보 등 2조1천억건, 92테라바이트의 빅데이터를 보유하고 있고, 한편, 건강보험심사평가원은 진료내역, 투약내용(의약품 안심서비스), 의약품 유통 등의 2조2천억건, 89테라바이트의 빅데이터를 보유하고 있으며, 경제협력개발기구(OECD)는 한국의 건강보험 빅데이터 순위가 2위라고 발표했었다. 건보공단과 심평원은 빅데이터를 민간에 널리 알리고 더 많이 개방하고 있다. (연합뉴스 2016.6.14 인터넷뉴스 참조)\n",
    "\n",
    "빅 데이터를 활용하면 미국 의료부문은 연간 3,300 억 달러(미 정부 의료 예산의 약 8%에 해당하는 규모)의 직간접적인 비용 절감 효과를 보일 것으로 전망된다.[28] 특히 임상분야에서는 의료기관 별 진료방법, 효능, 비용 데이터를 분석하여 보다 효과적인 진료방법을 파악하고 환자 데이터를 온라인 플랫폼화하여 의료협회 간 데이터 공유로 치료 효과를 제고하며 공중보건 영역에선 전국의 의료 데이터를 연계하여 전염병 발생과 같은 긴박한 순간에 빠른 의사결정을 가능케 할 전망이다.[29]\n",
    "\n",
    "한편, 의료 분야에서 빅 데이터가 효과를 발휘하기 위해서는 대량의 의료정보 수집이 필수적이기 때문에, 개인정보의 보호와 빅 데이터 활용이라는 두 가지 가치가 상충하게 되된다. 따라서, 의료 분야에서 빅 데이터의 활용과 보급을 위해서는 이러한 문제에 대한 가이드라인 마련이 필요한 상태이다.[30]\n",
    "\n",
    "기업 경영[편집]\n",
    "대규모의 다양한 데이터를 활용한 '빅데이터 경영'이 주목받으면서 데이터 품질을 높이고 방대한 데이터의 처리를 돕는 데이터 통합(Data Integration)의 중요성이 부각되고 있다.\n",
    "\n",
    "데이터 통합(DI)은 데이터의 추출, 변환, 적재를 위한 ETL 솔루션이 핵심인데 ETL 솔루션을 활용하면 일일이 수많은 데이터를 기업 데이터 포맷으로 코딩하지 않아도 되고 데이터 품질을 제고할 수 있기 때문에 DI는 빅데이터 환경에 꼭 필요한 데이터 솔루션으로 평가받고 있는 단계까지 진입되었다.\n",
    "\n",
    "한편 비즈니스 인텔리전스(Business Intelligence, BI)보다 진일보한 빅데이터 분석 방법이 비즈니스 애널리틱스(Business analytics, BA)인데 고급분석 범주에 있는 BA는 기본적으로 BI를 포함하면서도 미래 예측 기능과 통계분석, 확률 분석 등을 포함해 최적의 데이터 기반 의사결정을 가능케 하는 것으로 평가받고 있기도 하다.[31]\n",
    "\n",
    "마케팅[편집]\n",
    "인터넷으로 시작해서 인터넷으로 마감하는 생활, 스마트폰을 이용해 정보를 검색하고 쇼핑도하고 SNS를 이용해서 실시간으로 글을 남기는 등의 다양하게 인터넷을 이용하는 동안 남는 흔적같은 모인 데이터들을 분석하면 개인의 생활 패턴, 소비성향 등을 예측할 수 있고 기업들은 이런 데이터를 통해서 소비자가 원하는 것들을 미리 예측할 수 있다. 빅 데이터가 마케팅 자료로 활용되는 사례이다.[31]\n",
    "\n",
    "기상정보[편집]\n",
    "한반도 전역의 기상관측정보를 활용해 일기예보와 각종 기상특보 등 국가 기상서비스를 제공하고 있는 기상청은 정밀한 기상예측을 위한 분석 과정에서 발생하는 데이터 폭증에 대응하고자 빅데이터 저장시스템의 도입을 추진하였다.\n",
    "\n",
    "대다수 스토리지 기업들의 솔루션을 검토한 끝에 한국 IBM의 고성능 대용량 파일공유시스템(General Parallel File System, 이하 GPFS)을 적용한 스토리지 시스템을 선택하였다고 밝혔다.\n",
    "\n",
    "한국IBM이 기상청에 제공한 GPFS 기반의 빅데이터 저장시스템은 IBM 시스템 스토리지 제품군, 시스템 x서버 제품군과 고속 네트워킹 랙스위치(RackSwitch) 등이 통합돼 있는 시스템이다.[31]\n",
    "\n",
    "보안관리[편집]\n",
    "보안관리는 빅데이터 환경을 이용해 성장과 기술 발전을 동시에 이루는 분야로 분리한다. 클라우드 및 모바일 환경으로 접어들면서 물리/가상화 IT 시스템의 복잡성이 더욱 높아지고 있어 유무선 네트워크, 프라이빗/퍼블릭 클라우드, 모바일 애플리케이션과 기기관리 등 IT 시스템 전반에서 대대적인 변화가 예상되고 있어 막대한 양의 보안관리가 중요한 요소로 현실화되고 있다.[32]\n",
    "\n",
    "구글 번역[편집]\n",
    "구글에서 제공하는 자동 번역 서비스인 구글 번역은 빅 데이터를 활용한다. 지난 40년 간 컴퓨터 회사 IBM의 자동 번역 프로그램 개발은 컴퓨터가 명사, 형용사, 동사 등 단어와 어문의 문법적 구조를 인식하여 번역하는 방식으로 이뤄졌다. 이와 달리 2006년 구글은 수억 건의 문장과 번역문을 데이터베이스화하여 번역시 유사한 문장과 어구를 기존에 축적된 데이터를 바탕으로 추론해 나가는 통계적 기법을 개발하였다. 캐나다 의회의 수백만 건의 문서를 활용하여 영어-불어 자동번역 시스템개발을 시도한 IBM의 자동 번역 프로그램은 실패한 반면 구글은 수억 건의 자료를 활용하여 전 세계 58개 언어 간의 자동번역 프로그램 개발에 성공하였다. 이러한 사례로 미루어 볼 때, 데이터 양의 측면에서의 엄청난 차이가 두 기업의 자동 번역 프로그램의 번역의 질과 정확도를 결정했으며, 나아가 프로젝트의 성패를 좌우했다고 볼 수 있다.[31]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Python으로 단어 빈도를 세어 보기\n",
    "\n",
    "우선 map-reduce를 사용하지 않고 단어의 갯수를 세어본다.\n",
    "문서의 단어를 세는 알고리즘은 다음과 같다.\n",
    "단어별 갯수를 저장하기 위해서는 **dictionary**를 사용한다.\n",
    "\n",
    "```python\n",
    "키-갯수 dictionary d 초기화\n",
    "모든 단어를 반복:\n",
    "    단어w를 하나 읽음\n",
    "    단어w가 처음이면 d에 키를 생성하고 1개로\n",
    "    d에 있는 단어w이면 있는 키의 갯수를 하나 증가시킴\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 라이브러리\n",
    "\n",
    "* 영어는 라이브러리에서 제공된다\n",
    "\n",
    "Python nltk 라이브러리에서 제공하는 불용어를 사용하거나\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "englishStops = set(stopwords.words('english'))\n",
    "```\n",
    "\n",
    "또는 skleran 라이브러리를 활용해도 된다.\n",
    "```python\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "'the' in ENGLISH_STOP_WORDS # true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한글 불용어는 자신이 만든다\n",
    "\n",
    "한글 불용어는 불행히도 아직 제공되고 있지 못하여, 우리 스스로 임의로 만들어 쓰도록 하자.\n",
    "본문을 읽어서 단어빈도를 세어도 별 의미가 없을만한 단어를 불용어에 넣어두자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'간', '그와', '등', '이와', '이를', '전', '이런', '또는', '두', '이', '및'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'및' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'한' in stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어를 제거하고 출력해보자.\n",
    "텍스트에는 아직 제거해야할 단어들이 다수 포함되어 있다.\n",
    "컴마와 같은 기호, 숫자 등도 제거할 필요가 있다.\n",
    "\n",
    "\n",
    "> **UnicodeDecodeError**: 'cp949' 인코딩 오류\n",
    "\n",
    "> 파일을 텍스트로 읽고 (rt), encoding은 'utf8'로 해준다.\n",
    "> f=open(os.path.join(\"data\", \"ds_bigdata_wiki.txt\"), 'rt', encoding='UTF8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big/data/활용사례/의의[편집]/정치/사회[편집]/2008년/미국/대통령/선거[편집]/2008년/미국/대통령/선거에서/버락/오바마/미국/대통령/후보는/다양한/형태의/유권자/데이터베이스를/확보하여/분석,/활용한/'유권자/맞춤형/선거/전략'을/전개했다./당시/오바마/캠프는/인종,/종교,/나이,/가구형태,/소비수준과/같은/기본/인적/사항으로/유권자를/분류하는/것을/넘어서서/과거/투표/여부,/구독하는/잡지,/마시는/음료/유권자/성향까지/전화나/개별/방문을/소셜/미디어를/통해/유권자/정보를/수집하였다./수집된/데이터는/오바마/캠프/본부로/전송되어/유권자/데이터베이스를/온라인으로/통합관리하는/‘보트빌더(VoteBuilder.com)’시스템의/도움으로/유권자/성향/분석,/미결정/유권자/선별/,/유권자에/대한/예측을/해나갔다./바탕으로‘유권자/지도’를/작성한/뒤/‘유권자/맞춤형/선거/전략’을/전개하는/오바마/캠프는/비용/대비/효과적인/선거를/치를/수/있었다./대한민국/제19대/총선[편집]/중앙선거관리위원회는/대한민국/제19대/총선부터/소셜/네트워크/인터넷/상의/선거/운동을/상시/허용하였다.[15]/이에/소셜/미디어/상에서/선거/관련/데이터는/증폭되었으며,/2010년/대한민국/제5회/지방/선거/2011년/대한민국/재보궐선거에서/소셜/네트워크/서비스의/중요성을/확인한/정당들/또한/SNS/역량/지수를/공천/심사에/반영하는/등[16]/소셜/네트워크/활용에/주목했다./가운데/여론/조사/기관들은/기존/여론조사/방식으로/예측한/2010년/제5회/지방/선거/2011년/재보궐선거의/여론조사/결과와/실제/투표/결과와의/큰/차이를/보완하고자/빅/데이터/기술을/활용한/SNS/여론/분석을/시행했다./그러나/SNS/이용자의/대다수가/수도권/20~30대에/쏠려/있기에[17],/빅/데이터를/이용한/대한민국/제19대/총선에/대한/SNS/분석은/수도권으로/한정되어/일치하는/한계를/드러내기도/하였다./경제/경영[편집]/아마존닷컴의/추천/상품/표시///구글/페이스북의/맞춤형/광고[편집]/아마존닷컴은/모든/고객들의/구매/내역을/데이터베이스에/기록하고,/기록을/분석해/소비자의/소비/취향과/관심사를/파악한다.[18]/빅/데이터의/활용을/통해/아마존은/고객별로/'추천/상품(레코멘데이션)'을/표시한다./고객/한사람/한사람의/취미나/독서/경향을/찾아/일치한다고/생각되는/상품을/메일,/홈/페이지상에서/중점적으로/고객/한사람/한사람에게/자동적으로/제시하는/것이다.[19]/아마존닷컴의/추천/상품/표시와/같은/방식으로/구글/페이스북도/이용자의/검색/조건,/나아가/사진과/동영상/같은/비정형/데이터/사용을/즉각/처리하여/이용자에게/맞춤형/광고를/제공하는/빅데이터의/활용을/증대시키고/있다./문화[편집]/MLB/(메이저/리그/베이스볼)의/머니볼/이론/데이터/야구[편집]/머니볼/이론이란/경기/데이터를/철저하게/분석해/오직/데이터를/기반으로/적재적소에/선수들을/배치해/승률을/높인다는/게임/이론이다.[20]/이는/미국/메이저/리그/베이스볼/오클랜드/어슬레틱스의/구단장/빌리/빈이/리그/전체/25위에/해당하는/낮은/구단/지원금/속에서도/최소비용으로/최대효과를/거둔/상황에서/유래되었다./빌리/빈은/하치해/최하위에/그치던/팀을/4년/연속/포스트시즌에/진출시키고/메이저/리그/최초로/20연승이라는/신기록을/세우도록/탈바꿈/시켰다./미국/월스트리트/저널은/미국/경제에/큰/영향을/끼치는/파워/엘리트/30인에/워렌/버핏,/앨런/그린스펀과/함께/빌리/빈을/선정[21]/하는/머니볼/이론은/경영,/금융/분야에서도/주목받았다./최근/들어서/과학기술/카메라/기술의/발달로/더욱/정교한/데이터의/수집이/가능해졌으며/투구의/궤적/투수의/그립,/타구/방향,/야수의/움직임까지/잡아낼/수/있게/되었다./이처럼/기존의/정형/데이터뿐만/아닌/비정형/데이터의/수집과/분석,/활용을/통해/최근/야구경기에서/빅/데이터의/중요성은/더욱/커지고/있다./선수의/인기만을/쫓는/것이/아니라/팀별/승률이나/선수의/성적을/나타내는/수치와/야구를/관전한다면/그/재미는/배가된다./'출루율'은/타율로/인정되지/않는/볼넷을/포함하여/타자가/성공적으로/베이스를/밟은/횟수의/비율,/'장타율'은/타수마다/밟은/총/베이스를/계산해서/타격력이/얼마나/강한지를/나타내는/비율이다./출루율과/장타율/못지/않게/'타수'는/한두/경기에서/낸/성적이/아닌,/수천/번의/타석에/들어/좋은/성적을/만들어낸/선수를/선별하기/위한/기초/통계자료이다./이처럼/한/선수의/타율에서/팀의/역대/시리즈/전적까지/모든/것을/숫자로/표현할/수/있다고/해서/야구를/'통계의/스포츠'라고/부르기도/한다./야구뿐만/아니라/생활/곳곳에서/활용되는/통계는/복잡한/상황과/설명을/간단한/숫자로/바꿔주는/매우/강력한/도구이다.[22]/'프로파일링'과/'빅데이터'/기법을/활용한/프로그램/MBC/<프로파일링>[편집]/방송에는/19세/소년의/살인/심리를/파헤친/'용인살인사건의/재구성',/강남/3구/초등학교/85곳의/학업성취도평가/성적과/주변/아파트/매매가의/상관관계를/빅데이터(디지털/환경에서/발생한/방대한/규모의/데이터)를/통해/분석한/'강남,/부자일수록/공부를/잘할까'[23]/2014년/FIFA/월드컵/독일/우승과/'빅데이터'[편집]/브라질에서/개최된/2014년/FIFA/월드컵에서/독일은/준결승에서/개최국인/브라질을/7:1로/꺾고,/결승에서/아르헨티나와/연장전까지/가는/접전/끝에/1:0으로/승리를/거두었다./무패행진으로/우승을/차지한/독일/국가대표팀의/우승의/배경에는/'빅데이터'가/있었다./독일/국가대표팀은/SAP와/협업하여/훈련과/실전/경기에/'SAP/매치/인사이트'를/도입했다./SAP/매치/인사이트란/선수들에게/부착된/센서를/통해/운동량,/순간속도,/심박수,/슈팅동작/방대한/비정형/데이터를/수집,/분석한/결과를/감독과/코치의/태블릿PC로/전송하여/그들이/데이터를/기반으로/전술을/짜도록/도와주는/솔루션이다./기존에/감독의/경험이나/주관적/판단으로/결정되는/전략과는/달리,/SAP/매치/인사이트를/통해/이루어지는/분석은/선수들에/대한/분석/뿐만/아니라/상대팀/전력,/강점,/약점/종합적인/분석을/통해/좀/더/과학적인/전략을/수립할/수/있다./정보/수집에/쓰이는/센서/1개가/1분에/만들어내는/데이터는/총/12000여개로/독일/국가대표팀은/선수당/4개(골키퍼는/양/손목을/포함해/6개)의/센서를/부착했고,/90분/경기동안/한/선수당/약/432만개,/팀/전체로/약/4968만개의/데이터를/수집했다고/한다.월드컵8강/獨/전차군단/비밀병기는/'빅데이터'/과학기술/활용[편집]/통계학[편집]/데이터/마이닝이란/기존/데이터베이스/관리도구의/데이터/수집,/저장,/관리,/분석의/역량을/넘어서는/대량의/정형/비정형/데이터/집합/이러한/데이터로부터/가치를/추출하고/결과를/분석하는/기술로,/수집되는/‘빅/데이터’를/보완하고/마케팅,/시청률조사,/경영/등으로부터/체계화돼/분류,/예측,/연관분석/등의/데이터/마이닝을/거쳐/통계학적으로/결과를/도출해/내고/있다.[24][25]/대한민국에서는/2000년부터/정보통신부의/산하단체로/사단법인/한국BI데이터마이닝학회가/설립되어/데이터/마이닝에/관한/학술과/기술을/발전,/보급,/응용하고/있다./‎또한/국내ㆍ외/통계분야에서/서서히/빅/데이터/활용에/대한/관심과/필요성이/커지고/있는/가운데/국가통계/업무를/계획하고/방대한/통계자료를/처리하는/국가기관인/통계청이/빅/데이터를/연구하고/활용방안을/모색하기/위한/'빅/데이터/연구회'를/발족하였다.[26]/하지만/업계에/따르면,/미국과/영국,/일본/선진국들은/이미/빅/데이터를/다각적으로/분석해/조직의/전략방향을/제시하는/데이터과학자/양성에/사활을/걸고/있다./그러나/한국은/정부와/일부/기업이/데이터과학자/양성을/위한/프로그램을/진행/중에/있어/아직/걸음마/단계인/것으로/알려져/있다.[27]/생물정보학[편집]/최근/생물학에서/DNA,/RNA,/단백질/서열/유전자들의/발현과/조절에/대한/데이터의/양이/급격히/증가했고/이에/따라/빅/데이터를/활용한/생명의/이해에/관한/논의가/진행되고/있다./보건의료[편집]/국민건강보험공단은/가입자의/자격·보험료,/진료·투약내용,/건강검진/결과/생활습관/정보/2조1천억건,/92테라바이트의/빅데이터를/보유하고/있고,/한편,/건강보험심사평가원은/진료내역,/투약내용(의약품/안심서비스),/의약품/유통/등의/2조2천억건,/89테라바이트의/빅데이터를/보유하고/있으며,/경제협력개발기구(OECD)는/한국의/건강보험/빅데이터/순위가/2위라고/발표했었다./건보공단과/심평원은/빅데이터를/민간에/널리/알리고/더/많이/개방하고/있다./(연합뉴스/2016.6.14/인터넷뉴스/참조)/빅/데이터를/활용하면/미국/의료부문은/연간/3,300/억/달러(미/정부/의료/예산의/약/8%에/해당하는/규모)의/직간접적인/비용/절감/효과를/보일/것으로/전망된다.[28]/특히/임상분야에서는/의료기관/별/진료방법,/효능,/비용/데이터를/분석하여/보다/효과적인/진료방법을/파악하고/환자/데이터를/온라인/플랫폼화하여/의료협회/데이터/공유로/치료/효과를/제고하며/공중보건/영역에선/전국의/의료/데이터를/연계하여/전염병/발생과/같은/긴박한/순간에/빠른/의사결정을/가능케/할/전망이다.[29]/한편,/의료/분야에서/빅/데이터가/효과를/발휘하기/위해서는/대량의/의료정보/수집이/필수적이기/때문에,/개인정보의/보호와/빅/데이터/활용이라는/가지/가치가/상충하게/되된다./따라서,/의료/분야에서/빅/데이터의/활용과/보급을/위해서는/이러한/문제에/대한/가이드라인/마련이/필요한/상태이다.[30]/기업/경영[편집]/대규모의/다양한/데이터를/활용한/'빅데이터/경영'이/주목받으면서/데이터/품질을/높이고/방대한/데이터의/처리를/돕는/데이터/통합(Data/Integration)의/중요성이/부각되고/있다./데이터/통합(DI)은/데이터의/추출,/변환,/적재를/위한/ETL/솔루션이/핵심인데/ETL/솔루션을/활용하면/일일이/수많은/데이터를/기업/데이터/포맷으로/코딩하지/않아도/되고/데이터/품질을/제고할/수/있기/때문에/DI는/빅데이터/환경에/꼭/필요한/데이터/솔루션으로/평가받고/있는/단계까지/진입되었다./한편/비즈니스/인텔리전스(Business/Intelligence,/BI)보다/진일보한/빅데이터/분석/방법이/비즈니스/애널리틱스(Business/analytics,/BA)인데/고급분석/범주에/있는/BA는/기본적으로/BI를/포함하면서도/미래/예측/기능과/통계분석,/확률/분석/등을/포함해/최적의/데이터/기반/의사결정을/가능케/하는/것으로/평가받고/있기도/하다.[31]/마케팅[편집]/인터넷으로/시작해서/인터넷으로/마감하는/생활,/스마트폰을/이용해/정보를/검색하고/쇼핑도하고/SNS를/이용해서/실시간으로/글을/남기는/등의/다양하게/인터넷을/이용하는/동안/남는/흔적같은/모인/데이터들을/분석하면/개인의/생활/패턴,/소비성향/등을/예측할/수/있고/기업들은/데이터를/통해서/소비자가/원하는/것들을/미리/예측할/수/있다./빅/데이터가/마케팅/자료로/활용되는/사례이다.[31]/기상정보[편집]/한반도/전역의/기상관측정보를/활용해/일기예보와/각종/기상특보/국가/기상서비스를/제공하고/있는/기상청은/정밀한/기상예측을/위한/분석/과정에서/발생하는/데이터/폭증에/대응하고자/빅데이터/저장시스템의/도입을/추진하였다./대다수/스토리지/기업들의/솔루션을/검토한/끝에/한국/IBM의/고성능/대용량/파일공유시스템(General/Parallel/File/System,/이하/GPFS)을/적용한/스토리지/시스템을/선택하였다고/밝혔다./한국IBM이/기상청에/제공한/GPFS/기반의/빅데이터/저장시스템은/IBM/시스템/스토리지/제품군,/시스템/x서버/제품군과/고속/네트워킹/랙스위치(RackSwitch)/등이/통합돼/있는/시스템이다.[31]/보안관리[편집]/보안관리는/빅데이터/환경을/이용해/성장과/기술/발전을/동시에/이루는/분야로/분리한다./클라우드/모바일/환경으로/접어들면서/물리/가상화/IT/시스템의/복잡성이/더욱/높아지고/있어/유무선/네트워크,/프라이빗/퍼블릭/클라우드,/모바일/애플리케이션과/기기관리/IT/시스템/전반에서/대대적인/변화가/예상되고/있어/막대한/양의/보안관리가/중요한/요소로/현실화되고/있다.[32]/구글/번역[편집]/구글에서/제공하는/자동/번역/서비스인/구글/번역은/빅/데이터를/활용한다./지난/40년/컴퓨터/회사/IBM의/자동/번역/프로그램/개발은/컴퓨터가/명사,/형용사,/동사/단어와/어문의/문법적/구조를/인식하여/번역하는/방식으로/이뤄졌다./달리/2006년/구글은/수억/건의/문장과/번역문을/데이터베이스화하여/번역시/유사한/문장과/어구를/기존에/축적된/데이터를/바탕으로/추론해/나가는/통계적/기법을/개발하였다./캐나다/의회의/수백만/건의/문서를/활용하여/영어-불어/자동번역/시스템개발을/시도한/IBM의/자동/번역/프로그램은/실패한/반면/구글은/수억/건의/자료를/활용하여/세계/58개/언어/간의/자동번역/프로그램/개발에/성공하였다./이러한/사례로/미루어/볼/때,/데이터/양의/측면에서의/엄청난/차이가/기업의/자동/번역/프로그램의/번역의/질과/정확도를/결정했으며,/나아가/프로젝트의/성패를/좌우했다고/볼/수/있다.[31]/"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#f=open(os.path.join(\"data\", \"ds_bigdata_wiki.txt\"))\n",
    "f=open(os.path.join(\"data\", \"ds_bigdata_wiki.txt\"), 'rt', encoding='utf8')\n",
    "\n",
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])\n",
    "d = dict()\n",
    "for sent in f.readlines():\n",
    "    _words = sent.split()  # split into words\n",
    "    for word in _words:\n",
    "        if word not in stopwords: # remove stopwords\n",
    "            print(word, end='/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어빈도\n",
    "\n",
    "불용어를 제거하는 기능을 확인했으니, 이제 print문 자리에 단어빈도를 계산하는 로직을 넣어주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xed in position 10: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [52], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds_bigdata_wiki.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xed in position 10: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "f=open(os.path.join(\"data\", \"ds_bigdata_wiki.txt\"))\n",
    "f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp949 인코딩 오류는 주로 한글 텍스트 파일을 읽을 때 발생하는데, utf-8로 읽으면 간단히 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(\"data\", \"ds_bigdata_wiki.txt\"), 'r', encoding='utf-8') as f:\n",
    "   f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])\n",
    "d = dict()\n",
    "\n",
    "with open(os.path.join(\"data\", \"ds_bigdata_wiki.txt\"), 'r', encoding='utf-8') as f:\n",
    "    for sent in f.readlines():\n",
    "        _words = sent.split()  # split into words\n",
    "        for word in _words:\n",
    "            if word not in stopwords: # remove stopwords\n",
    "                if word not in d:\n",
    "                    d[word]=1\n",
    "                else:\n",
    "                    d[word]=d[word]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정렬\n",
    "\n",
    "단어빈도는 Python dictionary로 저장되어 있다.\n",
    "이를 정렬하여 다시 dictionary로 저장하자.\n",
    "\n",
    "dictionary는 key-value 쌍으로 구성되어 있다.\n",
    "저장된 데이터는 반복문을 사용하여:\n",
    "* **d.items()**으로 하나씩 key, value를 읽고\n",
    "* key=lambda x:x[1]은 값을 키로해서,\n",
    "* ```reverse=True```는 내림차순으로 읽는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSorted = {k: v for k, v in sorted(d.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "지금은 **전처리**를 하지 않고 처리하고 있다.\n",
    "교착어인 한국어의 특성을 감안해서 품사처리를 해야할 필요가 있다.\n",
    "이음동의도 어떻게 분류할 것인지 감안해야 한다.\n",
    "전처리를 하지 않은 문제점을 몇 가지 나열하자면:\n",
    "* '빅데이터'와 '빅', '데이터' 다른 단어로 인식한다. 물론 첫 줄에 영어로 씌인 'Big data'를 인식하지 못하는 것은 물론이다.\n",
    "* '대한'의 의미가 모호하다. '대한민국'인지, '~에 대한'의 의미인지 불분명하다.\n",
    "* 다 수의 한 글자 단어가 계산되었지만 의미가 모호하다.\n",
    "    * '등'이 사람 신체의 부분을 말하는 것인지 '~하는 등'인지 불분명하다.\n",
    "    * '수'란 단어 역시 '개수'를 의미하는 것인지 '~할 수'에 쓰이는 단어인지 불분명하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일정 빈도 이상을 선택하여 출력\n",
    "\n",
    "Python2의 d.iteritems()는 Python3에서는 items()로 변경되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터\t21\n",
      "데이터를\t18\n",
      "빅\t14\n",
      "있다.\t9\n",
      "수\t8\n",
      "데이터의\t8\n",
      "미국\t7\n",
      "통해\t7\n",
      "유권자\t6\n",
      "선거\t6\n",
      "대한\t6\n",
      "빅데이터\t6\n"
     ]
    }
   ],
   "source": [
    "d1 = dict()\n",
    "for key, value in dSorted.items():\n",
    "    if value>5:\n",
    "        d1[key]=value\n",
    "        print (f\"{key}\\t{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Pyspark로 단어빈도를 세어보기\n",
    "\n",
    "* RDD 생성\n",
    "\n",
    "파일로부터 testFile()함수를 이용해 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myRdd3=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_bigdata_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data\n",
      "활용사례 및 의의[편집]\n"
     ]
    }
   ],
   "source": [
    "for i in myRdd3.take(2):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### flatMap vs map\n",
    "\n",
    "flatMap()은 리스트안에 또 리스트가 있는 경우 이를 **하나의 리스트**로 만든다.\n",
    "반면에 map()은 **리스트 안에 또 리스트가 있는 구조를 보존**하고 처리한다.\n",
    "다음에서 보듯이 flatMap()은 모든 단어를 하나의 리스트로 만들고 있다.\n",
    "그러나 map() 파일의 줄마다 리스트를 만든다.\n",
    "\n",
    "줄 | 원본 | flatMap() 하고 나면 | map() 하고 나면\n",
    "-----| -----|-----|-----\n",
    "1 | Big data | 좌동 | [ 'Big', 'data' ]\n",
    "2 | 활용사례 및 의의[편집] | 좌동 | [ '활용사례', '및', '의의[편집]' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big/data/활용사례/및/의의[편집]/정치/및/사회[편집]/2008년/미국/"
     ]
    }
   ],
   "source": [
    "for i in wc3:\n",
    "    print (i, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "리스트를 출력해보자.\n",
    "반복문으로 리스트를 출력해도 2차원이라서 리스트를 완전하게 해체하지 못했다.\n",
    "데이터를 출력하려면 리스트 당 1개의 반복문이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wc3=myRdd3\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Big', 'data']/['활용사례', '및', '의의[편집]']/['정치', '및', '사회[편집]']/"
     ]
    }
   ],
   "source": [
    "for i in wc3:\n",
    "    print (i, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**리스트 안에 또 리스트**가 있는 구조이므로, \n",
    "for문을 중첩하여 출력하자. print() 문은 한글을 그대로 유니코드로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big/data/활용사례/및/의의[편집]/정치/및/사회[편집]/"
     ]
    }
   ],
   "source": [
    "for i in wc3:\n",
    "    for j in i:\n",
    "        print (j, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### stopwords 제거\n",
    "\n",
    "불용어를 처리해 보자. 한글은 유니코드로 영어는 소문자로 변환한 후 불용어는 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])\n",
    "wc3_stop1 = myRdd3\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .filter(lambda x: x.lower() not in stopwords)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big/data/활용사례/의의[편집]/정치/사회[편집]/2008년/미국/대통령/선거[편집]/"
     ]
    }
   ],
   "source": [
    "for i in wc3_stop1:\n",
    "    print (i, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단어 빈도\n",
    "\n",
    "당연하겠지만, 우선 각 단어마다 빈도1의 값을 주게 된다.\n",
    "다음에서 보듯이, **(단어,1)**로 만들게 된다.\n",
    "flatMap()을 하지 않고, **단순하게 map()**을 하여 갯수를 세면 **문장**으로 세어진다.\n",
    "\n",
    "줄 | map()을 한 경우 | flatMap()을 한 경우\n",
    "-----|-----|-----\n",
    "1 | (u'Big data', 1) | (u'Big', 1) (u'data', 1)\n",
    "2 | (u'\\ud65c\\uc6a9\\uc0ac\\ub840 \\ubc0f \\uc758\\uc758[\\ud3b8\\uc9d1]', 1) | (u'\\ud65c\\uc6a9\\uc0ac\\ub840', 1) (u'\\ubc0f', 1) (u'\\uc758\\uc758[\\ud3b8\\uc9d1]', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Big data', 1)/('활용사례 및 의의[편집]', 1)/('정치 및 사회[편집]', 1)/"
     ]
    }
   ],
   "source": [
    "wc3=myRdd3\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .take(3)\n",
    "for i in wc3:\n",
    "    print (i, end='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Big', 1)/('data', 1)/('활용사례', 1)/('및', 1)/('의의[편집]', 1)/('정치', 1)/('및', 1)/('사회[편집]', 1)/('2008년', 1)/('미국', 1)/"
     ]
    }
   ],
   "source": [
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .take(10)\n",
    "for i in wc3:\n",
    "    print (i, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 묶어서\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "flatMap | 파일 전체를 (flat해서) map. flat하지 않으면 줄바꿈으로 구분함.\n",
    "fliter | 불용어 목록에 있으면 제거함.\n",
    "map | 단어별로 **(x,1)**로 구성함\n",
    "reduceByKey | 동일한 단어(키)의 value, 즉 갯수를 서로 합하게 됨.\n",
    "map() | 앞 함수의 출력 (x,1)의 순서를 바꿈. 즉, 갯수x[1]를 앞으로 단어x[1]을 뒤로 자리 바꿈.\n",
    "sortByKey(false) | **내림차순** 정렬 (오름차순이 default)\n",
    "take(10) | 10개를 선택함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = set(['및','이를','등','이','이런','그와','또는','두', '이와', '전', '간'])\n",
    "\n",
    "wc3=myRdd3\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .filter(lambda x: x.lower() not in stopwords)\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "21 데이터\n",
      "18 데이터를\n",
      "14 빅\n",
      "9 있다.\n",
      "8 수\n",
      "8 데이터의\n",
      "7 미국\n",
      "7 통해\n",
      "6 유권자\n",
      "6 선거\n",
      "6 대한\n",
      "6 빅데이터\n",
      "5 활용한\n",
      "5 소셜\n",
      "5 대한민국\n"
     ]
    }
   ],
   "source": [
    "print (type(wc3))\n",
    "for i in wc3:\n",
    "    print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단어빈도로 가로막대그래프 그리기\n",
    "\n",
    "2차원으로 그래프를 작성하는 경우, 막대 또는 라인으로 그릴 것인지 결정해야 한다.\n",
    "막대그래프는 명목변수에 대해 측정 값을,\n",
    "라인그래프는 연속변수에 대해 측정 값의 추세를 나타낼 경우 유용하다.\n",
    "\n",
    "가로막대 그래프를 그려보자.\n",
    "그러면 x와 y 값을 설정해주어야 한다.\n",
    "x축은 ```barh()```로 개수를 나타내고\n",
    "y축은 단어를 나타낸다.\n",
    "\n",
    "이러한 x, y 값은 앞서 다음과 같은 [빈도, 단어]로부터 추출해야 한다. \n",
    "```\n",
    "[ 21, '데이터' ]\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "k=list()\n",
    "v=list()\n",
    "for i in wc3:\n",
    "    v.append(i[0])\n",
    "    k.append(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가로막대그래프 y축 위치는 range(len(v)) 즉 0~14까지, x는 v값으로 잡는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumMyeongjo')\n",
    "plt.barh(range(len(v)), v, color = 'black')  # x <- from (v)alues\n",
    "plt.yticks(range(len(v)), k)                 # y <- from (k)eys\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Nanum' 또는 'Myeongjo' 한글 폰트가 설치되었는지 확인하고, 그 중의 적당한 폰트를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanumMyeongjo /usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf\n",
      "NanumMyeongjo /usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import font_manager\n",
    "for font in font_manager.fontManager.ttflist:\n",
    "    if 'Myeongjo' in font.name:\n",
    "        print(font.name, font.fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combineByKey\n",
    "\n",
    "combineByKey()는 데이터를 특정 키를 기준으로 집단화하고, 집단별로 집계 작업을 수행하는 데 사용됩니다.\n",
    "\n",
    "키별로 집단화하고, 집단별 합계 및 갯수 ```(key, (sum, count))```를 계산한다.\n",
    "\n",
    "구분 | combiner | merge values | merge combiner\n",
    "-----|-----|-----|-----\n",
    "설명 | 새로운 키가 발견될 때마다 한 번 호출된다. 각 키에 대해 **```(value,1)```** 튜플 만든다 | 동일한 키의 값들을 병합하고, 값을 더해나감 (sum,count)<br>즉, **```sum+value```**,**```count+1```** | 다른 파티션에서 생성된 결합자를 병합하는 함수로, partition별로 combiner를 더함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **데이터 생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 ```_testRdd```를 사용해서 combineByKey()를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_list1=[(\"key1\",1),(\"key1\",3),(\"key2\",2),(\"key1\",2),(\"key2\",4),\n",
    "           (\"key1\",5),(\"key2\",6),\n",
    "           (\"key1\",7),(\"key1\",8),(\"key2\",9),(\"key2\",3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **partition이 1개이면 combiner, mergeValues만 작동**\n",
    "\n",
    "partition이 1개인 경우를 먼저 해보자. Rdd를 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_testRdd1=spark.sparkContext.parallelize(_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 partition은 1개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partition이 1개이면 키별로 값을 더해간다. 키가 존재하지 않으면 combiner()를, 존재하면 mergeValues()가 실행된다.\n",
    "* (key1, 1) key1은 처음 등장하는 키이므로 accumulator[key1] = createCombiner(1)\n",
    "* (key1, 3) key1은 존재하는 키이므로 accumulator[key1] = mergeValue(accumulator[key1], 3)\n",
    "* (key2, 2) key2는 처음 등장하는 키이므로 accumulator[key2] = createCombiner(2)\n",
    "* (key1, 2) key1은 존재하는 키이므로 accumulator[key1] = mergeValue(accumulator[key1], 2)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 출력결과를 해석해보자.\n",
    "key1의 값은 1,3,2,4,7,8이고 key2의 값은 2,4,6,9,3이다.\n",
    "앞서 설명대로:\n",
    "* key1의 첫째 값은 1이다. 이경우 combiner (```*```표기), 다음은 merge values (```#``` 표기)로 계산이 된다. 따라서 ```1*#3#2#5#7#8```가 출력된다.\n",
    "* key2는 ```2*#4#6#9#3```가 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', '1*#3#2#5#7#8'), ('key2', '2*#4#6#9#3')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd1.combineByKey(lambda v : str(v)+\"*\", lambda c, v : c+\"#\"+str(v), lambda c1, c2 : c1+'&'+c2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 서울,부산,분당에 대한 데이터에 대해 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seoul', '1*#1#1#1#1#1#1'), ('Busan', '1*#1#1#1#1#1'), ('Bundang', '1*#1#1')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.combineByKey(lambda v : str(v)+\"*\", lambda c, v : c+\"#\"+str(v), lambda c1, c2 : c1+'&'+c2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **partition이 복수이면 mergeCombiner가 작동**\n",
    "\n",
    "partition이 복수인 경우에는 mergeCombiner가 작동한다. 먼저 partition을 2개로 해서 Rdd를 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_testRdd2=spark.sparkContext.parallelize(_list1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions 0 -> [('key1', 1), ('key1', 3), ('key2', 2), ('key1', 2), ('key2', 4)]\n",
      "Partitions 1 -> [('key1', 5), ('key2', 6), ('key1', 7), ('key1', 8), ('key2', 9), ('key2', 3)]\n"
     ]
    }
   ],
   "source": [
    "partitions = _testRdd2.glom().collect()\n",
    "for num, partition in enumerate(partitions):\n",
    "    print(f'Partitions {num} -> {partition}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partition이 분할 되었으므로, partition별로 합산된다.\n",
    "* key1은 partition 0에서 1,3,2 그리고 partition 1에서 5,7,8이 연산된다. 그 결과 ```1*#3#2&5*#7#8```\n",
    "* key2는 2,4,6과 6,9,3이 각 각 다른 partition에서 연산되어 ```2*#4&6*#9#3```이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', '1*#3#2&5*#7#8'), ('key2', '2*#4&6*#9#3')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd2.combineByKey(lambda v : str(v)+\"*\", lambda c, v : c+\"#\"+str(v), lambda c1, c2 : c1+'&'+c2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **실제 해보기**\n",
    "\n",
    "앞서 사용했던 기호를 연산자로 변경해서, 실제 해보자.\n",
    "그 결과는 합계, 개수를 구할 수 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', (26, 6)), ('key2', (24, 5))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd1.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),\n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **평균 계산**\n",
    "\n",
    "combineByKey()함수로 **sum**, **count**를 구하게 된다. 그리고 map()함수로 **sum/count**를 계산하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_testCbkRdd=_testRdd1.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),                      \n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "과거에는 아래와 같이 lambda 함수의 인자를 별도로 3개 받았으나, 지금은 하나를 받도록 프로그램하자.\n",
    "\n",
    "```python\n",
    "averageByKey = _testCbkRdd.map(lambda (key,(sum,count)):(key,float(sum)/count))\n",
    "averageByKey.collectAsMap()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key1': 4.333333333333333, 'key2': 4.8}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByKey = _testCbkRdd.map(lambda x:(x[0],x[1][0]/x[1][1]))\n",
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### combineByKey 예제\n",
    "\n",
    "* combineByKey(x, y, z) - groupByKey()는 값 합계를 미리 계산하지 때문에 비효율적이다.\n",
    "* partition별로 무작위 배분되어 실행되나, 아래는 하나의 partition을 가정하고 예제를 설명한다.\n",
    "    * partition A: ('kim',86),('lim',87),('kim',91),('lim',79)\n",
    "    * partition B: ('kim',75),('lim',78),('lim',92),('lee',99)\n",
    "\n",
    "구분 | 함수명 | 설명\n",
    "-----|-----|-----\n",
    "x | Combiner 함수 | 값을 combine (V) -> C 예: (value,1)\n",
    "y | Merge value 함수 | 값을 merge (C, V) -> C 예: (sum,count)\n",
    "z | Merge combiners 함수 | combiner를 merge (C, C) -> C) 예: (K, C)\n",
    "\n",
    "데이터 | 적용 함수 | 결과\n",
    "-----|-----|-----\n",
    "('kim',86) | combiner | accum[kim],(86,1)\n",
    "('lim',87) | combiner | accum[lim],(87,1)\n",
    "('kim',75) | merge value | (accum[kim],75) -> accum[kim],(86+75,1+1) = (161,2)\n",
    "('kim',91) | merge value | (accum[kim],91) -> (161+91,2+1) = (252,3)\n",
    "('lim',78) | merge value | (accum[lim],78) -> (87+78,1+1) = (165,2)\n",
    "('lim',92) | merge value | (accum[lim],92) -> (165+92,2+1) = (257,3)\n",
    "('lim',79) | merge value | (accum[lim],79) -> (257+79,3+1) = (336,4)\n",
    "('lee',99) | combiner | accum[lee],(99,1)\n",
    "partition별 합산 | merge combiners | [('lim', (336, 4)), ('lee', (99, 1)), ('kim', (252, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),\n",
    "                                      ('kim',91),('lim',78),('lim',92),\n",
    "                                      ('lim',79),('lee',99)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marksByKey = marks.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kim', (252, 3)), ('lim', (336, 4)), ('lee', (99, 1))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "heightsByKey = heights.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 179.5, 'F': 163.0}\n"
     ]
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda x: (x[0],x[1][0]/x[1][1]))\n",
    "\n",
    "print (avgByKey.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 성적 합계 및 평균.\n",
    "\n",
    "아래 데이터를 RDD로 만들고, 성적의 합계 및 평균을 계산하세요.\n",
    "하위 문제별로 RDD를 생성하지 말고, 원본데이터에서 하나의 RDD만을 생성하고, 이를 변형해서 문제를 푸세요.\n",
    "\n",
    "이름 | 과목 | 점수\n",
    "-----|-----|-----\n",
    "김하나 | English | 100\n",
    "김하나 | Math | 80\n",
    "임하나 | English | 70\n",
    "임하나 | Math | 100\n",
    "김갑돌 | English | 82.3\n",
    "김갑돌 | Math | 98.5\n",
    "\n",
    "* 문제 3-1: 이름으로 합계를 구해보자. 올바른 출력은 다음과 같다.\n",
    "**이름과 점수로 데이터를 추출**하고, **이름별로 (이름을 키로)** 합계를 계산한다.\n",
    "\n",
    "```python\n",
    "'임하나' 170.0\n",
    "'김하나' 180.0\n",
    "'김갑돌' 180.8\n",
    "```\n",
    "* 문제 3-2: 과목으로 합계를 계산해 보자. 출력은 다음과 같이 나와야 한다.\n",
    "**과목과 점수로 데이터를 추출**하여, **과목별로 (과목을 키로)** 합계를 계산한다.\n",
    "\n",
    "```python\n",
    "'English' 252.3\n",
    "'Math' 278.5\n",
    "```\n",
    "\n",
    "* 문제 3-3: 이름으로 합계과 개수를 구해보자. 출력은 다음과 같이 계산된다.\n",
    "**이름과 점수로 데이터를 추출**하여, **이름별로 (이름을 키로)** 합계와 개수를 계산한다.\n",
    "\n",
    "```python\n",
    "'임하나' (170.0, 2)\n",
    "'김하나' (180.0, 2)\n",
    "'김갑돌' (180.8, 2)\n",
    "```\n",
    "\n",
    "* 문제 3-4: 이름으로 평균을 계산해 보자. 앞서 3-3에서 사용했던 결과를 활용하고, 올바른 출력은 다음과 같다.\n",
    "\n",
    "```python\n",
    "'임하나' 85.0\n",
    "'김하나' 90.0\n",
    "'김갑돌' 90.4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "우선 데이터를 만들어 보자. 이름, 과목, 성적을 개인별로 넣어 2차원 리스트로 만들어졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"'김하나','English', 100\",\n",
    "    \"'김하나','Math', 80\",\n",
    "    \"'임하나','English', 70\",\n",
    "    \"'임하나','Math', 100\",\n",
    "    \"'김갑돌','English', 82.3\",\n",
    "    \"'김갑돌','Math', 98.5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_marksRdd=spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 문제 3-1\n",
    "\n",
    "개인별 데이터를 컴마로 분리하고, 이름 x[0]과 성적 x[2]만 꺼내어 reduceBykey()를 구하면 합계를 구할 수 있다.\n",
    "여기서 중요한 것은 기존 데이터에서 필요한 이름, 성적만을 꺼내어 처리한다는 점이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# marks by name\n",
    "_marksbyname=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[0],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김하나' 180.0\n",
      "'임하나' 170.0\n",
      "'김갑돌' 180.8\n"
     ]
    }
   ],
   "source": [
    "for i in _marksbyname:\n",
    "  print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 문제 3-2\n",
    "\n",
    "개인별 데이터를 컴마로 분리하고, 과목 x[1]과 성적 x[2]만 꺼내어 reduceBykey()를 구하면 합계를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'English' 252.3\n",
      "'Math' 278.5\n"
     ]
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksbysubject=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()\n",
    "for i in _marksbysubject:\n",
    "  print (i[0],i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단계별로 풀어보기\n",
    "\n",
    "방금 실행했던 것처럼, 한 번에 스르르 문제가 풀리지는 않는다.\n",
    "단계별로 데이터를 먼저 변환하고, 그리고 문제에서 요구하는 합산을 하도록 한다.\n",
    "우선 데이터를 변환하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'김하나'\", \"'English'\", ' 100'],\n",
       " [\"'김하나'\", \"'Math'\", ' 80'],\n",
       " [\"'임하나'\", \"'English'\", ' 70'],\n",
       " [\"'임하나'\", \"'Math'\", ' 100'],\n",
       " [\"'김갑돌'\", \"'English'\", ' 82.3'],\n",
       " [\"'김갑돌'\", \"'Math'\", ' 98.5']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과목별 합계가 목적이므로 과목별 성적으로 변환해 놓아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'English'\", 100.0),\n",
       " (\"'Math'\", 80.0),\n",
       " (\"'English'\", 70.0),\n",
       " (\"'Math'\", 100.0),\n",
       " (\"'English'\", 82.3),\n",
       " (\"'Math'\", 98.5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],float(x[2])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* combineByKey를 적용하면?\n",
    "\n",
    "다음으로 어떤 함수를 적용하면 될까? 과목별 합계는 ```reduce```를 하면된다.\n",
    "합계, 개수를 구한다고 combineByKey를 적용하면 어떻게 될까?\n",
    "float 타입으로 combineByKey를 하면 ```'float' object is not subscriptable``` 오류가 발생한다.\n",
    "그렇다고 str으로 한다고 해서 문제가 풀리는 것은 아니다.\n",
    "combineByKey 함수는 합계를 계산할 수 없기 때문에 최종 값만 남기게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (192.168.219.151 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 3964, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\jsl\\AppData\\Local\\Temp\\ipykernel_21960\\3061361987.py\", line 6, in <lambda>\nTypeError: can only concatenate str (not \"int\") to str\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 3964, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\jsl\\AppData\\Local\\Temp\\ipykernel_21960\\3061361987.py\", line 6, in <lambda>\nTypeError: can only concatenate str (not \"int\") to str\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# marks by subject\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43m_marksRdd\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (192.168.219.151 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 3964, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\jsl\\AppData\\Local\\Temp\\ipykernel_21960\\3061361987.py\", line 6, in <lambda>\nTypeError: can only concatenate str (not \"int\") to str\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\rdd.py\", line 3964, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\jsl\\AppData\\Local\\Temp\\ipykernel_21960\\3061361987.py\", line 6, in <lambda>\nTypeError: can only concatenate str (not \"int\") to str\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],str(x[2])))\\\n",
    "    .combineByKey(lambda value: (value,1),\n",
    "                  lambda x, value: (x[0]+value + x[1]+1),\n",
    "                  lambda x, y: x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduceByKey를 적용하면?\n",
    "\n",
    "reduceByKey는 문자열에 대해 이어 붙이기를 하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'English'\", ' 100 70 82.3'), (\"'Math'\", ' 80 100 98.5')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],str(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reduce를 적용하면?\n",
    "\n",
    "reduce를 적용하면 어떻게 될까? 하나의 값으로 줄이려고 하기 때문에 합산이 되지 않는다.\n",
    "이런 시행착오를 거치는 것은 어떻게 보면 필요하다.\n",
    "문제는 단 번에 풀리지 않을 수 있다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'English'\",\n",
       " 100.0,\n",
       " \"'Math'\",\n",
       " 80.0,\n",
       " \"'English'\",\n",
       " 70.0,\n",
       " \"'Math'\",\n",
       " 100.0,\n",
       " \"'English'\",\n",
       " 82.3,\n",
       " \"'Math'\",\n",
       " 98.5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],float(x[2])))\\\n",
    "    .reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 문제 3-3\n",
    "\n",
    "합계, 개수를 계산해 보자. combineByKey()를 이용해서 계산해야 한다.\n",
    "먼저 데이터를 **이름, 과목, 데이터** -> **이름, 점수**로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# marks by name\n",
    "_marksbyname2=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[0],float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# sum, counts by name\n",
    "sum_counts = _marksbyname2.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김하나' (180.0, 2) \n",
      "'임하나' (170.0, 2) \n",
      "'김갑돌' (180.8, 2) \n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print (each, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 문제 3-4\n",
    "\n",
    "개인별 평균은 3-3에서 구했던 합계, 개수를 사용하여 계산한다.\n",
    "평균을 계산하기 위해 float() 형변환을 해주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'김하나' 90.0 \n",
      "'임하나' 85.0 \n",
      "'김갑돌' 90.4 \n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "averageByKey = sum_counts\\\n",
    "    .map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    "    .collect()\n",
    "for i in averageByKey:\n",
    "    for j in i:\n",
    "        print (j, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 서울시 지하철호선별 승차인원 평균 구하기.\n",
    "\n",
    "### 문제\n",
    "\n",
    "정량데이터는 보통 집단화하고 빈도, 평균, 합계 등 서술통계를 계산한다.\n",
    "\n",
    "'별'이란 집단화하는 기준이고, 여기서는 '호선'으로 집단화하여 승차인원의 평균을 계산한다.\n",
    "\n",
    "데이터는 서울시 지하철호선별 역별 승하차 인원 정보 파일을 읽어서 가져오고, 그로부터 평균을 구해보자.\n",
    "\n",
    "* 파일 명 CARD_SUBWAY_MONTH_201501.csv를 다운로드 받아서 일부만 테스트용 데이터로 사용한다.\n",
    "* 참고로 오픈API 샘플URL http://openapi.seoul.go.kr:8088/(인증키)/xml/CardSubwayStatsNew/1/5/20151101\n",
    "\n",
    "### 해결\n",
    "\n",
    "집단화하기 위해서는 PairRDD로 만들면 해결할 수 있다. 그리고 나서 집단별로 평균을 계산할 수 있다.\n",
    "평균을 계산하려면, 전체 합계와 개수가 있어야 가능하다.\n",
    "\n",
    "평균을 계산하면 아래와 같이 작성된다.\n",
    "\n",
    "```\n",
    "[('2호선', 10529.0),\n",
    " ('3호선', 9236.0),\n",
    " ('4호선', 5704.0),\n",
    " ('경부선', 19989.6),\n",
    " ('경원선', 1194.75)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "* 2015년 1월 csv파일의 일부를 가져와서 리스트를 생성한다.\n",
    "* 데이터 헤더: \"사용일자\",\"노선명\",\"역ID\",\"역명\",승차총승객수,하차총승객수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_sub=[\"20150101,2호선,0236,영등포구청,6199,6219\",\n",
    "\"20150101,2호선,0237,당산,7982,8946\",\n",
    "\"20150101,2호선,0238,합정,17406,15241\",\n",
    "\"20150101,3호선,0309,지축,515,538\",\n",
    "\"20150101,3호선,0310,구파발,6879,6260\",\n",
    "\"20150101,3호선,0311,연신내,20031,19470\",\n",
    "\"20150101,3호선,0312,불광,9519,11029\",\n",
    "\"20150101,4호선,0425,회현,7465,7574\",\n",
    "\"20150101,4호선,0426,서울역,3943,10823\",\n",
    "\"20150101,경부선,1002,남영,4340,4535\",\n",
    "\"20150101,경부선,1003,용산,28980,27684\",\n",
    "\"20150101,경부선,1004,노량진,23021,23862\",\n",
    "\"20150101,경부선,1005,대방,6360,6476\",\n",
    "\"20150101,경부선,1006,영등포,37247,36102\",\n",
    "\"20150101,경원선,1008,이촌,1940,1507\",\n",
    "\"20150101,경원선,1009,서빙고,911,1000\",\n",
    "\"20150101,경원선,1010,한남,1885,1863\",\n",
    "\"20150101,경원선,1011,옥수,43,37\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_subRdd = spark.sparkContext.parallelize(_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 읽혀졌는지 한 줄만 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150101,2호선,0236,영등포구청,6199,6219\n"
     ]
    }
   ],
   "source": [
    "for i in _subRdd.take(1):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 데이터 정리\n",
    "\n",
    "* map()을 사용해서 컴마를 제외할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150101 2호선 0236 영등포구청 6199 6219 \n",
      "20150101 2호선 0237 당산 7982 8946 \n",
      "20150101 2호선 0238 합정 17406 15241 \n",
      "20150101 3호선 0309 지축 515 538 \n",
      "20150101 3호선 0310 구파발 6879 6260 \n",
      "20150101 3호선 0311 연신내 20031 19470 \n",
      "20150101 3호선 0312 불광 9519 11029 \n",
      "20150101 4호선 0425 회현 7465 7574 \n",
      "20150101 4호선 0426 서울역 3943 10823 \n",
      "20150101 경부선 1002 남영 4340 4535 \n",
      "20150101 경부선 1003 용산 28980 27684 \n",
      "20150101 경부선 1004 노량진 23021 23862 \n",
      "20150101 경부선 1005 대방 6360 6476 \n",
      "20150101 경부선 1006 영등포 37247 36102 \n",
      "20150101 경원선 1008 이촌 1940 1507 \n",
      "20150101 경원선 1009 서빙고 911 1000 \n",
      "20150101 경원선 1010 한남 1885 1863 \n",
      "20150101 경원선 1011 옥수 43 37 \n"
     ]
    }
   ],
   "source": [
    "for i in _subRdd.map(lambda x:x.split(',')).collect():\n",
    "    for j in i:\n",
    "        print (j, end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 호선별 데이터 개수를 출력해 본다.\n",
    "* map()의 **3번째 인덱스는 앞에서부터 4번째 철자**를 출력한다.\n",
    "* 컴마로 분리해서 인덱스를 사용해야 제대로 데이터를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:int(x[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6199,\n",
       " 7982,\n",
       " 17406,\n",
       " 515,\n",
       " 6879,\n",
       " 20031,\n",
       " 9519,\n",
       " 7465,\n",
       " 3943,\n",
       " 4340,\n",
       " 28980,\n",
       " 23021,\n",
       " 6360,\n",
       " 37247,\n",
       " 1940,\n",
       " 911,\n",
       " 1885,\n",
       " 43]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:x.split(',')).map(lambda x:int(x[4])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 총 승차인원의 합계를 reduce()로 합산한다.\n",
    "\n",
    "* reduce()함수는 인자를 2개 받아서 합계를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:x.split(',')).map(lambda x:(int(x[4]))).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 호선별 승차인원 평균을 계산한다.\n",
    "\n",
    "* Pair RDD를 사용하므로, 사전에 데이터를 (호선, 승차인원) 형식으로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_subLineByPassengers=_subRdd.map(lambda x:x.split(',')).map(lambda x: (x[1],int(x[4])))\n",
    "sum_counts = _subLineByPassengers.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2호선 (31587, 3) \n",
      "3호선 (36944, 4) \n",
      "4호선 (11408, 2) \n",
      "경부선 (99948, 5) \n",
      "경원선 (4779, 4) \n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print (each, end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2호선 31587 3\n",
      "3호선 36944 4\n",
      "4호선 11408 2\n",
      "경부선 99948 5\n",
      "경원선 4779 4\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    print (i[0],i[1][0],i[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 위와 달리 map()을 사용하여 평균을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "averageByKey = sum_counts.map(lambda x: (x[0],x[1][0]/x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2호선', 10529.0),\n",
       " ('3호선', 9236.0),\n",
       " ('4호선', 5704.0),\n",
       " ('경부선', 19989.6),\n",
       " ('경원선', 1194.75)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByKey.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 spark-submit\n",
    "\n",
    "Spark를 설치하면, 일괄실행할 수 있는 ```spark-submit``` 명령어가 따라온다.\n",
    "따라서 별도의 sys.path 설정은 하지 않아도 된다.\n",
    "\n",
    "### log 설정\n",
    "\n",
    "spark-submit을 실행하면 로그가 출력되는데, 그 양이 상당하다.\n",
    "spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하자.\n",
    "\n",
    "spark가 설치된 디렉토리에서 ```conf/```로 이동하자.\n",
    "그 디렉토리에서 log4j.properties.templates를 찾아서, 아래 줄을 찾아 수정하자.\n",
    "\n",
    "```python\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME']=os.getcwd()\n",
    "os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "def doIt():\n",
    "    print (\"---------RESULT-----------\")\n",
    "    print (spark.version)\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print (\"mean=\",rdd.mean())\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print (\"{} \".format(num))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "일괄실행을 해보자. 실행주체는 (1) Python 인터프리터이거나 (2) 스파크 클러스터가 모두 가능하고, 우선 (1) Python으로 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------RESULT-----------\n",
      "3.4.1\n",
      "mean= 499.5\n",
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n",
      "SUCCESS: The process with PID 32848 (child process of PID 38264) has been terminated.\n",
      "SUCCESS: The process with PID 38264 (child process of PID 26648) has been terminated.\n",
      "SUCCESS: The process with PID 26648 (child process of PID 30020) has been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "[Stage 0:>                                                         (0 + 1) / 10]\n",
      "\n",
      "[Stage 0:=====>                                                    (1 + 1) / 10]\n",
      "\n",
      "[Stage 0:===========>                                              (2 + 1) / 10]\n",
      "\n",
      "[Stage 0:=================>                                        (3 + 1) / 10]\n",
      "\n",
      "[Stage 0:=======================>                                  (4 + 1) / 10]\n",
      "\n",
      "[Stage 0:=============================>                            (5 + 1) / 10]\n",
      "\n",
      "[Stage 0:==================================>                       (6 + 1) / 10]\n",
      "\n",
      "[Stage 0:========================================>                 (7 + 1) / 10]\n",
      "\n",
      "[Stage 0:==============================================>           (8 + 1) / 10]\n",
      "\n",
      "[Stage 0:====================================================>     (9 + 1) / 10]\n",
      "23/10/09 09:42:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "!python3 src/ds_spark_rdd_hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 스파크 클러스터에 위 파일을 넘겨 일괄 실행하도록 한다. 잠깐, 실행명령 spark-submit은 어디에서 온 것일까?\n",
    "\n",
    "윈도우에서는 where 명령으로 간단히 알 수 있다 (리눅스는 which)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\Scripts\\spark-submit\n",
      "C:\\Users\\jsl\\AppData\\Roaming\\Python\\Python39\\Scripts\\spark-submit.cmd\n"
     ]
    }
   ],
   "source": [
    "!where spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!spark-submit src/ds_spark_rdd_hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---------RESULT-----------\n",
    "3.4.1\n",
    "mean= 499.5\n",
    "1 \n",
    "4 \n",
    "9 \n",
    "16 \n",
    "\n",
    "환경변수를 읽지 못하고 있다. os.environ의 설정은 주피터노트북에서는 효과적, 여기서는 그렇지 않으니 외부에서 설정해야겠다.\n",
    "23/10/09 09:40:43 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "23/10/09 09:40:44 INFO SparkContext: Running Spark version 3.4.1\n",
    "23/10/09 09:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "23/10/09 09:40:45 INFO ResourceUtils: ==============================================================\n",
    "23/10/09 09:40:45 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
    "23/10/09 09:40:45 INFO ResourceUtils: ==============================================================\n",
    "23/10/09 09:40:45 INFO SparkContext: Submitted application: myApp\n",
    "컴퓨팅 자원을 할당하고 있다.\n",
    "23/10/09 09:40:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
    "23/10/09 09:40:45 INFO ResourceProfile: Limiting resource is cpu\n",
    "23/10/09 09:40:45 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
    "보안 설정을 하고 있다. 윈도우와 달리 유닉스/리눅스에서 기본으로 필요한 설정이다.\n",
    "23/10/09 09:40:45 INFO SecurityManager: Changing view acls to: jsl\n",
    "23/10/09 09:40:45 INFO SecurityManager: Changing modify acls to: jsl\n",
    "23/10/09 09:40:45 INFO SecurityManager: Changing view acls groups to: \n",
    "23/10/09 09:40:45 INFO SecurityManager: Changing modify acls groups to: \n",
    "23/10/09 09:40:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jsl; groups with view permissions: EMPTY; users with modify permissions: jsl; groups with modify permissions: EMPTY\n",
    "23/10/09 09:40:45 INFO Utils: Successfully started service 'sparkDriver' on port 53654.\n",
    "23/10/09 09:40:45 INFO SparkEnv: Registering MapOutputTracker\n",
    "23/10/09 09:40:45 INFO SparkEnv: Registering BlockManagerMaster\n",
    "23/10/09 09:40:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "23/10/09 09:40:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "23/10/09 09:40:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
    "임시 출력에 필요한 디렉토리 설정이다.\n",
    "23/10/09 09:40:46 INFO DiskBlockManager: Created local directory at C:\\Users\\jsl\\AppData\\Local\\Temp\\blockmgr-93eb2a31-9827-4400-97a3-4ea9e3cf67c4\n",
    "23/10/09 09:40:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
    "23/10/09 09:40:46 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "23/10/09 09:40:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
    "23/10/09 09:40:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
    "워커의 executor 설정이다.\n",
    "23/10/09 09:40:46 INFO Executor: Starting executor ID driver on host 192.168.219.151\n",
    "23/10/09 09:40:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
    "23/10/09 09:40:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53658.\n",
    "23/10/09 09:40:46 INFO NettyBlockTransferService: Server created on 192.168.219.151:53658\n",
    "23/10/09 09:40:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
    "23/10/09 09:40:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.219.151, 53658, None)\n",
    "23/10/09 09:40:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.219.151:53658 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.219.151, 53658, None)\n",
    "23/10/09 09:40:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.219.151, 53658, None)\n",
    "23/10/09 09:40:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.219.151, 53658, None)\n",
    "23/10/09 09:40:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
    "현재 디렉토리에 필요한 웨어하우스 설정이다.\n",
    "23/10/09 09:40:47 INFO SharedState: Warehouse path is 'file:/C:/Users/jsl/Code/201711111/spark-warehouse'.\n",
    "작업의 시작을 알리고 있다.\n",
    "23/10/09 09:40:48 INFO SparkContext: Starting job: mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14\n",
    "작업계획이 시작된다.\n",
    "23/10/09 09:40:48 INFO DAGScheduler: Got job 0 (mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14) with 10 output partitions\n",
    "23/10/09 09:40:48 INFO DAGScheduler: Final stage: ResultStage 0 (mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14)\n",
    "23/10/09 09:40:48 INFO DAGScheduler: Parents of final stage: List()\n",
    "23/10/09 09:40:48 INFO DAGScheduler: Missing parents: List()\n",
    "23/10/09 09:40:48 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14), which has no missing parents\n",
    "23/10/09 09:40:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.8 KiB, free 434.4 MiB)\n",
    "23/10/09 09:40:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.4 MiB)\n",
    "23/10/09 09:40:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.219.151:53658 (size: 4.8 KiB, free: 434.4 MiB)\n",
    "23/10/09 09:40:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
    "작업 tasks이 시작된다.\n",
    "23/10/09 09:40:48 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (PythonRDD[1] at mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
    "23/10/09 09:40:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks resource profile 0\n",
    "23/10/09 09:40:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.219.151, executor driver, partition 0, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
    "23/10/09 09:40:50 INFO PythonRunner: Times: total = 1135, boot = 1121, init = 13, finish = 1\n",
    "23/10/09 09:40:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1592 bytes result sent to driver\n",
    "23/10/09 09:40:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (192.168.219.151, executor driver, partition 1, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:50 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
    "23/10/09 09:40:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1351 ms on 192.168.219.151 (executor driver) (1/10)\n",
    "23/10/09 09:40:50 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 53659\n",
    "23/10/09 09:40:51 INFO PythonRunner: Times: total = 1159, boot = 1156, init = 2, finish = 1\n",
    "23/10/09 09:40:51 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1549 bytes result sent to driver\n",
    "23/10/09 09:40:51 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (192.168.219.151, executor driver, partition 2, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:51 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
    "23/10/09 09:40:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1205 ms on 192.168.219.151 (executor driver) (2/10)\n",
    "23/10/09 09:40:52 INFO PythonRunner: Times: total = 1041, boot = 1038, init = 2, finish = 1\n",
    "23/10/09 09:40:52 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1549 bytes result sent to driver\n",
    "23/10/09 09:40:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (192.168.219.151, executor driver, partition 3, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:52 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
    "23/10/09 09:40:52 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1073 ms on 192.168.219.151 (executor driver) (3/10)\n",
    "23/10/09 09:40:53 INFO PythonRunner: Times: total = 1176, boot = 1173, init = 2, finish = 1\n",
    "23/10/09 09:40:53 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1506 bytes result sent to driver\n",
    "23/10/09 09:40:53 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (192.168.219.151, executor driver, partition 4, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:53 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
    "23/10/09 09:40:53 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1211 ms on 192.168.219.151 (executor driver) (4/10)\n",
    "23/10/09 09:40:55 INFO PythonRunner: Times: total = 1457, boot = 1455, init = 2, finish = 0\n",
    "23/10/09 09:40:55 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1549 bytes result sent to driver\n",
    "23/10/09 09:40:55 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (192.168.219.151, executor driver, partition 5, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:55 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 1494 ms on 192.168.219.151 (executor driver) (5/10)\n",
    "23/10/09 09:40:55 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
    "23/10/09 09:40:56 INFO PythonRunner: Times: total = 1233, boot = 1231, init = 2, finish = 0\n",
    "23/10/09 09:40:56 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1506 bytes result sent to driver\n",
    "23/10/09 09:40:56 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (192.168.219.151, executor driver, partition 6, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:56 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
    "23/10/09 09:40:56 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 1275 ms on 192.168.219.151 (executor driver) (6/10)\n",
    "23/10/09 09:40:57 INFO PythonRunner: Times: total = 1247, boot = 1245, init = 1, finish = 1\n",
    "23/10/09 09:40:57 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1506 bytes result sent to driver\n",
    "23/10/09 09:40:57 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (192.168.219.151, executor driver, partition 7, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:57 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
    "23/10/09 09:40:57 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 1287 ms on 192.168.219.151 (executor driver) (7/10)\n",
    "23/10/09 09:40:58 INFO PythonRunner: Times: total = 1081, boot = 1078, init = 2, finish = 1\n",
    "23/10/09 09:40:58 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1549 bytes result sent to driver\n",
    "23/10/09 09:40:58 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (192.168.219.151, executor driver, partition 8, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:40:58 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 1124 ms on 192.168.219.151 (executor driver) (8/10)\n",
    "23/10/09 09:40:58 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
    "23/10/09 09:40:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
    "23/10/09 09:41:00 INFO PythonRunner: Times: total = 1258, boot = 1256, init = 2, finish = 0\n",
    "23/10/09 09:41:00 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1592 bytes result sent to driver\n",
    "23/10/09 09:41:00 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (192.168.219.151, executor driver, partition 9, PROCESS_LOCAL, 7343 bytes) \n",
    "23/10/09 09:41:00 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 1296 ms on 192.168.219.151 (executor driver) (9/10)\n",
    "23/10/09 09:41:00 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
    "23/10/09 09:41:01 INFO PythonRunner: Times: total = 1164, boot = 1161, init = 2, finish = 1\n",
    "23/10/09 09:41:01 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1549 bytes result sent to driver\n",
    "23/10/09 09:41:01 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 1192 ms on 192.168.219.151 (executor driver) (10/10)\n",
    "23/10/09 09:41:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
    "23/10/09 09:41:01 INFO DAGScheduler: ResultStage 0 (mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14) finished in 12.764 s\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
    "23/10/09 09:41:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Job 0 finished: mean at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:14, took 12.834734 s\n",
    "23/10/09 09:41:01 INFO SparkContext: Starting job: collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Got job 1 (collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16) with 1 output partitions\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Final stage: ResultStage 1 (collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16)\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Parents of final stage: List()\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Missing parents: List()\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16), which has no missing parents\n",
    "23/10/09 09:41:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.7 KiB, free 434.4 MiB)\n",
    "23/10/09 09:41:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 434.4 MiB)\n",
    "23/10/09 09:41:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.219.151:53658 (size: 3.7 KiB, free: 434.4 MiB)\n",
    "23/10/09 09:41:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
    "23/10/09 09:41:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16) (first 15 tasks are for partitions Vector(0))\n",
    "23/10/09 09:41:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
    "23/10/09 09:41:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 10) (192.168.219.151, executor driver, partition 0, PROCESS_LOCAL, 7377 bytes) \n",
    "23/10/09 09:41:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 10)\n",
    "23/10/09 09:41:02 INFO PythonRunner: Times: total = 1010, boot = 1009, init = 1, finish = 0\n",
    "23/10/09 09:41:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 10). 1337 bytes result sent to driver\n",
    "23/10/09 09:41:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 10) in 1041 ms on 192.168.219.151 (executor driver) (1/1)\n",
    "23/10/09 09:41:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
    "23/10/09 09:41:02 INFO DAGScheduler: ResultStage 1 (collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16) finished in 1.054 s\n",
    "23/10/09 09:41:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
    "23/10/09 09:41:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
    "23/10/09 09:41:02 INFO DAGScheduler: Job 1 finished: collect at C:\\Users\\jsl\\Code\\201711111\\src\\ds_spark_rdd_hello.py:16, took 1.059589 s\n",
    "23/10/09 09:41:02 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
    "23/10/09 09:41:02 INFO SparkUI: Stopped Spark web UI at http://192.168.219.151:4040\n",
    "23/10/09 09:41:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
    "23/10/09 09:41:02 INFO MemoryStore: MemoryStore cleared\n",
    "23/10/09 09:41:02 INFO BlockManager: BlockManager stopped\n",
    "23/10/09 09:41:02 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
    "23/10/09 09:41:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
    "23/10/09 09:41:02 INFO SparkContext: Successfully stopped SparkContext\n",
    "23/10/09 09:41:03 INFO ShutdownHookManager: Shutdown hook called\n",
    "23/10/09 09:41:03 INFO ShutdownHookManager: Deleting directory C:\\Users\\jsl\\AppData\\Local\\Temp\\spark-35b5b5cc-3398-4f28-90e7-6b27cb81bc39\n",
    "23/10/09 09:41:03 INFO ShutdownHookManager: Deleting directory C:\\Users\\jsl\\AppData\\Local\\Temp\\spark-deb95f0c-e0df-4745-b0b1-c4119fae9fb4\n",
    "23/10/09 09:41:03 INFO ShutdownHookManager: Deleting directory C:\\Users\\jsl\\AppData\\Local\\Temp\\spark-35b5b5cc-3398-4f28-90e7-6b27cb81bc39\\pyspark-70ea527c-260c-400c-904f-4dcbe98f869e\n",
    "23/10/09 09:41:03 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\\Users\\jsl\\AppData\\Local\\Temp\\spark-35b5b5cc-3398-4f28-90e7-6b27cb81bc39\\pyspark-70ea527c-260c-400c-904f-4dcbe98f869e\n",
    "java.nio.file.NoSuchFileException: C:\\Users\\jsl\\AppData\\Local\\Temp\\spark-35b5b5cc-3398-4f28-90e7-6b27cb81bc39\\pyspark-70ea527c-260c-400c-904f-4dcbe98f869e\n",
    "\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\n",
    "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\n",
    "\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\n",
    "\tat java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)\n",
    "\tat java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)\n",
    "\tat java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:197)\n",
    "\tat java.base/java.nio.file.Files.readAttributes(Files.java:1848)\n",
    "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
    "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
    "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
    "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
    "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
    "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
    "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
    "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
    "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
    "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
    "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
    "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
    "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
    "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
    "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
    "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
    "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
    "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:577)\n",
    "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
    "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
    "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_reduceBykey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_reduceBykey.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME']=os.getcwd()\n",
    "os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "def doIt():\n",
    "    print(\"---------RESULT-----------\")\n",
    "    myRdd=spark.sparkContext\\\n",
    "        .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "    res=myRdd\\\n",
    "        .flatMap(lambda x:x.split())\\\n",
    "        .map(lambda x:(x,1))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .map(lambda x:(x[1],x[0]))\\\n",
    "        .sortByKey(False)\\\n",
    "        .take(10)\n",
    "    for i in res:\n",
    "        print(i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\n",
      "\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\n",
      "\n",
      "                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------RESULT-----------\n",
      "(7, 'Spark')\n",
      "(6, 'Apache')\n",
      "(5, '아파치')\n",
      "(4, '스파크')\n",
      "(3, 'the')\n",
      "(2, 'an')\n",
      "(1, 'Wikipedia')\n",
      "(1, 'is')\n",
      "(1, 'open')\n",
      "(1, 'source')\n",
      "SUCCESS: The process with PID 20740 (child process of PID 20140) has been terminated.\n",
      "SUCCESS: The process with PID 20140 (child process of PID 852) has been terminated.\n",
      "SUCCESS: The process with PID 852 (child process of PID 37984) has been terminated.\n"
     ]
    }
   ],
   "source": [
    "!python3 src/ds_spark_rdd_reduceBykey.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 246ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/6ms)\n",
      "---------RESULT-----------\n",
      "(7, u'Spark')\n",
      "(6, u'Apache')\n",
      "(3, u'the')\n",
      "(2, u'an')\n",
      "(1, u'and')\n",
      "(1, u'\\uc18c\\uc2a4')\n",
      "(1, u'is')\n",
      "(1, u'Wikipedia')\n",
      "(1, u'AMPLab,')\n",
      "(1, u'maintained')\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_rdd_reduceBykey.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: RDD를 사용하여 word vector를 생성하기.\n",
    "\n",
    "### 문제\n",
    "사람은 연설이나 에세이 등을 읽고 나면 그 내용이 무엇이고 무엇을 말하려고 하는지 알 수 있다. 주제가 무엇이고 심지어는 숨겨진 행간도 이해할 수 있다.\n",
    "그러나 컴퓨터는 문서를 읽고 의미를 파악해 내는 것이 쉽지 않다.\n",
    "그 대신 문서에 어떤 단어가 쓰였고, 많이 쓰인 단어가 무엇인지 알아내어 어떤 내용인지 알아내게 된다.\n",
    "이 경우 쓰인 단어의 빈도를 **word vector**라고 하며, 문서로 부터 이를 만들어 내는 작업이 필요하다.\n",
    "\n",
    "> **Bag of words** 모델\n",
    "> \n",
    "> BoW 모델은 텍스트 문서를 단어의 집합 \"bag of words\"으로 보고 수치 데이터로 변환하는 것을 말하여, 기계학습 또는 텍스트 분석에 사용할 수 있다.\n",
    "\n",
    "> BoW는 간단하고 이해하기 쉬우며, 문맥을 무시하고 단어의 순서를 고려하지 않는 한계가 있다. 이러한 한계를 극복하기 위해 Word2Vec, FastText, BERT 등과 같은 고급 자연어 처리 기술이 개발되었다.\n",
    "\n",
    "> 텍스트 -> 전처리 (특수문자, 불용어 등 제거) -> 토큰화 (단어로 분할) -> 어휘사전, 단어빈도 구축 -> Word Vector(단어와 빈도)로 표현할 수 있다.\n",
    "\n",
    "> 문장 | 단어 | bag of words 표현\n",
    "> -----|-----|-----\n",
    "> I like to play football. | I, like, to, play, football | {\"I\":1, \"like\":1, \"to\":1, \"play\":1, \"football\":1}\n",
    "> He also likes to play football. | He, also, likes, to, play, football | {\"He\":1, \"also\":1, \"likes\":1, \"to\":1, \"play\":1, \"football\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 해결\n",
    "\n",
    "분석하려는 문서를 RDD로 만들고, map-reduce 알고리듬으로 단어의 빈도를 계산할 수 있다.\n",
    "\n",
    "```Spark mllib``` 라이브러리에서는 자연어처리 기법을 제공하고 있다.\n",
    "TF-IDF, Word2Vec 등을 사용하면 단어의 빈도 및 관련어 등을 분석해 낼 수 있다.\n",
    "\n",
    "다음 장에서 배우겠지만 데이터프레임을 생성하고 ml 라이브러를 사용할 수도 있다.\n",
    "ml은 DataFrame을 위한 라이브러리로서 Tokenizer, Stopwords, NGram 등을 분석할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "파일 전체를 읽어서, 단어의 수를 세어 본다.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "1 | sparkContext.textFile()로 파일을 읽어 RDD로 만든다.\n",
    "2 | flatMap()을 으로 공백으로 분리하여 RDD를 생성한다. 이러한 변환은 실제 연산까지 일어나지 않는다, 즉 lazy변환이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "파일은 이미 만들어 놓은 wiki를 사용한다. 아래와 같이 파일이 있는지 확인해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD를 생성해보자. 문서파일은 문장으로 이루어져 있고, 이를 기준으로 word vector를 구성할 수 있다.\n",
    "그러나 문서 **전체**를 word vector로 표현하려면 **flatMap()** 함수를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "wikiRdd=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단어집합\n",
    "\n",
    "문장을 split()으로 분리하여 단어의 집합으로 만든다. 리스트를 모두 1차원으로 만드는 flatMap() 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words=wikiRdd\\\n",
    "    .flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 중간과정이므로 PipelineRDD가 만들어졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "print (type(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 개수는 count() 함수로 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 갯수:  72\n"
     ]
    }
   ],
   "source": [
    "print (\"단어 갯수: \", words.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 분리된 단어를 collect()함수로 취합한다.\n",
    "**collect() 결과는 list**로 만들어 진다.\n",
    "모든 항목을 출력하려면 list의 인덱스 [:]를 사용한다.\n",
    "유니코드를 지원하므로, 한글이 잘 출력되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어:  ['Wikipedia', 'Apache', 'Spark', 'is', 'an', 'open', 'source', 'cluster', 'computing', 'framework.', '아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.', 'Apache', 'Spark', 'Apache', 'Spark', 'Apache', 'Spark', 'Apache', 'Spark', '아파치', '스파크', '아파치', '스파크', '아파치', '스파크', '아파치', '스파크', 'Originally', 'developed', 'at', 'the', 'University', 'of', 'California,', \"Berkeley's\", 'AMPLab,', 'the', 'Spark', 'codebase', 'was', 'later', 'donated', 'to', 'the', 'Apache', 'Software', 'Foundation,', 'which', 'has', 'maintained', 'it', 'since.', 'Spark', 'provides', 'an', 'interface', 'for', 'programming', 'entire', 'clusters', 'with', 'implicit', 'data', 'parallelism', 'and', 'fault-tolerance.']\n"
     ]
    }
   ],
   "source": [
    "print (\"전체 단어: \", words.collect()[0:72])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단어빈도\n",
    "\n",
    "이제 map-reduce를 같이 사용하여 단어를 세어 tuple로 만들어 보자. 즉 **(단어, 1) 구조**로 만들어 **같은 단어는 나중에 서로 더할 수** 있게 만들어 놓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wc = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2: flatMap()은 리스트나 배열같은 구조를 하나의 평면(flat)의 리스트로 만든다. 그 함수 안의 .split()은 앞서 설명한 바와 같이 파일 텍스트를 공백으로 분리한다.\n",
    "- L3: map()함수는 (단어, 1) 이런 식으로 tuple로 만든다.\n",
    "    - .lower()는 모든 단어를 소문자로 만들고,\n",
    "    - .rstrip(), .lstrp() 함수는 **양끝에 붙어있을 수 있는 불필요한 구문 (new lines, commas, periods)을 제거**하는데, 매개변수가 없으면 whitespace를 제거한다.\n",
    "\n",
    "함수 | 설명\n",
    "-----|-----\n",
    "rstrip() | 꼬리에 붙는 문자를 제거\n",
    "lstrip() | 머리에 붙는 문자를 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "아직 단어별로 갯수를 계산하지 않았기 때문에, 모두 1인 값을 가진다.\n",
    "sortByKey()는 **올림차순**을 기본으로 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1), ('an', 1), ('an', 1), ('and', 1), ('apache', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 빈도 집계\n",
    "\n",
    "이제 단어의 갯수를 합계내어 보자. 아래 방법 모두 **동일한 결과를 산출**한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "```reduceByKey(add)``` | ```add``` operator를 사용하면, 단어튜플의 수를 키별로 더할 수 있다.\n",
    "```groupByKey().mapValues(sum)``` | ```mapValues()```를 사용하여 value의 'sum'을 계산할 수 있다.\n",
    "```groupByKey().map(lambda (x,iter): (x,len(iter)))``` | (key,value)의 구조를 사용하여 합계를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 하나. reduceByKey()\n",
    "\n",
    "Python의 연산자 add() 함수를 사용해서 할 수 있다.\n",
    "**operator.add()**는 reduce()함수의 숫자 인자 x,y를 받아서 x+y 연산을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wcReduceByKey = wc.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcReduceByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또는 add대신에 reduceByKey()에 lambda함수를 사용해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wcReduceByKey = wc.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcReduceByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "요약하면 reduceByKey() 함수는 워드벡터를 집계하기 위해 사용하는데, 정리하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "wikiRdd=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "\n",
    "wc=wikiRdd\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcReduceByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 둘. groupByKey(), mapValues()\n",
    "\n",
    "groupByKey()는 단어키로 동일한 단어는 집단화한다.\n",
    "집단화 하면 **PairRDD가 되고, 즉 key-value 쌍**으로 구성된다.\n",
    "**mapValues()는 각 쌍의 value에 대해서 sum 연산**을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wcGroupByKey = wc.groupByKey().mapValues(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcGroupByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 셋. groupByKey(), map, len\n",
    "\n",
    "mapValues()를 사용하지 않고 map()으로 **len()**갯수를 세어도 동일한 결과를 얻을 수 있다.\n",
    "**단어별로 1개씩 만들어 놓았으므로 len()으로 세면 단어갯수의 합계**가 된다.\n",
    "\n",
    "전에는 ```wc.groupByKey().map(lambda (x,v): (x,len(v)))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wcGroupByKey2 = wc.groupByKey().map(lambda x: (x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amplab', 1),\n",
       " ('an', 2),\n",
       " ('and', 1),\n",
       " ('apache', 6),\n",
       " ('at', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('california', 1),\n",
       " ('cluster', 1),\n",
       " ('clusters', 1),\n",
       " ('codebase', 1)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcGroupByKey2.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 넷. 줄로 구분하여 단어 빈도\n",
    "\n",
    "```flatMap()```을 사용하지 않으면 **줄로 구분하여 단어 빈도**를 셀 수 있다.\n",
    "줄로 구분하여 리스트를 만들게 되므로, 결과는 2차원 리스트가 된다.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "1 | ```sparkContext.textFile()``` 파일을 읽어 RDD 생성\n",
    "2 | 불필요한 컴마, 점, 하이픈을 제거하고 소문자로\n",
    "3 | 공백으로 분리\n",
    "4 | 반복문으로 단어별 튜플구조화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wc = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wikipedia', 1)]\n",
      "[('apache', 1), ('spark', 1), ('is', 1), ('an', 1), ('open', 1), ('source', 1), ('cluster', 1), ('computing', 1), ('framework', 1)]\n",
      "[('아파치', 1), ('스파크는', 1), ('오픈', 1), ('소스', 1), ('클러스터', 1), ('컴퓨팅', 1), ('프레임워크이다', 1)]\n",
      "[('apache', 1), ('spark', 1), ('apache', 1), ('spark', 1), ('apache', 1), ('spark', 1), ('apache', 1), ('spark', 1)]\n",
      "[('아파치', 1), ('스파크', 1), ('아파치', 1), ('스파크', 1), ('아파치', 1), ('스파크', 1), ('아파치', 1), ('스파크', 1)]\n",
      "[('originally', 1), ('developed', 1), ('at', 1), ('the', 1), ('university', 1), ('of', 1), ('california', 1), (\"berkeley's\", 1), ('amplab', 1)]\n",
      "[('the', 1), ('spark', 1), ('codebase', 1), ('was', 1), ('later', 1), ('donated', 1), ('to', 1), ('the', 1), ('apache', 1), ('software', 1), ('foundation', 1)]\n",
      "[('which', 1), ('has', 1), ('maintained', 1), ('it', 1), ('since', 1)]\n",
      "[('spark', 1), ('provides', 1), ('an', 1), ('interface', 1), ('for', 1), ('programming', 1), ('entire', 1), ('clusters', 1), ('with', 1)]\n",
      "[('implicit', 1), ('data', 1), ('parallelism', 1), ('and', 1), ('fault', 1), ('tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "for e in wc.collect():\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 묶어서\n",
    "\n",
    "지금까지의 작업을 프로그램으로 묶어보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_rdd_wordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_rdd_wordCount.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "def doIt():\n",
    "    wikiRdd=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "    wc=wikiRdd\\\n",
    "        .flatMap(lambda x: x.split())\\\n",
    "        .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .sortByKey()\\\n",
    "        .collect()\n",
    "    for w in wc:\n",
    "        print (w)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "    #os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: Unix/Linux 운영 체제에서 사용되며, 스크립트 파일의 첫 부분에 위치하여 해당 스크립트를 실행할 Python 인터프리터를 지정. 윈도우에서는 인식되지 않으며, 파일을 확장자 .py로 저장하면 실행할 Python 인터프리터를 지정하게 된다.\n",
    "- L2: 스크립트 파일에서 사용되는 문자 인코딩을 명시한다. Python 2에서는 특히 중요하며, Python 3에서는 기본적으로 UTF-8을 사용하며, 생략해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/09/12 20:28:13 WARN Utils: Your hostname, jsl-smu resolves to a loopback address: 127.0.1.1; using 117.16.44.45 instead (on interface eth0)\n",
      "20/09/12 20:28:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jsl/.local/lib/python3.6/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "20/09/12 20:28:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/09/12 20:28:14 INFO SparkContext: Running Spark version 3.0.0\n",
      "20/09/12 20:28:14 INFO ResourceUtils: ==============================================================\n",
      "20/09/12 20:28:14 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "20/09/12 20:28:14 INFO ResourceUtils: ==============================================================\n",
      "20/09/12 20:28:14 INFO SparkContext: Submitted application: myApp\n",
      "20/09/12 20:28:14 INFO SecurityManager: Changing view acls to: jsl\n",
      "20/09/12 20:28:14 INFO SecurityManager: Changing modify acls to: jsl\n",
      "20/09/12 20:28:14 INFO SecurityManager: Changing view acls groups to: \n",
      "20/09/12 20:28:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/09/12 20:28:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jsl); groups with view permissions: Set(); users  with modify permissions: Set(jsl); groups with modify permissions: Set()\n",
      "20/09/12 20:28:14 INFO Utils: Successfully started service 'sparkDriver' on port 39383.\n",
      "20/09/12 20:28:14 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/09/12 20:28:14 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/09/12 20:28:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/09/12 20:28:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/09/12 20:28:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "20/09/12 20:28:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cd9e2585-38d4-478e-bb42-e4919493af3c\n",
      "20/09/12 20:28:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "20/09/12 20:28:14 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/09/12 20:28:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/09/12 20:28:15 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "20/09/12 20:28:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://117.16.44.45:4041\n",
      "20/09/12 20:28:15 INFO Executor: Starting executor ID driver on host 117.16.44.45\n",
      "20/09/12 20:28:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39703.\n",
      "20/09/12 20:28:15 INFO NettyBlockTransferService: Server created on 117.16.44.45:39703\n",
      "20/09/12 20:28:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/09/12 20:28:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 117.16.44.45, 39703, None)\n",
      "20/09/12 20:28:15 INFO BlockManagerMasterEndpoint: Registering block manager 117.16.44.45:39703 with 434.4 MiB RAM, BlockManagerId(driver, 117.16.44.45, 39703, None)\n",
      "20/09/12 20:28:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 117.16.44.45, 39703, None)\n",
      "20/09/12 20:28:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 117.16.44.45, 39703, None)\n",
      "20/09/12 20:28:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jsl/Code/git/bb/jsl/pyds/spark-warehouse/').\n",
      "20/09/12 20:28:15 INFO SharedState: Warehouse path is 'file:/home/jsl/Code/git/bb/jsl/pyds/spark-warehouse/'.\n",
      "20/09/12 20:28:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.4 KiB, free 434.3 MiB)\n",
      "20/09/12 20:28:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 434.3 MiB)\n",
      "20/09/12 20:28:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 117.16.44.45:39703 (size: 23.6 KiB, free: 434.4 MiB)\n",
      "20/09/12 20:28:16 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "20/09/12 20:28:16 INFO FileInputFormat: Total input paths to process : 1\n",
      "20/09/12 20:28:16 INFO SparkContext: Starting job: collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12) as input to shuffle 0\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Got job 0 (collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12) with 1 output partitions\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12)\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12), which has no missing parents\n",
      "20/09/12 20:28:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 434.2 MiB)\n",
      "20/09/12 20:28:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.2 MiB)\n",
      "20/09/12 20:28:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 117.16.44.45:39703 (size: 7.1 KiB, free: 434.4 MiB)\n",
      "20/09/12 20:28:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200\n",
      "20/09/12 20:28:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12) (first 15 tasks are for partitions Vector(0))\n",
      "20/09/12 20:28:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "20/09/12 20:28:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 117.16.44.45, executor driver, partition 0, PROCESS_LOCAL, 7388 bytes)\n",
      "20/09/12 20:28:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/09/12 20:28:17 INFO HadoopRDD: Input split: file:/home/jsl/Code/git/bb/jsl/pyds/data/ds_spark_wiki.txt:0+572\n",
      "20/09/12 20:28:17 INFO PythonRunner: Times: total = 375, boot = 315, init = 57, finish = 3\n",
      "20/09/12 20:28:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1806 bytes result sent to driver\n",
      "20/09/12 20:28:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 869 ms on 117.16.44.45 (executor driver) (1/1)\n",
      "20/09/12 20:28:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/09/12 20:28:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55625\n",
      "20/09/12 20:28:17 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12) finished in 1.034 s\n",
      "20/09/12 20:28:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "20/09/12 20:28:17 INFO DAGScheduler: running: Set()\n",
      "20/09/12 20:28:17 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "20/09/12 20:28:17 INFO DAGScheduler: failed: Set()\n",
      "20/09/12 20:28:17 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12), which has no missing parents\n",
      "20/09/12 20:28:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.1 KiB, free 434.2 MiB)\n",
      "20/09/12 20:28:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 434.2 MiB)\n",
      "20/09/12 20:28:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 117.16.44.45:39703 (size: 5.4 KiB, free: 434.4 MiB)\n",
      "20/09/12 20:28:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200\n",
      "20/09/12 20:28:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12) (first 15 tasks are for partitions Vector(0))\n",
      "20/09/12 20:28:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "20/09/12 20:28:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 117.16.44.45, executor driver, partition 0, NODE_LOCAL, 7143 bytes)\n",
      "20/09/12 20:28:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "20/09/12 20:28:17 INFO ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
      "20/09/12 20:28:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "20/09/12 20:28:17 INFO PythonRunner: Times: total = 45, boot = -323, init = 368, finish = 0\n",
      "20/09/12 20:28:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2658 bytes result sent to driver\n",
      "20/09/12 20:28:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 104 ms on 117.16.44.45 (executor driver) (1/1)\n",
      "20/09/12 20:28:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "20/09/12 20:28:17 INFO DAGScheduler: ResultStage 1 (collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12) finished in 0.119 s\n",
      "20/09/12 20:28:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "20/09/12 20:28:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "20/09/12 20:28:17 INFO DAGScheduler: Job 0 finished: collect at /home/jsl/Code/git/bb/jsl/pyds/src/ds_rdd_wordCount.py:12, took 1.241607 s\n",
      "('amplab', 1)\n",
      "('an', 2)\n",
      "('and', 1)\n",
      "('apache', 6)\n",
      "('at', 1)\n",
      "(\"berkeley's\", 1)\n",
      "('california', 1)\n",
      "('cluster', 1)\n",
      "('clusters', 1)\n",
      "('codebase', 1)\n",
      "('computing', 1)\n",
      "('data', 1)\n",
      "('developed', 1)\n",
      "('donated', 1)\n",
      "('entire', 1)\n",
      "('fault-tolerance', 1)\n",
      "('for', 1)\n",
      "('foundation', 1)\n",
      "('framework', 1)\n",
      "('has', 1)\n",
      "('implicit', 1)\n",
      "('interface', 1)\n",
      "('is', 1)\n",
      "('it', 1)\n",
      "('later', 1)\n",
      "('maintained', 1)\n",
      "('of', 1)\n",
      "('open', 1)\n",
      "('originally', 1)\n",
      "('parallelism', 1)\n",
      "('programming', 1)\n",
      "('provides', 1)\n",
      "('since', 1)\n",
      "('software', 1)\n",
      "('source', 1)\n",
      "('spark', 7)\n",
      "('the', 3)\n",
      "('to', 1)\n",
      "('university', 1)\n",
      "('was', 1)\n",
      "('which', 1)\n",
      "('wikipedia', 1)\n",
      "('with', 1)\n",
      "('소스', 1)\n",
      "('스파크', 4)\n",
      "('스파크는', 1)\n",
      "('아파치', 5)\n",
      "('오픈', 1)\n",
      "('컴퓨팅', 1)\n",
      "('클러스터', 1)\n",
      "('프레임워크이다', 1)\n",
      "20/09/12 20:28:17 INFO SparkUI: Stopped Spark web UI at http://117.16.44.45:4041\n",
      "20/09/12 20:28:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/09/12 20:28:17 INFO MemoryStore: MemoryStore cleared\n",
      "20/09/12 20:28:17 INFO BlockManager: BlockManager stopped\n",
      "20/09/12 20:28:17 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/09/12 20:28:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/09/12 20:28:18 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/09/12 20:28:18 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/09/12 20:28:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff31d11-10f1-4114-90fd-1cc378dff1c1/pyspark-a0edd5fb-83a7-42e3-8348-d4f1fe54e884\n",
      "20/09/12 20:28:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-53c92fef-29c1-4bab-a155-b3c58e632d57\n",
      "20/09/12 20:28:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff31d11-10f1-4114-90fd-1cc378dff1c1\n"
     ]
    }
   ],
   "source": [
    "!spark-submit src/ds_rdd_wordCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-5: word vector를 생성하기.\n",
    "\n",
    "다음과 같은 노래가사에 대해 단어빈도를 계산하고 출력한다.\n",
    "\n",
    "```\n",
    "\"Yesterday 어제는\"\n",
    "\"All my troubles seemed to far away 내 모든 문제들이 참 멀리 있는 것 같았는데\"\n",
    "\"Now it looks as though they're here to stay 이제는 그 모든 문제들이 여기에 있는 것처럼 보여\"\n",
    "\"Oh, I believe in yesterday 오오, 어제가 좋았는데\"\n",
    "\"Suddenly 갑자기 그-림-자-가 2021 그-림-자-가 2021\"\n",
    "\"I'm not half the man I used to be 난 예전 내 모습의 반도 못한 사람이 되어버렸어\"\n",
    "\"There's a shadow hanging over me 내 위에 그림자가 드리워져 있네\"\n",
    "\"Oh!, yesterday came suddenly 오오!, 이제는 너무 갑자기 와버렸어..\"\n",
    "```\n",
    "\n",
    "* 1) 단어빈도 계산\n",
    "\n",
    "- 대소문자 구별 (Yesterday, yesterday는 동일한 단어로 인식한다)\n",
    "- 불필요한 느낌표 '!', 글자 사이 하이픈 '-', 쉼표 ',', 점 '.' 등 제거\n",
    "(예: oh! -> oh, '그-림-자-가' -> '그림자가' 된다)\n",
    "- 한 글자 단어 제거 ('i', '내' 등)\n",
    "- 숫자 제거\n",
    "\n",
    "* 2) 빈도순으로 출력, 전체단어 수 출력\n",
    "\n",
    "```\n",
    "[(3, 'yesterday'),\n",
    " (3, 'to'),\n",
    " (3, '그림자가'),\n",
    " (2, '모든'),\n",
    " (2, '문제들이'),\n",
    " (2, '있는'),\n",
    " (2, '이제는'),\n",
    " (2, 'oh'),\n",
    " (2, '오오'),\n",
    " (2, 'suddenly'),\n",
    " (2, '갑자기')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 일부 수정된 노래가사이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday=[\n",
    "    [\"Yesterday 어제는\"],\n",
    "    [\"All my troubles seemed to far away 내 모든 문제들이 참 멀리 있는 것 같았는데\"],\n",
    "    [\"Now it looks as though they're here to stay 이제는 그 모든 문제들이 여기에 있는 것처럼 보여\"],\n",
    "    [\"Oh, I believe in yesterday 오오, 어제가 좋았는데\"],\n",
    "    [\"Suddenly 갑자기 그-림-자-가 2021 그-림-자-가 2021\"],\n",
    "    [\"I'm not half the man I used to be 난 예전 내 모습의 반도 못한 사람이 되어버렸어\"],\n",
    "    [\"There's a shadow hanging over me 내 위에 그림자가 드리워져 있네\"],\n",
    "    [\"Oh!, yesterday came suddenly 오오!, 이제는 너무 갑자기 와버렸어..\"],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "#os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "yRdd = spark.sparkContext.parallelize(yesterday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 구조를 잘 살펴보면 리스트내 리스트 그 다음에는 따옴표로 구성되어 있어서, flatMap()을 해주어 2차 리스트를 풀어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yesterday', '어제는', 'All', 'my']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yRdd.flatMap(lambda x: x)\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 2차원을 1차원으로 ```['Yesterday 어제는', 'All my troubles...것 같았는데']``` 이렇게 변환된다. 리스트 하나씩 입력되어 평탄화 flat 하기 때문이다.\n",
    "- L2: ```['Yesterday', '어제는', 'All', 'my']``` 로 변환된다. 문장열 하나씩 입력해서 단어로 분할, 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어빈도 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yesterday', 1),\n",
       " ('어제는', 1),\n",
       " ('All', 1),\n",
       " ('my', 1),\n",
       " ('troubles', 1),\n",
       " ('seemed', 1),\n",
       " ('to', 3),\n",
       " ('far', 1),\n",
       " ('away', 1),\n",
       " ('내', 3),\n",
       " ('모든', 2),\n",
       " ('문제들이', 2),\n",
       " ('참', 1),\n",
       " ('멀리', 1),\n",
       " ('있는', 2),\n",
       " ('것', 1),\n",
       " ('같았는데', 1),\n",
       " ('Now', 1),\n",
       " ('it', 1),\n",
       " ('looks', 1),\n",
       " ('as', 1),\n",
       " ('though', 1),\n",
       " (\"they're\", 1),\n",
       " ('here', 1),\n",
       " ('stay', 1),\n",
       " ('이제는', 2),\n",
       " ('그', 1),\n",
       " ('여기에', 1),\n",
       " ('것처럼', 1),\n",
       " ('보여', 1),\n",
       " ('Oh,', 1),\n",
       " ('I', 2),\n",
       " ('believe', 1),\n",
       " ('in', 1),\n",
       " ('yesterday', 2),\n",
       " ('오,', 1),\n",
       " ('어제가', 1),\n",
       " ('좋았는데', 1),\n",
       " ('Suddenly', 1),\n",
       " ('갑자기', 2),\n",
       " ('그-림-자-가', 2),\n",
       " ('2021', 2),\n",
       " (\"I'm\", 1),\n",
       " ('not', 1),\n",
       " ('half', 1),\n",
       " ('the', 1),\n",
       " ('man', 1),\n",
       " ('used', 1),\n",
       " ('be', 1),\n",
       " ('난', 1),\n",
       " ('예전', 1),\n",
       " ('모습의', 1),\n",
       " ('반도', 1),\n",
       " ('못한', 1),\n",
       " ('사람이', 1),\n",
       " ('되어버렸어', 1),\n",
       " (\"There's\", 1),\n",
       " ('a', 1),\n",
       " ('shadow', 1),\n",
       " ('hanging', 1),\n",
       " ('over', 1),\n",
       " ('me', 1),\n",
       " ('위에', 1),\n",
       " ('그림자가', 1),\n",
       " ('드리워져', 1),\n",
       " ('있네', 1),\n",
       " ('Oh!,', 1),\n",
       " ('came', 1),\n",
       " ('suddenly', 1),\n",
       " ('오!,', 1),\n",
       " ('너무', 1),\n",
       " ('와버렸어..', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yRdd.flatMap(lambda x: x)\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도순으로 출력, 빈도 1이하는 제외\n",
    "\n",
    "앞은 빈도의 순서가 지켜지지 않았지만, 이번에는 내림차순으로 한 번만 나타난 단어는 제외하고 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'to'),\n",
       " (3, '내'),\n",
       " (2, '모든'),\n",
       " (2, '문제들이'),\n",
       " (2, '있는'),\n",
       " (2, '이제는'),\n",
       " (2, 'I'),\n",
       " (2, 'yesterday'),\n",
       " (2, '갑자기'),\n",
       " (2, '그-림-자-가'),\n",
       " (2, '2021')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yRdd.flatMap(lambda x: x)\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .filter(lambda x:x[0]>1)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L5: 빈도 먼저 놓는다\n",
    "- L6: 빈도순으로 정렬\n",
    "- L7: 빈도 1이하는 제외 ```filter(lambda x:x[0]>1)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소문자, 하이픈, 느낌표, 숫자를 제외하고 단어빈도 계산해서 출력 (빈도 1이하는 제외)\n",
    "\n",
    "* 대소문자: Yesterday, yesterday를 동일한 단어로 계산. 줄4 ```x.lower()```\n",
    "* 하이픈: 그림자가, 그-림-자-가를 동일한 단어로 계산. ```.replace('-','')```\n",
    "* 느낌표: oh, oh! ```.lstrip('!')```\n",
    "* 숫자: ```map(lambda x: re.sub('\\d','',x))``` 또는 ```.filter(lambda x: x.isdigit())```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'yesterday'),\n",
       " (3, 'to'),\n",
       " (3, '내'),\n",
       " (3, '그림자가'),\n",
       " (2, '모든'),\n",
       " (2, '문제들이'),\n",
       " (2, '있는'),\n",
       " (2, '이제는'),\n",
       " (2, 'i'),\n",
       " (2, 'suddenly'),\n",
       " (2, '갑자기')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "yRdd.flatMap(lambda x: x)\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x.isdigit())\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-','').lower())\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.').lstrip('!'), 1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .filter(lambda x:x[0]>1)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1글자 단어 제외\n",
    "\n",
    "1글자 단어 (예를 들어 'i', '내') 제외하고 단어빈도 계산 (빈도 1이하는 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = yRdd.flatMap(lambda x: x)\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-','').replace('!','').lower())\\\n",
    "    .map(lambda x: x.lower().rstrip().lstrip().rstrip(',').rstrip('.'))\\\n",
    "    .filter(lambda x: len(x)>1)\\\n",
    "    .filter(lambda x: not x.isdigit())\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .filter(lambda x:x[0]>1)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L5: 불필요한 철자를 제거한 다음에 한 글자 단어를 제외한다.\n",
    "- L6: 숫자 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'yesterday'),\n",
       " (3, 'to'),\n",
       " (3, '그림자가'),\n",
       " (2, '모든'),\n",
       " (2, '문제들이'),\n",
       " (2, '있는'),\n",
       " (2, '이제는'),\n",
       " (2, 'oh'),\n",
       " (2, '오오'),\n",
       " (2, 'suddenly'),\n",
       " (2, '갑자기')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=list()\n",
    "v=list()\n",
    "for i in wc:\n",
    "    v.append(i[0])\n",
    "    k.append(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L4: 리스트에 단어빈도(v)를 추가\n",
    "- L5: 리스트에 단어(k)를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.rc('font', family='NanumMyeongjo')\n",
    "plt.rcParams[\"font.family\"] = \"Malgun Gothic\" \n",
    "plt.barh(range(len(v)), v, color = 'black')  # x <- from (v)alues\n",
    "plt.yticks(range(len(v)), k)                 # y <- from (k)eys\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제\n",
    "\n",
    "앞의 \"경기도 의정부시_인구현황_20230731.csv\" 파일의 두번째 컬럼(인구수)의 합계를 구해보자.\n",
    "\n",
    "* (1) 데이터는 한 줄씩 구성되어 있는데, 이 중에 컬럼2만을 추출해서 출력 (아래와 같은 출력이다)\n",
    "['37515 ',\n",
    " '29759 ',\n",
    " ...]\n",
    "\n",
    "* (2) 컬럼2를 map-reduce 합계해서 출력 (헤더를 제외하고, 문자-->숫자로 형변환해야 합산이 된다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

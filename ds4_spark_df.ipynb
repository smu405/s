{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Spark DataFrame\n",
    "\n",
    "* Last updated 202310_202010_20191029_20170606_20170221_20161125\n",
    "\n",
    "Spark DataFrame을 생성하고, 관련 API를 배운다. Spark SQL을 사용하여 데이터를 조회하고, MongoDB에 쓰고, 읽어본다.\n",
    "\n",
    "* S.2 Jupyter Notebook에서 SparkSession 생성\n",
    "* S.3 DataFrame: 특징, Schema, DataFrame의 API\n",
    "* S.4 DataFrame 생성: schema, RDD, Pandas, csv, tsv, JSON, Parquet\n",
    "* S.5 DataFrame API: range, withColumn, Drop, udf, withColumnRenamed, 그래프, 컬럼선택 select, filter,\n",
    "컬럼의 내용 변경 regexp_replace, groupBy, F 함수, 행 추가, partition, 통계 요약 describe, 결측값\n",
    "* S.6 Spark SQL: 네트워크에 불법 침입, Twitter JSON, 뉴욕 신생아, 우버 운행기록, JDBC, MongoDB Spark connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 4.1 SparkSession 생성\n",
    "\n",
    "PIP의 pyspark을 설치한 경우에는, 별도로 경로와 라이브러리를 추가하지 않아도 된다.\n",
    "다만 Python 2, 3 여러 버전이 설치된 경우에는 아래와 같이 그 경로를 설정해 주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]=sys.executable         #\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=sys.executable  #\"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "print (spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 DataFrame\n",
    "\n",
    "### 특징\n",
    "\n",
    "DataFrame은 **행, 열로 구조화**된 데이터구조이다.\n",
    "\n",
    "관계형데이터베이스 RDB의 테이블이나 엑셀 쉬트sheet와 비슷하다.\n",
    "또는 Pandas 또는 R을 사용해 보았다면 거기서 제공되는 DataFrame과 유사하다.\n",
    "\n",
    "Apache Spark 1.0에서는 **SchemaRDD**라는 명칭으로 시험적으로 제공되었다. 이름에서 보듯이 RDD에 스키마를 얹어서 만든 개념이다.\n",
    "그러나 Spark의 DataFrame은 **대용량 데이터를 처리하기 위해 만들어진 프레임워크로 분산**해서 사용할 수 있게 고안되었다.\n",
    "\n",
    "앞서 사용했던 **RDD가 schema를 정하지 않는** 것과 달리, **DataFrame은 모델 schema**를 설정해서 사용을 한다. '열'에 대해 명칭 및 **데이터 타잎**을 가지고 있고, 이를 지켜서 데이터를 저장하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Schema\n",
    "\n",
    "**Row**는 DataFrame의 행으로, 데이터 요소항목을 묶어서 구성한다. **Python list나 dict**를 사용하여 Row를 구성할 수 있다.\n",
    "**Column**은 DataFrame의 **열**이고, 다음과 같은 **데이터 타잎**을 가진다.\n",
    "\n",
    "```python\n",
    "Spark | Python\n",
    "- NullType\n",
    "- StringType | string\n",
    "- BinaryType | bytearray\n",
    "\n",
    "- BooleanType | bool\n",
    "- DateType | datetime.date\n",
    "- TimestampType | datetime.datetime\n",
    "\n",
    "- DecimalType\n",
    "- ByteType | int\n",
    "- ShortType | int\n",
    "- IntegerType | int\n",
    "- LongType | long\n",
    "- FloatType | float\n",
    "- DoubleType | float\n",
    "\n",
    "- ArrayType | list, tuple, array\n",
    "- MapType | dict\n",
    "- StructType | list or tuple\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame의 API\n",
    "\n",
    "RDD와 마찬가지로 DataFrame을 구성하여 **머신러닝**의 입력데이터로 사용할 수 있다.\n",
    "현재 버전 2.0부터 RDD에 대한 지원은 줄여나가고, **버전 3.0 이후에는 DataFrame API를 공식적으로 지원**한다고 발표한 바 있다. RDD보다 우선적으로 사용하는 것이 좋겠다. DataFrame에서 제공하는 API는 아래와 같다.\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```json``` | json 파일에서 읽기 | ```spark.read.json(\"employee.json\")```\n",
    "```show``` | DataFrame에 로딩된 데이터 읽기 | ```df.show()```\n",
    "```schema``` | 데이터 schema 보기 | ```df.printSchema()```\n",
    "```select``` | **열을 선택** | ```df.select(\"name\")```\n",
    "```filter``` | 조건으로 선택하여 **행을 선택** | ```df.filter(df[\"age\"] > 23).show()```\n",
    "```groupBy``` | 그룹으로 나누기 | ```df.groupBy(\"age\").count().show()```\n",
    "```dropna``` | na를 삭제 | ```df.dropna()``` ```df.na.drop()```\n",
    "```fillna``` | na를 값으로 채우기 | ```fillna()```\n",
    "```count``` | 행 세기 | ```df.count()```\n",
    "```drop``` | 열을 삭제 | ```df.drop(\"name\")```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 DataFrame 생성\n",
    "\n",
    "DataFrame은 관계형데이터베이스 RDB 테이블과 같이, schema를 정해서 생성한다.\n",
    "**schema를 정해주지 않으면, Spark가 자동으로 유추**하게 된다.\n",
    "Python 리스트, RDD, Pandas DataFrame, Hive, csv, JSON, RDB, XML, Parquet, Apache Cassandra 등 다양한 채널에서 읽어서 DataFrame을 만들 수 있다.\n",
    "\n",
    "* **```spark.createDataFrame()```** 함수는 Python List, RDD, Pandas DataFrame 등에서 읽어서 DataFrame을 만들 수 있다.\n",
    "* 또는 **```spark.read.text, spark.read.json, spark.read.parquet, spark.read.load```** 함수로 옵션을 설정해서 외부 파일 등에서 읽을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.1 schema 생성하기\n",
    "\n",
    "DataFrame은 데이터 모델 schema를 정의하고, 각 컬럼의 명칭과 데이터타입을 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 자동으로 인식하는 schema\n",
    "\n",
    "우선 단순하게 Python 자료구조를 사용해서 생성해 본다.\n",
    "아래 열이 3개인 데이터를 **```createDataFrame()```** 함수를 사용하여 넣어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[\n",
    "        ('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**컬럼**명은 **일련번호**를 가지고 생성된다. schema를 정하지 않았으므로, 열은 '_1', '_2'와 같이 명명된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 경우 Spark가 **자동으로 schema를 설정**한다.\n",
    "```printSchema()``` 함수로 schema를 출력해 보기로 하자.\n",
    "\n",
    "* Schema가 자동으로 유추되기 때문에 **데이터 타잎**도 마찬가지이다. 물론 올바르게 되지 않을 경우도 있다는 점에 유의한다. 아래에서 보듯이 컬럼 1 학년은 ```string```, 컬럼 2 이름은 ```string```, 컬럼 3 키는 ```long```으로 인식하고 있다.\n",
    "* ```nullable```은 결측값이 허용되는지를 말하는 것이다. ```true```이면 허용된다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 줄의 Row()를 출력해 보자.\n",
    "\n",
    "출력하는 속성 3개가 앞서 정의한 schema와 일치하는지 확인하자.\n",
    "즉 첫째, 둘째는 문자열로 세째는 ```long``` 타입이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='1', _2='kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "print (myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명 설정\n",
    "\n",
    "앞서 컬럼 Column을 정의하지 않고 DataFrame을 생성하였는데, 이번에는 **컬럼명을 정해서** 생성하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year','name','height']\n",
    "_myDf = spark.createDataFrame(myList, cols)\n",
    "\n",
    "_myDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 컬럼명을 ```['year','name','height']```** 정해준다.\n",
    "- L2: **```createDataFrame()```** 함수에 **인자로 컬럼명을 리스트로 설정한다.\n",
    "- L4: ```.columns``` 컬럼명을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 한 줄 출력해 보면, 컬럼명이 변경되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='1', name='kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print (_myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 100개를 생성해 보자.\n",
    "우선 데이터의 원천이 되는 names, items를 정의하자.\n",
    "여기서 하나씩 선택하여 데이터를 생성하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "\n",
    "coffeeDf = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: names 리스트를 정해준다.\n",
    "- L2: items 리스트를 정의한다.\n",
    "- L4: names, items 리스트에서 **modulus**를 활용하여 하나씩 선택하여 데이터를 생성한다.\n",
    "**names**는 4개이므로 4로 나눈 나머지와 **items**는 6개이므로 6으로 나눈 나머지를 하나씩 선택하고 있다.\n",
    "그리고 컬럼명을 ```[\"name\",\"coffee\"]```으로 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동으로 생성된 schema가 데이터타입을 잘 집어내고 있다.\n",
    "name, coffee 모두 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coffee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|    coffee|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row 객체를 사용해서 생성\n",
    "\n",
    "**Row** 객체를 사용해 보자.\n",
    "Row는 **이름(Column)이 붙여진 행**으로 **관계형데이터베이스 레코드 Record**에 해당한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Person = Row('year','name', 'height')\n",
    "\n",
    "row1=Person('1','kim, js', 170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L3: Row의 속성 명은 'year', 'name', 'height'로 명명한다.\n",
    "- L5: Person이라는 Row에 데이터를 넣어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row 속성 읽기\n",
    "\n",
    "속성명을 읽을 때에는 **```row.key```** 또는 Python dictionary형식으로 **```row[key]```**와 같이 속성을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js 170\n"
     ]
    }
   ],
   "source": [
    "print (\"row1: \", row1.year, row1.name, row1.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Row를 Dictionary로 저장\n",
    "\n",
    "Row는 Dictionary로 쉽게 변환할 수 있다. Row의 속성과 값이 Dictionary의 키, 밸류로 바뀌게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '1', 'name': 'kim, js', 'height': 170}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary에서 키 또는 값을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['year', 'name', 'height'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['1', 'kim, js', 170])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict().values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row에서 DataFrame 생성\n",
    "\n",
    "위에서 설정한 Row를 사용하여 DataFrame을 만들어 보자.\n",
    "**Python list에 Row를 넣어** 구성한다.\n",
    "첫번째는 앞서 만든 row1 객체를 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [\n",
    "          row1,\n",
    "          Person('1', 'lee, sm', 175),\n",
    "          Person('2', 'lim, yg', 180),\n",
    "          Person('2', 'lee', 170)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```printSchema()```를 해보면, **데이터 타잎**은 ```string```, ```long```으로 Spark에서 **자동** 인식되었다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema를 정의하고 생성\n",
    "\n",
    "모델 schema를 정하고, **데이터 타잎**을 정의해 DataFrame을 생성해 본다.\n",
    "\n",
    "```\n",
    "StructType([\n",
    "    StructField(컬럼명, StringType(), True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "\n",
    "**```StructType```** 으로 구조체를 선언하고, 컬럼에 대해 **```StructField```**  를 설정한다.\n",
    "* **컬럼**의 명칭\n",
    "* 앞서 소개했던 **데이터 타잎**\n",
    "* 마지막은 **NULL**이 허용되는지 여부\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 **myRows**를 데이터로, **mySchema**에서는 컬럼 명과 데이터타잎을 정의하여 ```createDataFrame()```함수의 인자로 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 설정한 데이터타입으로 설정되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='1', name='kim, js', height=170)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.2 RDD에서 생성하기\n",
    "\n",
    "RDD는 schema가 정해지지 않은 비구조적 데이터이다.\n",
    "이와 같이 **schema를 정의하지 않으면, Spark는 schema를 유추**하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 자동 인식\n",
    "\n",
    "RDD로부터 DataFrame을 생성할 수 있다. 이 경우 schema를 설정하지 않으면 자동으로 인식된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170), ('1','lee, sm', 175), ('2','lim, yg',180), ('2','lee',170)]\n",
    "\n",
    "myRdd = spark.sparkContext.parallelize(myList)\n",
    "\n",
    "rddDf=myRdd.toDF()\n",
    "rddDf1=spark.createDataFrame(myRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- L5~6: ```toDF()```로 변환하거나 직접 ```createDataFrame()``` 함수를 사용하여 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row를 사용\n",
    "\n",
    "학년year는 앞에서는 **```string```** 으로 인식되었다. 이번에는 **형변환** 을 해 본다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]), name=x[1], height=int(x[2])))\n",
    "_myDf=spark.createDataFrame(_myRdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L3: RDD의 ```map()``` 함수를 사용하여 각 속성에 명칭, year, name, height를 설정한다. 0, 1번째 속성을 ```int()``` 함수로 형변환을 한다.\n",
    "- L4: RDD에서 DataFrame을 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=1, name='kim, js', height=170)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```Row()```를 사용하여 RDD를 생성할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1=Row(name=\"js1\", age=10)\n",
    "r2=Row(name=\"js2\", age=20)\n",
    "_myRdd=spark.sparkContext.parallelize( [r1,r2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='js1', age=10), Row(name='js2', age=20)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  schema를 정의하고 생성\n",
    "\n",
    "앞서 보았듯이, schema를 정의하고 RDD에서 DataFrame을 생성할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "schema=StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "_myDf=spark.createDataFrame(_myRdd, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L4~8: **```StructType```** 을 선언하고,\n",
    "컬럼에 대해 **```StructField```** 를 **컬럼명**, **데이터 타잎**, **NULL**이 허용되는지 여부를 설정한다.\n",
    "과거 버전에서는 컬럼명이 정렬되면서 age, name 순서가 변경되었다.\n",
    "현재는 **컬럼명을 정렬하지 않으므로, 순서대로** 아래와 같이 생성하면 된다.\n",
    "- L10: RDD, schema를 적용하여 DataFrame을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| js1| 10|\n",
      "| js2| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "schema를 정해서 RDD로부터 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4.3 Pandas\n",
    "\n",
    "Spark Dataframe은 다른 프로그래밍 언어에서도 분석도구로 많이 사용되는 형식이다.\n",
    "엑셀 스프레드쉬트와 비슷하다. 또한 최근 많이 사용되는 **R**의 Dataframe이나 **Python Pandas**를 예로 들 수 있다.\n",
    "Spark와 Pandas의 Dataframe을 비교하면,\n",
    "**Pandas**는 데이터 양이 적은 경우, Spark는 분산처리할 수 있으므로 빅데이터에 보다 적합하다.\n",
    "API를 사용하게 되면 Spark Dataframe과 Pandas 간에는 차이가 있다.\n",
    "\n",
    "구분 | DataFrame | Pandas\n",
    "-------|-------|-------\n",
    "csv 파일 읽기 | read.json() | read_csv()\n",
    "데이터타입 | inferschema=True 설정하면 추정 | 모두 strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe을 Pandas로 변환\n",
    "\n",
    "Spark Dataframe을 ```toPandas()``` 함수를 사용하여 **Pandas로 변환**할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas에서 csv 쓰기\n",
    "\n",
    "Dataframe을 **csv**파일로 내보내고 Pandas로 읽어보자.\n",
    "\n",
    "Dataframe을 csv로 쓰려면 라이브러리 ```com.databricks.spark.csv```를 사용해야 한다. **파일이 아니라 디렉토리가 생성**되고 그 안에 파일로 쓰여지게 된다. Pandas를 사용하면 우리가 보통 사용하는 하나의 파일로 쓰여진다.\n",
    "\n",
    "한 번 생성이 되면 덮어쓰기를 하지 않기 때문에, 다시 write() 할 경우에는 이전 디렉토리를 삭제하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))\n",
    "myDf.write.format('csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Py4JJavaError                             Traceback (most recent call last)\n",
    "Cell In [70], line 2\n",
    "      1 import os\n",
    "----> 2 myDf.write.format('csv').save(os.path.join('data','_myDf.csv'))\n",
    "...\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o278.save\n",
    ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
    "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장하면서 py4j 오류가 발생하는데, 앞서 윈도우 입출력을 할 경우에 발생한다.\n",
    "\n",
    "주피터 노트북의 커널을 재시작하고, 환경변수를 설정하고 재실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HADOOP_HOME']=\"C:\\\\Users\\\\jsl\\\\Code\\\\201711111\" #os.getcwd()\n",
    "os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "_myDf=spark.createDataFrame([('1','kim, js', 170),('1','lee, sm', 175)])\n",
    "_myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.part-00000-43fee45d-8e7d-4853-8448-ce873bedf202-c000.csv.crc',\n",
       " '._SUCCESS.crc',\n",
       " 'part-00000-43fee45d-8e7d-4853-8448-ce873bedf202-c000.csv',\n",
       " '_SUCCESS']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"data/_myDf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas를 이용하여 Dataframe을 csv파일로 내보낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas에서 컬럼을 생성,삭제 해보자.\n",
    "\n",
    "recode - 현재 변수 값을 다시 줄 경우\n",
    "\n",
    "나라별 국제전화코드는 Japan: 81, South Korea: 82, Hong Kong: 852, Australia: 61을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "icc = pd.DataFrame( { 'country': ['South Korea','Japan','Hong Kong'],'codes': [81, 82, 852] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81\n",
       "1        Japan     82\n",
       "2    Hong Kong    852"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전화코드가 81인 경우 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc[icc['codes']==81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.4 csv 파일에서 생성\n",
    "\n",
    "#### RDD에서 DataFrame\n",
    "\n",
    "앞서 RDD에서 읽었던 csv파일을 다시 읽어보자.\n",
    "\n",
    "```sparkContext.textFile()``` 함수로 읽은 파일은 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(\"data\", \"ds_spark_2cols.csv\")\n",
    "lines = spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD에서 ```Row()```를 사용하여 Dataframe으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_col12 = lines.map(lambda l: l.split(\",\"))\n",
    "col12 = _col12.map(lambda p: Row(col1=int(p[0].strip()), col2=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(col12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col1=35, col2=2),\n",
       " Row(col1=40, col2=27),\n",
       " Row(col1=12, col2=38),\n",
       " Row(col1=15, col2=31),\n",
       " Row(col1=21, col2=1),\n",
       " Row(col1=14, col2=19),\n",
       " Row(col1=46, col2=1),\n",
       " Row(col1=10, col2=34),\n",
       " Row(col1=28, col2=3),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=16, col2=2),\n",
       " Row(col1=30, col2=3),\n",
       " Row(col1=32, col2=2),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=31, col2=2),\n",
       " Row(col1=22, col2=1),\n",
       " Row(col1=12, col2=3),\n",
       " Row(col1=39, col2=29),\n",
       " Row(col1=19, col2=37),\n",
       " Row(col1=25, col2=2)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame으로 직접 읽기\n",
    "\n",
    "format().load() 또는 csv() 함수로 csv 파일을 읽어서 DataFrame을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format load\n",
    "\n",
    "csv(파일) 또는 format(\"csv\").load(파일) 명령어로 csv파일을 DataFrame으로 읽어올 수 있다.\n",
    "\n",
    "특정 형식을 지정할 경우 ```.format('com.databricks.spark.csv')```와 같이 특정 csv 패키지를 적용할 수 있다. 보다 어려운 이 방법은 spark.jars.packages=com.databricks:spark-csv_2.11:1.5.0 의 설정이 필요하다 (또는 설정파일 ```spark-defaults.conf```에 추가할 수 있다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('csv')\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 2: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- 4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "inferschema를 제외하면, string으로 자동인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      " |-- 3: string (nullable = true)\n",
      " |-- 4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### csv\n",
    "\n",
    "또는 \n",
    "```csv(\"path\")```로 직접 DataFrame으로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .csv(os.path.join('data', 'ds_spark.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.5 tsv 파일 읽기\n",
    "\n",
    "tsv (tab-separated values)는 **Tab으로 분리된 파일**을 말한다.\n",
    "'\\t'이 포함되어 있는 경우, 혹시 string으로 데이터타잎을 설정하기도 한다 (과거 Spark 버전에서)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAB은 whitespace이므로 그냥 split()을 해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.658985, 4.285136])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "URL로 가서 http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights\n",
    "마우스로 긁어서 50행만 복사해 보자.\n",
    "18세 1993년 18세 이하 225,000 건의 키 (in), 몸무게 (lbs) 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RDD\n",
    "\n",
    "우선 RDD로 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "_tRdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tsv는 '\\t'로 분리해도 되고, split() 함수는 <TAB>을 포함한 whitespace로 분할하게 되므로 그냥 두어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "_tRddSplitted = _tRdd.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형변환\n",
    "\n",
    "위 tsv 파일에서 생성한 RDD를 탭으로 분리하면서, ```float()```로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=_tRdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 설정\n",
    "\n",
    "schema를 **자동으로 설정하면, string**으로 읽혀진다.\n",
    "schema를 설정한다고 해도, string -> integer, double로 형변환은 이루어지지 않는다.\n",
    "string의 **형변환을 명시적**으로 해주어야 한다.\n",
    "\n",
    "```python\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRdd, mySchema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame 생성\n",
    "\n",
    "위 tRdd로부터 컬럼명을 주어 DataFrame을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDfNamed = spark.createDataFrame(tRdd, [\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDfNamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDfNamed.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼을 split으로 분할\n",
    "\n",
    "```.read.text()``` 함수를 이용해서 파일을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = spark.read.text(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDftxt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력: ```<TAB>``` 으로 분리되지 못해 전체를 하나로 출력하고 있다 (변수명 value, 타입은 string으로 읽고 있어, 분리해야 한다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pyspark.sql.functions은 함수이므로, import할 경우\n",
    "\n",
    "* ```import pyspark.sql.functions.split``` 이렇게 하지 않고, \n",
    "* ```from pyspark.sql.functions import split``` 이렇게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(tDftxt['value'], '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "분리된 컬럼은 getItem() 함수로 가져와서 각 각 weight, height 컬럼이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = tDftxt.withColumn('weight', split_col.getItem(1))\n",
    "tDftxt = tDftxt.withColumn('height', split_col.getItem(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------+\n",
      "|            value|weight|height|\n",
      "+-----------------+------+------+\n",
      "| 1\\t65.78\\t112.99| 65.78|112.99|\n",
      "| 2\\t71.52\\t136.49| 71.52|136.49|\n",
      "| 3\\t69.40\\t153.03| 69.40|153.03|\n",
      "| 4\\t68.22\\t142.34| 68.22|142.34|\n",
      "| 5\\t67.79\\t144.30| 67.79|144.30|\n",
      "| 6\\t68.70\\t123.30| 68.70|123.30|\n",
      "| 7\\t69.80\\t141.49| 69.80|141.49|\n",
      "| 8\\t70.01\\t136.46| 70.01|136.46|\n",
      "| 9\\t67.90\\t112.37| 67.90|112.37|\n",
      "|10\\t66.78\\t120.67| 66.78|120.67|\n",
      "|11\\t66.49\\t127.45| 66.49|127.45|\n",
      "|12\\t67.62\\t114.14| 67.62|114.14|\n",
      "|13\\t68.30\\t125.61| 68.30|125.61|\n",
      "|14\\t67.12\\t122.46| 67.12|122.46|\n",
      "|15\\t68.28\\t116.09| 68.28|116.09|\n",
      "|16\\t71.09\\t140.00| 71.09|140.00|\n",
      "|17\\t66.46\\t129.50| 66.46|129.50|\n",
      "|18\\t68.65\\t142.97| 68.65|142.97|\n",
      "|19\\t71.23\\t137.90| 71.23|137.90|\n",
      "|20\\t67.13\\t124.04| 67.13|124.04|\n",
      "+-----------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDftxt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Row 데이터의 split\n",
    "\n",
    "```Row``` 객체에 ```name```이 있다. 이 데이터에는 컴마가 포함되어 있다.\n",
    "이 데이터를 split해보려고 한다면, 오류가 발생한다.\n",
    "\n",
    "split하려는 ```name```은 컬럼이다. 이를 rdd로 변환한 후 ```map()``` 함수를 적용하려면 \n",
    "```'Column' object is not callable``` 오류가 발생한다.\n",
    "\n",
    "즉, 컬럼이기 때문에 ```map()``` 함수 적용이 가능하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmyDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf.name.rdd.map(lambda line:[line.split(',')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```dict```로 변환하면 키에는 속성명 ```name```이, 밸류에는 ```kim, js```가 할당된다.\n",
    "그렇게 하고 나면, ```kim, js```를 컴마를 기준으로 분리할 수 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "r=Row(name=u'kim, js')\n",
    "rd=r.asDict()                   # transform to Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kim', ' js']\n"
     ]
    }
   ],
   "source": [
    "for i in rd.values():\n",
    "    print(i.split(','))         # split values by comma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### csv 함수로 tsv 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|_c0|  _c1|   _c2|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "|  2|71.52|136.49|\n",
      "|  3| 69.4|153.03|\n",
      "|  4|68.22|142.34|\n",
      "|  5|67.79| 144.3|\n",
      "|  6| 68.7| 123.3|\n",
      "|  7| 69.8|141.49|\n",
      "|  8|70.01|136.46|\n",
      "|  9| 67.9|112.37|\n",
      "| 10|66.78|120.67|\n",
      "| 11|66.49|127.45|\n",
      "| 12|67.62|114.14|\n",
      "| 13| 68.3|125.61|\n",
      "| 14|67.12|122.46|\n",
      "| 15|68.28|116.09|\n",
      "| 16|71.09| 140.0|\n",
      "| 17|66.46| 129.5|\n",
      "| 18|68.65|142.97|\n",
      "| 19|71.23| 137.9|\n",
      "| 20|67.13|124.04|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### S.4.6 JSON 파일에서 생성\n",
    "\n",
    "#### JSON\n",
    "JSON은 JavaScript Object Notation, 즉 자바스크립트에서 사용되는 표기법. 사람이 읽을 수 있는 텍스트로 표기하며, key-value 쌍으로 되어 있다.\n",
    "현재 널리 쓰이고 있어 XML 대용으로 널리 쓰이고 있다.\n",
    "Spark example 폴더에 있는 ```people.json``` JSON 파일이다.\n",
    "\n",
    "```python\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트위터 데이터\n",
    "\n",
    "아래는 트윗 1개의 샘플이고, JSON 형식으로 구성되어 있다.\n",
    "\n",
    "실제 트윗 데이터를 구할 수 없다면 아래 샘플을 파일로 저장한 후 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%%writefile src/ds_twitter_seoul_3.json\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일에서 트윗 읽기\n",
    "\n",
    "그러나 JSON파일은 **JSON이 아니라 문자열**이다. 파일에서 읽은 후 **JSON으로 변환**을 해야 한다.\n",
    "트윗은 '\\n'으로 하나씩 구분되어 있고 ```readlines()``` 함수로 전체를 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('src','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_json_str = json.loads(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas에서  트윗 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 Tweet데이터는 json형식이다. 따라서 list 구조로 만들어 주려면, 앞 뒤로 대괄호 ```[ ]```를 넣고, 각 tweet은 컴마로 연결한다.\n",
    "```join()``` 함수는 인자를 구분자 \",\"로 병합한다.\n",
    "\n",
    "파일을 'rb' 바이너리(이진수 표현)로 읽고 있다는 점에 유의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'bytes'> 길이: 6910\n"
     ]
    }
   ],
   "source": [
    "#data_json_str =\"[\" + \",\".join(data) + \"]\"\n",
    "data_json_str = b\"[\" + b\",\".join(data) + b\"]\"\n",
    "print(\"Type: {} 길이: {}\".format(type(data_json_str), len(data_json_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 문자열 합성을 한다. 예를 들면, \",\".join([\"A\", \"B\", \"C\"]) -> A,B,C\n",
    "- L2: TypeError 문자열이 아닌 bytes를 합성하기 때문에 오류가 발생한다.\n",
    "- L3: data는 bytes이고, 이를 join()하는 함수는 str을 대상으로 하기 때문에, 문자열에 ```b```를 붙여 bytes로 변환한 후 연결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 JSON으로 변환이 되지 않았고, 문자열 전체 길이를 알아 보자. 파일의 크기를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "tweetPd = pd.read_csv(BytesIO(data_json_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L3: BytesIO를 해주지 않으면 TypeError: Expected file path name or file-like object, got <class 'bytes'> type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[{\"contributors\": null</th>\n",
       "      <th>\"truncated\": false</th>\n",
       "      <th>\"text\": \"RT @soompi: #SEVENTEEN’s Mingyu</th>\n",
       "      <th>Jin Se Yeon</th>\n",
       "      <th>And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\"</th>\n",
       "      <th>\"is_quote_status\": false</th>\n",
       "      <th>\"in_reply_to_status_id\": null</th>\n",
       "      <th>\"id\": 801657325836763136</th>\n",
       "      <th>\"favorite_count\": 0</th>\n",
       "      <th>\"entities\": {\"symbols\": []</th>\n",
       "      <th>...</th>\n",
       "      <th>\"is_translator\": false}.1</th>\n",
       "      <th>\"geo\": null.1</th>\n",
       "      <th>\"in_reply_to_user_id_str\": null.1</th>\n",
       "      <th>\"possibly_sensitive\": false.1</th>\n",
       "      <th>\"lang\": \"en\".2</th>\n",
       "      <th>\"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\"</th>\n",
       "      <th>\"in_reply_to_status_id_str\": null.1</th>\n",
       "      <th>\"place\": null.1</th>\n",
       "      <th>\"metadata\": {\"iso_language_code\": \"en\".1</th>\n",
       "      <th>\"result_type\": \"recent\"}}.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  [{\"contributors\": null   \"truncated\": false  \\\n",
       "0                      ]                  NaN   \n",
       "\n",
       "    \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu   Jin Se Yeon  \\\n",
       "0                                        NaN           NaN   \n",
       "\n",
       "    And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\"  \\\n",
       "0                                                NaN                                                   \n",
       "\n",
       "    \"is_quote_status\": false   \"in_reply_to_status_id\": null  \\\n",
       "0                        NaN                             NaN   \n",
       "\n",
       "    \"id\": 801657325836763136   \"favorite_count\": 0  \\\n",
       "0                        NaN                   NaN   \n",
       "\n",
       "    \"entities\": {\"symbols\": []  ...   \"is_translator\": false}.1  \\\n",
       "0                          NaN  ...                         NaN   \n",
       "\n",
       "    \"geo\": null.1   \"in_reply_to_user_id_str\": null.1  \\\n",
       "0             NaN                                 NaN   \n",
       "\n",
       "    \"possibly_sensitive\": false.1   \"lang\": \"en\".2  \\\n",
       "0                             NaN              NaN   \n",
       "\n",
       "    \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\"  \\\n",
       "0                                              NaN   \n",
       "\n",
       "    \"in_reply_to_status_id_str\": null.1   \"place\": null.1  \\\n",
       "0                                   NaN               NaN   \n",
       "\n",
       "    \"metadata\": {\"iso_language_code\": \"en\".1   \"result_type\": \"recent\"}}.1  \n",
       "0                                        NaN                           NaN  \n",
       "\n",
       "[1 rows x 211 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetPd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 Pandas로 읽어 보자. Pandas 라이브러리에서 제공하는 read_json()함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweetPd = pd.read_json(os.path.join('src','ds_twitter_seoul_3.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape()은 행과 열의 갯수를 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 26)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetPd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() 함수는 행의 갯수를 나타내는데, 숫자가 서로 다른 것은 비워있는 경우가 서로 다르기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                  0\n",
      "truncated                    68\n",
      "text                         68\n",
      "is_quote_status              68\n",
      "in_reply_to_status_id         0\n",
      "id                           68\n",
      "favorite_count               68\n",
      "entities                      4\n",
      "retweeted                    68\n",
      "coordinates                   0\n",
      "source                       68\n",
      "in_reply_to_screen_name       0\n",
      "in_reply_to_user_id           0\n",
      "retweet_count                68\n",
      "id_str                       68\n",
      "favorited                    68\n",
      "retweeted_status             17\n",
      "user                         36\n",
      "geo                           0\n",
      "in_reply_to_user_id_str       0\n",
      "possibly_sensitive           68\n",
      "lang                         68\n",
      "created_at                   68\n",
      "in_reply_to_status_id_str     0\n",
      "place                         0\n",
      "metadata                      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (tweetPd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼 'id'를 10개만 읽어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "symbols                  801657325836763136\n",
       "user_mentions            801657325836763136\n",
       "hashtags                 801657325836763136\n",
       "urls                     801657325836763136\n",
       "contributors             801657325836763136\n",
       "truncated                801657325836763136\n",
       "text                     801657325836763136\n",
       "is_quote_status          801657325836763136\n",
       "in_reply_to_status_id    801657325836763136\n",
       "id                       801657325836763136\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetPd['id'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame에서 트윗 읽기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfile= os.path.join('src','ds_twitter_seoul_3.json')\n",
    "\n",
    "tweetDf= spark.read.json(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- iso_language_code: string (nullable = true)\n",
      " |    |-- result_type: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- possibly_sensitive: boolean (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- contributors: string (nullable = true)\n",
      " |    |-- coordinates: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- symbols: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- extended_entities: struct (nullable = true)\n",
      " |    |    |-- media: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |-- media_url: string (nullable = true)\n",
      " |    |    |    |    |-- media_url_https: string (nullable = true)\n",
      " |    |    |    |    |-- sizes: struct (nullable = true)\n",
      " |    |    |    |    |    |-- large: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- medium: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- small: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |    |-- thumb: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- h: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- resize: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- w: long (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- favorite_count: long (nullable = true)\n",
      " |    |-- favorited: boolean (nullable = true)\n",
      " |    |-- geo: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |    |-- in_reply_to_status_id: string (nullable = true)\n",
      " |    |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |    |-- is_quote_status: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- metadata: struct (nullable = true)\n",
      " |    |    |-- iso_language_code: string (nullable = true)\n",
      " |    |    |-- result_type: string (nullable = true)\n",
      " |    |-- place: string (nullable = true)\n",
      " |    |-- possibly_sensitive: boolean (nullable = true)\n",
      " |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- retweeted: boolean (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- truncated: boolean (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- default_profile: boolean (nullable = true)\n",
      " |    |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- entities: struct (nullable = true)\n",
      " |    |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- url: struct (nullable = true)\n",
      " |    |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- indices: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- favourites_count: long (nullable = true)\n",
      " |    |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- following: boolean (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |    |-- is_translator: boolean (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- listed_count: long (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- notifications: boolean (nullable = true)\n",
      " |    |    |-- profile_background_color: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |    |-- profile_banner_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url: string (nullable = true)\n",
      " |    |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |    |-- profile_link_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |    |-- profile_text_color: string (nullable = true)\n",
      " |    |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |    |-- protected: boolean (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |    |    |-- time_zone: string (nullable = true)\n",
      " |    |    |-- translator_type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- utc_offset: long (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: boolean (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: boolean (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrame에서도 컬럼 'id', 'text'를 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|                id|                text|\n",
      "+------------------+--------------------+\n",
      "|801657325836763136|RT @soompi: #SEVE...|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.select('id', 'text').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트윗은 계층적 구조를 가지고 있다. 예를 들어 'user' 항목은 다음과 같은 세부항목으로 구성되어 있다.\n",
    "그 세부항목을 읽으려면 ```user.id```와 같이 점 연산자로 명세하면 된다.\n",
    "'user.created_at'는 'user'의 'created_at'이고 'created_at'은 최상위계층의 항목을 말한다.\n",
    "\n",
    "\n",
    "user\": {\n",
    "    ...\n",
    "    \"id\": 791090169818521600,\n",
    "    ...\n",
    "    \"followers_count\": 0,\n",
    "    ...\n",
    "    \"friends_count\": 7,\n",
    "    ...\n",
    "    \"lang\": \"es\",\n",
    "    ...\n",
    "    \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\",\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+--------------------+\n",
      "|                id|followers_count|          created_at|          created_at|\n",
      "+------------------+---------------+--------------------+--------------------+\n",
      "|791090169818521600|              0|Wed Oct 26 01:32:...|Thu Nov 24 05:22:...|\n",
      "+------------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweetDf.select('user.id', 'user.followers_count', 'user.created_at', 'created_at').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 월드컵 데이터 JSON의 URL에서 DataFrame 생성\n",
    "\n",
    "url에서 데이터를 직접 읽어 DataFrame을 생성하는 방법은 지원되지 않고 있다.\n",
    "requests 라이브러리를 사용하여, url에서 데이터를 가져오고, DataFrame을 생성하여 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### URL에서 JSON 읽기\n",
    "\n",
    "웹에서 읽는 JSON은 문자열이라는 점에 유의한다. 따라서 json()함수로 변환하는 것이 필요하다.\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    \"Year\": 1930,\n",
    "    \"Team\": \"Argentina\",\n",
    "    \"Number\": \"\",\n",
    "    \"Position\": \"GK\",\n",
    "    \"FullName\": \"Ãngel Bossio\",\n",
    "    \"Club\": \"Club AtlÃ©tico Talleres de Remedios de Escalada\",\n",
    "    \"ClubCountry\": \"Argentina\",\n",
    "    \"DateOfBirth\": \"1905-5-5\",\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    ...\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "request.get() 함수는 Response를 반환한다.\n",
    "이러한 Reponse에 포함된 텍스트를 사용하게 된다.\n",
    "웹에서 읽은 텍스트는 어떤 자료구조를 가졌다고 하더라도 단순 문자열이다. 텍스트를 살펴보고, 구조가 있다면 알맞게 변환해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Response:  <class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of Response: \", type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 읽은 텍스트를 json으로 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print (type(wc), type(wc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc는 아래와 같이 ```[ {...}, {...}, ...]```, 즉 배열내에 dictionary가 컴마로 연결되어 있다.\n",
    "이와 같이 파일에서 읽을 경우, 그 구조를 정확하게 이해하는 것이 중요하다.\n",
    "\n",
    "```python\n",
    "[{'Competition': 'World Cup',\n",
    "  'Year': 1930,\n",
    "  'Team': 'Argentina',\n",
    "  'Number': '',\n",
    "  'Position': 'GK',\n",
    "  'FullName': 'Ãngel Bossio',\n",
    "  'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
    "  'ClubCountry': 'Argentina',\n",
    "  'DateOfBirth': '1905-5-5',\n",
    "  'IsCaptain': False},\n",
    " {'Competition': 'World Cup',\n",
    "  'Year': 1930,\n",
    "  'Team': 'Argentina',\n",
    "  'Number': '',\n",
    "  'Position': 'GK',\n",
    "  'FullName': 'Juan Botasso',\n",
    "  'Club': 'Quilmes AtlÃ©tico Club',\n",
    "  'ClubCountry': 'Argentina',\n",
    "  'DateOfBirth': '1908-10-23',\n",
    "  'IsCaptain': False},\n",
    " ... ]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Competition': 'World Cup',\n",
       " 'Year': 1930,\n",
       " 'Team': 'Argentina',\n",
       " 'Number': '',\n",
       " 'Position': 'GK',\n",
       " 'FullName': 'Ãngel Bossio',\n",
       " 'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       " 'ClubCountry': 'Argentina',\n",
       " 'DateOfBirth': '1905-5-5',\n",
       " 'IsCaptain': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame 생성\n",
    "\n",
    "위 ```wc```는 JSON을 포함하는 리스트이다.\n",
    "JSON은 key, value로 구성되어, 컬럼명을 key에서 가져올 수 있다.\n",
    "버전에 따라 주의가 필요하다. **딕셔너리에서는 바로 스키마를 추정할 수 없고 pyspark.sql.Row를 사용하라고 경고**가 뜨기도 한다.\n",
    "(UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "_wcDf=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', Competition='World Cup', DateOfBirth='1905-5-5', FullName='Ãngel Bossio', IsCaptain=False, Number='', Position='GK', Team='Argentina', Year=1930)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_wcDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row를 이용해서 Dictionary에서 DataFrame 생성하기\n",
    "\n",
    "앞서 살펴보았듯이, Python dict로부터 DataFrame을 생성하려면 pyspark.sql.Row를 활용할 수 있다.\n",
    "\n",
    "wc는 리스트이고 그 요소는 dictionary이다.\n",
    "dictionary 구조를 풀어서 Row에 넘겨주어야 한다.\n",
    "우선 Python에서 어떻게 인자를 풀어서 넘겨주는지 배워보자.\n",
    "\n",
    "인자가 복수일 경우 **리스트, dictionary와 같은 구조**로 묶게 되는데, \n",
    "\n",
    "이러한 이터러블(iterable) 객체 (예: 리스트, 튜플, 문자열 등)의 요소를 개별적인 변수로 추출하거나 함수 호출 시 인수로 전달할 때 **별표 연산자** ```*args```, ```**args```를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 별표 1개 ```*```: **리스트**에서 인자 풀어내기\n",
    "\n",
    "예를 들어, range() 함수는 시작과 끝을 인자로 가지는 함수이다.\n",
    "시작과 끝이라는 2개의 인자를 함수에 넘겨주어 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "range의 인자를 보자. 정수 2개를 인자로 넘겨줘야 하지만, 그 자리에 리스트를 넘겨주면 타입오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmyList\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "list(range(myList))  # TypeError: 'list' object cannot be interpreted as an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작, 끝 변수가 따로 없을 경우, **리스트를 넘겨주고 여기서 풀어서 하나씩 인자를 사용할 경우** ```*```를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(*myList))  # unpack args and get 1 for the start and 6 for the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예를 보자. f()는 복수의 인자를 받아서 하나씩 출력하는 함수이다.\n",
    "f()를 호출할 때 여러 개를 넘겨주면, 1개만 받으므로 타입오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "f() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: f() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "def f(args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복수의 인자를 받게 될 것이라면 * 연산자를 사용한다. 그러면 복수의 인자를 풀어서 사용하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0~1~2~3~"
     ]
    }
   ],
   "source": [
    "def f(*args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "def myFn(a, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "myList = [1, 2, 3]\n",
    "myFn(*myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 별표 2 개 ```**```: **dictionary**에서 인자 풀어내기\n",
    "\n",
    "dictionary를 언패킹할 경우 ```*``` 하나인 경우, 키가 출력된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id name uni\n"
     ]
    }
   ],
   "source": [
    "def myFn(a, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "myDict = {\"id\":1, \"name\":\"jsl\", \"uni\":\"smu\"}\n",
    "myFn(*myDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```**```두개인 경우, 값이 출력된다. 이 때 함수의 인자 명은 dictionary의 키명과 일치해야 하고, 그렇지 않으면 TypeError가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 jsl smu\n"
     ]
    }
   ],
   "source": [
    "def myFn(id, name, uni):\n",
    "    print(id, name, uni)\n",
    "\n",
    "myDict = {\"id\":1, \"name\":\"jsl\", \"uni\":\"smu\"}\n",
    "myFn(**myDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary에서 Dataframe 생성\n",
    "\n",
    "반복문으로 하나의 dictionary를 가져와 ```Row()```를 생성하고 있다.\n",
    "별표 2개를 겹쳐 쓴 ```**```는 dictionary를 풀어서 Row 생성자에 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 리스트에서 Row 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Row('1', 'kim, js', 170)>, <Row('1', 'lee, sm', 175)>, <Row('2', 'lim, yg', 180)>, <Row('2', 'lee', 170)>]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170), ('1','lee, sm', 175), ('2','lim, yg',180), ('2','lee',170)]\n",
    "rows=[Row(*each) for each in myList]\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L4: ```*each```는 리스트의 각 요소의 묶음을 푼다(언패킹)는 의미이다. ```Row(*each)```는 ```Row('1','kim, js',170)```가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 잘 읽혔는지, 일부를 출력해보자. 아래에서 보듯이, 아르헨티나 문자가 유니코드로 잘 출력이 되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  DataFrame의 shema 설정\n",
    "\n",
    "wcRdd에서 DataFrame을 생성하려고 하면 아래와 같이 schema를 설정할 수 있다.\n",
    "그러나 영어가 아닌 다른 나라의 유니코드 문자 또는 생년월일의 결측값 등으로 오류가 발생한다.\n",
    "자동으로 인식한 Schema가 더 낫게 인식하고 있다.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "json으로부터 RDD는 바로 생성할 수 있다. RDD를 만든 후, 그로부터 DataFrame을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Competition': 'World Cup',\n",
       "  'Year': 1930,\n",
       "  'Team': 'Argentina',\n",
       "  'Number': '',\n",
       "  'Position': 'GK',\n",
       "  'FullName': 'Ãngel Bossio',\n",
       "  'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       "  'ClubCountry': 'Argentina',\n",
       "  'DateOfBirth': '1905-5-5',\n",
       "  'IsCaptain': False}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDfFromRdd = spark.createDataFrame(wcRdd)\n",
    "wcDfFromRdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', Competition='World Cup', DateOfBirth='1905-5-5', FullName='Ãngel Bossio', IsCaptain=False, Number='', Position='GK', Team='Argentina', Year=1930)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDfFromRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측 값\n",
    "\n",
    "\n",
    "* ```null```은 결측, 즉 \"no value\" 또는 \"nothing\"을 말한다.\n",
    "* ```NaN```은 \"Not a Number\", 즉 수학에서 0.0/0.0과 같이 의미가 없는 연산의 결과를 말한다.\n",
    "\n",
    "```IsCaptain``` 항목은 boolean 타입이라 isnan() 함수가 오류를 발생한다. ```'isnan(`IsCaptain`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`IsCaptain`' is of boolean type.```\n",
    "그래서 항목에서 제거하고 확인하기로 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = wcDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove('IsCaptain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|Competition|isnan(Competition)|\n",
      "+-----------+------------------+\n",
      "|  World Cup|             false|\n",
      "|  World Cup|             false|\n",
      "+-----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "wcDf.select(\"Competition\", isnan(\"Competition\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|Competition|Competition aliased|\n",
      "+-----------+-------------------+\n",
      "|  World Cup|              false|\n",
      "|  World Cup|              false|\n",
      "+-----------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.select(\"Competition\", isnull(\"Competition\").alias(\"Competition aliased\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|Competition|Year|Team|Number|Position|FullName|Club|ClubCountry|DateOfBirth|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|          0|   0|   0|     0|       0|       0|   0|          0|          0|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "wcDf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|Competition|Year|Team|Number|Position|FullName|Club|ClubCountry|DateOfBirth|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "|          0|   0|   0|     0|       0|       0|   0|          0|          0|\n",
      "+-----------+----+----+------+--------+--------+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "wcDf.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 명령어가 상당히 복잡하다.\n",
    "명령문은 한 줄이지만, 자세히 들여다 보면, 꽤 많은 명령문이 작용하고 있다는 것을 알 수 있다.\n",
    "\n",
    "* 컬럼을 하나씩 읽는 반복문을 보자. ```for c in cols```에서 컬럼을 하나씩 c를 읽고 있다. \n",
    "이런 반복문에서 cols는 전체 컬럼을 지칭한다 (IsCaptain은 제외).\n",
    "* isNull 함수는 앞에 컬럼을 붙여서 ```col(c).isNull()``` 이렇게 해야 한다. 반면에 ```pyspark.sql.functions.isnan()```함수는 인자로 c를 받는다. 두 함수가 작용하는 차이는 없다.\n",
    "* ```when(condition, value)```함수는 조건을 설정하고, 그 조건에 맞으면 ```value```로 설정한다 (when-otherwise는 곧 배우게 된다)\n",
    "즉 결측값이 있는 경우 컬럼명을 넣고, ```count()``` 함수로 개수를 센다. when 명령어에 ```pyspark.sql.Column.otherwise()```가 같이 사용되지 않다면, 반환 값이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 형변환\n",
    "\n",
    "스키마를 자동으로 유추한 형식을 보면,\n",
    "DateOfBirth, Number 컬럼이 문자열로 인식되어 있어 만족스럽지 못하다.\n",
    "컬럼 'DateOfBirth'는 'DoB'로 ```DateType()```, 컬럼 'Number'는 'NumberInt'로 \"Integer\" 형으로 설정해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ```DateType``` 형변환\n",
    "\n",
    "* Python ```datetime```을 사용한 DateType 컬럼 생성\n",
    "\n",
    "Python ```datetime.strptime(년월일문자열, 형식)```은 년월일 문자열을 다음과 같은 년, 월, 일 형식으로 변환해준다.\n",
    "\n",
    "표시 | 설명               |예\n",
    "---|--------------------|--------------------\n",
    "%d | 숫자로 표현한 2자리 일자 | 01, 02, ..., 31\n",
    "%m | 숫자로 표현한 2자리 월  | 01, 02, ..., 12\n",
    "%y | 숫자로 표현한 2자리 년수 (세기 불포함) | 00, 01, ..., 99\n",
    "%Y | 숫자로 표현한 4자리 년수 (세기 포함)  | 0001, 0001, ..., 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print (datetime.strptime(\"11/01/2023\", '%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datetime을 사용해서 일자를 DateType()으로 변환해보자.\n",
    "이 때 사용자정의함수 udf()를 만들어 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn()은 컬럼을 추가하는 명령어이다. 방금 만든 udf 함수를 호출해서 형변환을 하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.withColumn('date1', toDate(wcDf['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 결과를 출력하면 아래와 같이 ```ValueError```가 출력된다.\n",
    "\n",
    "```python\n",
    "wcDf.take(1)\n",
    "\n",
    "ValueError: time data '1905-5-5' does not match format '%m/%d/%Y'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pyspark ```to_date()``` 함수\n",
    "\n",
    "이번에는 pyspark에서 제공하는 to_date() 함수를 사용하자.\n",
    "방금 오류가 발생한 컬럼은 제거하고 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.drop('date1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_date() 함수는 string (StringType)을 date (DateType) 으로 형변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "_wcDfCasted=wcDf.withColumn('date2', to_date(wcDf['DateOfBirth'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pyspark ```cast()``` 함수\n",
    "\n",
    "pyspark의 cast() 함수로 형변환을 해보자.\n",
    "DateOfBirth와 더불어 Number 컬럼도 같이 Integer로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "wcDfCasted = _wcDfCasted.withColumn('date3', _wcDfCasted['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 ```Column``` 객체의 ```astype()``` 함수를 사용해서 형변환을 해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "wcDfCasted = _wcDfCasted.withColumn('date3', F.col('DateOfBirth').astype(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', F.col('Number').astype(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그대로 출력하면 오류가 발생할 수 있다.\n",
    "```SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0```라는 오류가 발생하면\n",
    "**```spark.sql.legacy.timeParserPolicy=LEGACY```**라고 설정을 변경해주어야 한다.\n",
    "\n",
    "\"spark.sql.legacy.timeParserPolicy\" 설정은 시간 관련 데이터 형식을 파싱할 때의 규정을 정하기 위해 설정하는데, 몇 가지의 가능한 설정이 있다.\n",
    "\n",
    "- \"CORRECTED\" (기본값): ISO 8601과 호환되지 않는 날짜 및 시간 데이터 형식을 수정하여 파싱.\n",
    "- \"LEGACY\": 과거 버전의 Spark와 호환되도록 시간 데이터를 파싱\n",
    "- \"EXCEPTION\": ISO 8601과 호환되지 않는 잘못된 데이터 형식을 파싱하려고 할 때 예외를 발생하게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 to_data() 또는 cast() 함수를 사용하여 일자 형변환 결과를 올바르게 출력하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date2=datetime.date(1905, 5, 5), date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 정리\n",
    "\n",
    "지금까지 작성한 코드는 **상호대화형**으로 작성되었다.\n",
    "배우거나 연습하기 위해 작성된 코드가 포함되었기 때문에 기능이 중복된 코드가 있을 수 있다.\n",
    "\n",
    "프로그램을 하는 **목적**이 무엇인지를 생각하고, **필요한 코드**만 남겨두자.\n",
    "URL에서 JSON 데이터를 읽어 DataFrame을 생성하고, 한 줄 출력하는 코드만 필요하다.\n",
    "일괄실행하기 위해서는 spark를 생성하는 코드가 있어야 한다.\n",
    "정리하면서 라이브러리는 앞 부분으로 위치시킨다.\n",
    "주석도 코드를 이해하는데 도움이 되므로 넣어준다.\n",
    "프로그램 절차에 맞추어 수정이 필요한 부분도 있다. 예를 들어 wcDfCasted는 wcDf로 변경하였다.\n",
    "\n",
    "아래 프로그램에 main() 함수를 넣어서 정리하면 더욱 좋겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import pyspark\n",
    "os.environ[\"PYSPARK_PYTHON\"]=sys.executable        #\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=sys.executable #\"/usr/bin/python3\"\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# read url json\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "\n",
    "# read dictionary into Row\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)\n",
    "\n",
    "# cast DoB string into date, Number string into integer\n",
    "wcDfCasted = wcDf.withColumn('date3', wcDf['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))\n",
    "\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.7 Parquet 파일 읽기, 쓰기\n",
    "\n",
    "Parquet(파케이)는 나무조각을 붙여넣은 마룻바닥이라는 뜻으로, Apache Hadoop에서는 컬럼별로 저장하는 데이터 압축형식으로, DataFrame에서 이 파일을 쓰거나, 읽을 수 있다. \n",
    "\n",
    "```python\n",
    "-rw-r--r-- 1 jsl jsl 522 10월  7 15:27 part-r-00000-0318688b-018f-4e55-858b-b4b78ac56532.snappy.parquet\n",
    "-rw-r--r-- 1 jsl jsl   0 10월  7 15:27 _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용했던 ```_myDf```를 parquet형식으로 저장해보자. 역시 winutils.exe, hadoop.dll 파일들을 설정해 두어야 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_myDf.write.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 쓰여진 parquet으로부터 읽어서 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_pDf=spark.read.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_pDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 DataFrame API 사용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 DataFrame 생성\n",
    "\n",
    "비어있는 DataFrame을 생성하려면, 빈 schema를 설정하고 emptyRdd()를 사용해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema = StructType([])\n",
    "emptyDf = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emptyDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range\n",
    "\n",
    "비어 있는 DataFrame보다는 일련의 수를 가진 DataFrame을 만들어 보자.\n",
    "```range(start, end=None, setp=1, numPartitions=None)``` 함수는 Python range() 함수와 같이 정수를 생성하고, 컬럼명 id를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(0, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 DataFrame을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수를 사용하려면, DataFrame이 만들어져 있어야 하고, 그 결과 컬럼이 생성된다. DataFrame을 생성하지 않은 채로 함수를 실행해려면 range() 함수가 유용하다.\n",
    "\n",
    "range(1)로 컬럼을 하나 생성한 후, current_date()를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-11-20|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark.range(1).select(F.current_date()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼명을 alias()로 정하면서, unix_timestamp()를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|current_timestamp|\n",
      "+-----------------+\n",
      "|       1732055950|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(F.unix_timestamp().alias(\"current_timestamp\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 결과 값만을 추출하려면 조금 복잡하다. rdd로 변환하고, collect() 한후 인덱스를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(current_timestamp=1732055969)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1).select(F.unix_timestamp().alias(\"current_timestamp\")).rdd.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1732055970"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1).select(F.unix_timestamp().alias(\"current_timestamp\")).rdd.collect()[0]['current_timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 추가 withColumn, 컬럼 삭제 Drop\n",
    "\n",
    "```withColumn()```은 열을 추가한다.\n",
    "앞서 사용하였던 키, 몸무게 파일에서 DataFrame을 생성하고 컬럼을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼은 자동 생성이 되었으므로, ```'_c0', '_c1', '_c2'```이고 사용하기 불편하므로 의미있는 컬럼으로 변경하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있는 ```_1```행을 ```integer```로 형변환해서 ```id```행을 만들고, 기존의 ```_1```행을 삭제해보자.\n",
    "```drop()```은 열을 삭제할 때 사용한다. 같은 방식으로 나머지도 변경해보자.\n",
    "컬럼은 점연산자 tDf._c0 또는 인덱스 tDf['_c1'] 어느 방식이나 모두 가능하다.\n",
    "\n",
    "cast() 함수에는 \"integer\" 또는 Spark의 데이터타입 IntegerType()이라고 적어도 된다.\n",
    "또는 statement로 적어도 된다.\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "tDf.withColumn(\"id\", F.expr(\"CAST(id AS INTEGER)\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.withColumn(\"id\", tDf._c0.cast(\"integer\"))\n",
    "tDf = tDf.withColumn(\"height\", tDf['_c1'].cast(\"double\"))\n",
    "tDf = tDf.withColumn(\"weight\", tDf['_c2'].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼의 형변환은 ```Column```의 ```astype()```을 사용해도 동일한 결과를 보여주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "tDf = tDf.withColumn(\"id\", F.col('_c0').astype(\"integer\"))\n",
    "tDf = tDf.withColumn(\"height\", F.col('_c1').astype(\"double\"))\n",
    "tDf = tDf.withColumn(\"weight\", F.col('_c2').astype(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼이 중복되었으므로, 삭제하자. 하나씩 또는 다음과 같이 한꺼번에 삭제할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.drop('_c0').drop('_c1').drop('_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형변환이 잘 되었고 컬럼명이 변경된 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자정의 함수 udf\n",
    "\n",
    "UDF User Defined Functions는 사용자 정의함수로서 DataFrame의 withColumn() 함수와 같이 사용되어 새로운 컬럼을만드는 경우 유용하다.\n",
    "\n",
    "보통 함수와 같이 함수명과 반환 값을 미리 정의해서 lambda 함수 또는 다른 함수를 사용할 수 있다.\n",
    "\n",
    "**다른 함수를 직접 사용할 수 없고, udf()를 통해서 호출**해야 한다.\n",
    "\n",
    "udf()는 **코드가 복잡한 경우에 함수를 분리해서 처리**하면 유용하겠다.\n",
    "\n",
    "먼저 DataFrame을 생성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame([('1','kim, js',170),('1','lee, sm',175), ('2','lim, yg',180), ('2','lee',170)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| _1|     _2| _3|\n",
      "+---+-------+---+\n",
      "|  1|kim, js|170|\n",
      "|  1|lee, sm|175|\n",
      "|  2|lim, yg|180|\n",
      "|  2|    lee|170|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼명 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=myDf.withColumnRenamed('_1','year').withColumnRenamed('_2','name').withColumnRenamed('_3','height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 대문자 변환 withColumn\n",
    "\n",
    "대문자로 변환하고 반환하는 함수를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uppercase(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uppercase(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "myDf는 'year', 'name', 'height' 컬럼을 가지고 있는데 그 중 'name'을 대문자로 변환해보자.\n",
    "\n",
    "앞서 정의된 함수를 바로 호출하면 AttributeError 오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m myDf \u001b[38;5;241m=\u001b[39m myDf\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnameUpper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43muppercase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn [21], line 2\u001b[0m, in \u001b[0;36muppercase\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muppercase\u001b[39m(s):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", uppercase(myDf.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "udf()를 통해 함수를 호출해야 한다.\n",
    "반환값의 타잎을 ```StringType()```으로 정의하고 있다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 udf() 함수를 withColumn()과 같이 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+---------+\n",
      "|year|   name|height|nameUpper|\n",
      "+----+-------+------+---------+\n",
      "|   1|kim, js|   170|  KIM, JS|\n",
      "|   1|lee, sm|   175|  LEE, SM|\n",
      "|   2|lim, yg|   180|  LIM, YG|\n",
      "|   2|    lee|   170|      LEE|\n",
      "+----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 Double변환 withColumn\n",
    "\n",
    "udf() 함수에 lambda를 사용해도 된다.\n",
    "**withColumn()에서 다른 함수를 호출하지 못하고, udf() 함수를 통해** 호출한다.\n",
    "\n",
    "앞서 배운 ```withColumn()```에서 udf를 호출해서 ```height``` 컬럼을 ```long```에서 ```DoubleType()```으로 만들어 보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x), DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\", toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dtypes```로 컬럼명과 데이터타입을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 'string'),\n",
       " ('name', 'string'),\n",
       " ('height', 'bigint'),\n",
       " ('nameUpper', 'string'),\n",
       " ('heightD', 'double')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 정수변환 withColumn\n",
    "\n",
    "year의 문자열을 정수로 형변환 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- nameUpper: string (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "toint=udf(lambda x:int(x), IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\", toint(myDf['year']))\n",
    "\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+---------+-------+-----+\n",
      "|year|   name|height|nameUpper|heightD|yearI|\n",
      "+----+-------+------+---------+-------+-----+\n",
      "|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|   1|lee, sm|   175|  LEE, SM|  175.0|    1|\n",
      "|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yearI 컬럼명만 선택하여 조회해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|yearI|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('yearI').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 조건에 따른 withColumn\n",
    "\n",
    "udf()함수를 사용하여, 조건에 따라 컬럼을 생성해보자.\n",
    "키를 175를 기준으로 이분화하고 반환값을 ```StringType()```으로 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+---------+-------+-----+----------+\n",
      "|year|   name|height|nameUpper|heightD|yearI|height>175|\n",
      "+----+-------+------+---------+-------+-----+----------+\n",
      "|   1|kim, js|   170|  KIM, JS|  170.0|    1|   shorter|\n",
      "|   1|lee, sm|   175|  LEE, SM|  175.0|    1|    taller|\n",
      "|   2|lim, yg|   180|  LIM, YG|  180.0|    2|    taller|\n",
      "|   2|    lee|   170|      LEE|  170.0|    2|   shorter|\n",
      "+----+-------+------+---------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 변경 withColumnRenamed\n",
    "\n",
    "앞서 ```withCoumun()``` 명령어로 열을 추가해 보았다. ```withColumnRenamed()``` 함수는 컬럼명을 변경한다. 컬럼명을 ```name```에서 ```full name```으로 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf=tDf.withColumnRenamed('id','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| ID|height|weight|\n",
      "+---+------+------+\n",
      "|  1| 65.78|112.99|\n",
      "|  2| 71.52|136.49|\n",
      "|  3|  69.4|153.03|\n",
      "+---+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 그래프\n",
    "\n",
    "#### plot\n",
    "\n",
    "Spark에는 그래프를 그리는 기능이 없다.\n",
    "Python matplotlib을 이용해서 그래프를 표현한다.\n",
    "plot() 함수 인자를 보자.\n",
    "2차원 ```plot()```을 하기 위해서는, x와 y축 값이 필요하다.\n",
    "x, y 데이터는 배열이어야 한다.\n",
    "DataFrame -> RDD로 변환하고, map() 함수를 사용하여 배열로부터 weight, height를 분리해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, y 값이 배열이고, 올바르게 추출되었는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.78 71.52 69.4  68.22 67.79]\n",
      "[112.99 136.49 153.03 142.34 144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print (np.array(_weightRdd)[:5])\n",
    "print (np.array(_heightRdd)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 추출한 height, weight RDD를 numpy array를 사용해서 2차원 x, y로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot, violinplot\n",
    "\n",
    "하나의 컬럼만 선택해서 그래프를 그려보자.\n",
    "* boxplot의 가운데 박스 IQR (Interquartile Range)은 4분위 값의 2, 3번째와 50% 값, 즉 25%~75%의 구간을 의미한다.\n",
    "그리고 위 아래 가로선은 IQR 값의 1.5배되는 구역을 나타낸다. 값의 분포와 outlier를 확인하기 편리하다.\n",
    "* violin plot 역시 boxplot과 유사한 기능이지만, 밀도를 보여주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = tDf.select(\"height\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>68.05240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.82398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>63.48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>66.94000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.86500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>69.18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.80000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         height\n",
       "count  50.00000\n",
       "mean   68.05240\n",
       "std     1.82398\n",
       "min    63.48000\n",
       "25%    66.94000\n",
       "50%    67.86500\n",
       "75%    69.18000\n",
       "max    71.80000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "botplot() 함수는 인자를 배열로 받는다. 또는 Pandas의 DataFrame도 가능하다. height가 배열로 올바르게 설정되었는지 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)  # subplot location\n",
    "ax1 = plt.boxplot(height)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2 = sns.violinplot(data=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate functions\n",
    "\n",
    "avg, collect_list, countDistinct, count, kurtosis, max, min, mean, skewness, stddev, sum, variance 등의 함수가 지원된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary 형식\n",
    "\n",
    "agg() 함수에 dictionary 형식으로 ```컬렴명: aggregate functions```으로 적어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(height)|\n",
      "+-------------+\n",
      "|           50|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    kurtosis(height)|\n",
      "+--------------------+\n",
      "|-0.00944222604387468|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"kurtosis\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(height)|\n",
      "+-----------------+\n",
      "|68.05240000000002|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(height)|\n",
      "+-----------+\n",
      "|      63.48|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "tDf.agg(F.min(\"height\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 컬럼 조회 select\n",
    "\n",
    "컬럼은 그 명칭을 사용하거나 ```myDf.name```, 인덱스 ```myDf['name']```로 선택할 수 있다.\n",
    "\n",
    "컬럼 선택 | 예제 | 권고\n",
    "-----|-----|-----\n",
    "점 연산자로 컬럼을 선택 | myDf.name | 변수명에 공백이 있거나 특수문자 또는 숫자로 시작하는 등의 경우 제한적이다.\n",
    "인덱스로 컬럼을 선택 | myDf['name'] | 인덱스를 사용해서 컬럼을 조회할 수 있다. 정수 인덱스도 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명으로 직접 show() 할 수 없다.\n",
    "\n",
    "컬럼명만 아래와 같이 적어주면, 컬럼을 지칭하게 된다.\n",
    "그러한 컬럼명으로는 show() 할 수 없다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col() 함수로 컬럼을 지정할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "F.col('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 컬럼 데이터를 조회하기 위해 컬럼에 show(), collect() 함수를 사용해서는 안된다. ```select()```를 사용해서 컬럼을 지정하여 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2fdc8d3e858e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf['name'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 조회\n",
    "\n",
    "컬럼은 ```select()```로 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|kim, js|\n",
      "|lee, sm|\n",
      "|lim, yg|\n",
      "|    lee|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 컬럼을 조회하기 위해서는 해당 컬럼을 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|lee, sm|   175|\n",
      "|lim, yg|   180|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_name=myDf.select('name', 'height').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 리스트를 넣어주어 여러 컬럼을 선택할 수도 있다.\n",
    "**리스트를 풀어야 하므로, 앞서 배웠던 ```*``` 연산자**를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|lee, sm|   175|\n",
      "|lim, yg|   180|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['name', 'height']\n",
    "myDf.select(*cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼을 List로 변환\n",
    "\n",
    "select() 함수는 Row()로 구성된 컬럼을 선택하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='kim, js'),\n",
       " Row(name='lee, sm'),\n",
       " Row(name='lim, yg'),\n",
       " Row(name='lee')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List로 변환하려면, map() 함수로는 가능하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='kim, js'),\n",
       " Row(name='lee, sm'),\n",
       " Row(name='lim, yg'),\n",
       " Row(name='lee')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 Row List를 제거하기 위해서 인덱스를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim, js', 'lee, sm', 'lim, yg', 'lee']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는  flatMap()으로 2차원 구조 Row List를 1차원 List로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim, js', 'lee, sm', 'lim, yg', 'lee']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select like\n",
    "\n",
    "```%``` 연산자는 0 또는 그 이상의 문자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------+\n",
      "|   name|height|name LIKE %lee%|\n",
      "+-------+------+---------------+\n",
      "|kim, js|   170|          false|\n",
      "|lee, sm|   175|           true|\n",
      "|lim, yg|   180|          false|\n",
      "|    lee|   170|           true|\n",
      "+-------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.like(\"%lee%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------------+\n",
      "|   name|height|startswith(name, kim)|\n",
      "+-------+------+---------------------+\n",
      "|kim, js|   170|                 true|\n",
      "|lee, sm|   175|                false|\n",
      "|lim, yg|   180|                false|\n",
      "|    lee|   170|                false|\n",
      "+-------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.startswith(\"kim\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col() 함수로 컬럼을 지정하여 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.filter(F.col('name').startswith(\"kim\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+\n",
      "|   name|height|endswith(name, lee)|\n",
      "+-------+------+-------------------+\n",
      "|kim, js|   170|              false|\n",
      "|lee, sm|   175|              false|\n",
      "|lim, yg|   180|              false|\n",
      "|    lee|   170|               true|\n",
      "+-------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.endswith(\"lee\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### alias\n",
    "\n",
    "이름을 변경할 경우 alias() 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 명칭을 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf1 = myDf.alias(\"myDf1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼의 명칭을 변경해보자.\n",
    "그러기 위해서는 우선 컬럼을 **```select()```** 함수로 골라낸 후, **```alias```**로 **컬럼명**을 정할 수 있다.\n",
    "name 컬럼을 **```substr```**으로 1,3문자를 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       kim|\n",
      "|       lee|\n",
      "|       lim|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf1.select(myDf1.name.substr(1,3).alias(\"short name\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 명령어에서 col() 함수를 사용하여 동일한 기능을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       kim|\n",
      "|       lee|\n",
      "|       lim|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf1.select(F.col(\"name\").substr(1,3).alias(\"short name\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 행과 열을 선택 select, when, otherwise\n",
    "\n",
    "when(condition, value)함수는 조건을 설정하고, 그 조건에 맞으면 value로 설정한다.\n",
    "pyspark.sql.Column.otherwise()가 같이 사용되지 않다면, 반환 값이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------+\n",
      "|height|CASE WHEN (height < 175) THEN 1 ELSE 0 END|\n",
      "+------+------------------------------------------+\n",
      "|   170|                                         1|\n",
      "|   175|                                         0|\n",
      "|   180|                                         0|\n",
      "|   170|                                         1|\n",
      "+------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", when(myDf.height < 175, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다항 조건을 설정하려면 ```when```을 겹쳐 쓰면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------------------+\n",
      "|height|CASE WHEN (height <= 170) THEN 1 WHEN ((height > 170) AND (height <= 175)) THEN 2 ELSE 0 END|\n",
      "+------+--------------------------------------------------------------------------------------------+\n",
      "|   170|                                                                                           1|\n",
      "|   175|                                                                                           2|\n",
      "|   180|                                                                                           0|\n",
      "|   170|                                                                                           1|\n",
      "+------+--------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", \n",
    "            when(myDf.height <= 170, 1)\n",
    "            .when((myDf.height > 170) & (myDf.height <= 175), 2)\n",
    "            .otherwise(0)\n",
    "           )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias 명령어로 컬럼을 변경해 줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|<175|\n",
      "+------+----+\n",
      "|   170|   1|\n",
      "|   175|   0|\n",
      "|   180|   0|\n",
      "|   170|   1|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", (when(myDf.height < 175, 1).otherwise(0)).alias('<175')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 0, 1이 아닌 문자열로 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\").otherwise(\"<175\").alias(\"how tall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn() 함수를 사용하면, DataFrame에 컬럼을 추가하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-----+---------+--------+\n",
      "|_c0|year|   name|height|heightD|yearI|nameUpper|how tall|\n",
      "+---+----+-------+------+-------+-----+---------+--------+\n",
      "|  0|   1|kim, js|   170|  170.0|    1|  KIM, JS|    <175|\n",
      "|  1|   1|lee, sm|   175|  175.0|    1|  LEE, SM|    <175|\n",
      "|  2|   2|lim, yg|   180|  180.0|    2|  LIM, YG|    >175|\n",
      "|  3|   2|    lee|   170|  170.0|    2|      LEE|    <175|\n",
      "+---+----+-------+------+-------+-----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.withColumn('how tall', when(myDf['heightD'] >175.0, \">175\").otherwise(\"<175\"))\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 행과 열을 선택  where, select\n",
    "\n",
    "\n",
    "DataFrame은 관계형데이터베이스의 테이블과 매우 유사하다. **SQL 명령**을 사용하듯이 ```where()```, ```select()```, ```groupby()``` 함수를 사용할 수 있다.\n",
    "\n",
    "* 행: ```where()```에 따라 컬럼의 **조건에 맞는 행을 선택**하고 (where는 실행하면 filter와 동일한 효과를 보인다),\n",
    "* 열: 앞서 배운 ```select()```로 열을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+\n",
      "|_c0|year|   name|height|nameUpper|\n",
      "+---+----+-------+------+---------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|\n",
      "|  3|   2|    lee|   170|      LEE|\n",
      "+---+----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "\n",
    "앞서 ```select()```는 열을 선택하는데 반면, ```filter()```는 조건에 따라 행을 선택한다.\n",
    "앞서 배웠던 ```where()```와 유사한 기능을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-----+---------+\n",
      "|_c0|year|   name|height|heightD|yearI|nameUpper|\n",
      "+---+----+-------+------+-------+-----+---------+\n",
      "|  2|   2|lim, yg|   180|  180.0|    2|  LIM, YG|\n",
      "+---+----+-------+------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.filter(myDf['height'] > 175).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col() 함수로 컬럼을 지정해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   2|lim, yg|   180|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.filter(col('height') > 175).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regexp_replace 컬럼의 내용 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+\n",
      "|_c0|year|   name|height|nameUpper|nameNew|\n",
      "+---+----+-------+------+---------+-------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|kim, js|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|lim, sm|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|lim, yg|\n",
      "|  3|   2|    lee|   170|      LEE|    lim|\n",
      "+---+----+-------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "_heightDf = myDf.withColumn('nameNew', regexp_replace('name', 'lee', 'lim'))\n",
    "_heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "컬럼을 학년에 따라,\n",
    "```groupBy()```하면 아래와 같이 데이터만 집단화하게 된다.\n",
    "집단화하면 개수를 세거나, 합계를 내거나 어떤 통계량을 계산이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f794caf5b38>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.groupby(myDf['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy하고 max\n",
    "\n",
    "컬럼을 기준으로 구분지어서 평균, 합계, 갯수, 최대, 최소 등을 구할 수 있다.\n",
    "첫 컬럼 학년을 ```groupby()```해서 최대값 ```max()```를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|max(height)|\n",
      "+----+-----------+\n",
      "|   1|        175|\n",
      "|   2|        180|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```col()``` 함수를 사용하여 컬럼을 지정하여 ```max()```를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|max(height)|\n",
      "+----+-----------+\n",
      "|   1|        175|\n",
      "|   2|        180|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(col('year')).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy, agg\n",
    "\n",
    "```agg()```는 합계 함수를 계산할 수 있으며, 지원하는 함수는 ```avg, max, min, sum, count```이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```year``` 컬럼에 대해 ```agg()``` 함수로 계산할 수 있다.\n",
    "\n",
    "dictionary 형식으로 key는 컬럼명, value는 합계 함수를 적어준다.\n",
    "예를 들어 {\"heightD\":\"avg\"}에서 \"heightD\"는 컬럼명, \"avg\"는 합계함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy 국가별 인원수\n",
    "\n",
    "월드컵 데이터를 groupBy 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|ClubCountry|count|\n",
      "+-----------+-----+\n",
      "|   England |    4|\n",
      "|   Paraguay|   93|\n",
      "|     Russia|   51|\n",
      "|        POL|   11|\n",
      "|        BRA|   27|\n",
      "|    Senegal|    1|\n",
      "|     Sweden|  154|\n",
      "|   Colombia|    1|\n",
      "|        FRA|  155|\n",
      "|        ALG|    8|\n",
      "|   England |    1|\n",
      "|       RUS |    1|\n",
      "|     Turkey|   65|\n",
      "|      Zaire|   22|\n",
      "|       Iraq|   22|\n",
      "|    Germany|  206|\n",
      "|        RSA|   16|\n",
      "|        ITA|  224|\n",
      "|        UKR|   38|\n",
      "|        GHA|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy(wcDf.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy 국가별 포지션별 인원수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+\n",
      "|ClubCountry|    |  DF|  FW|  GK|  MF|\n",
      "+-----------+----+----+----+----+----+\n",
      "|   England |null|null|   2|null|   2|\n",
      "|   Paraguay|null|  26|  37|  10|  20|\n",
      "|     Russia|null|  20|  11|   4|  16|\n",
      "|        POL|null|   2|   2|   3|   4|\n",
      "|        BRA|null|   7|   5|   4|  11|\n",
      "|    Senegal|null|null|null|   1|null|\n",
      "|     Sweden|null|  40|  47|  25|  42|\n",
      "|   Colombia|null|null|   1|null|null|\n",
      "|        ALG|null|   2|null|   6|null|\n",
      "|        FRA|null|  46|  41|  18|  50|\n",
      "|   England |null|null|null|null|   1|\n",
      "|       RUS |null|null|null|   1|null|\n",
      "|     Turkey|null|  20|  13|  12|  20|\n",
      "|      Zaire|null|   6|   5|   3|   8|\n",
      "|       Iraq|null|   6|   4|   3|   9|\n",
      "|    Germany|null|  64|  51|  16|  75|\n",
      "|        RSA|null|   5|   2|   3|   6|\n",
      "|        UKR|null|  13|   7|   4|  14|\n",
      "|        ITA|null|  74|  42|  19|  89|\n",
      "|        CMR|null|   1|   1|   1|null|\n",
      "+-----------+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy('ClubCountry').pivot('Position').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F 함수\n",
    "\n",
    "또는 아래와 같이 별도 ```pyspark.sql.functions```을 사용할 수 있다.\n",
    "앞서 pyspark.sql.functions은 함수이므로, ```from pyspark.sql.functions import split``` 이렇게 한다.\n",
    "또는 ```from pyspark.sql import functions as F```라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(heightD)|max(heightD)|avg(heightD)|sum(heightD)|\n",
      "+------------+------------+------------+------------+\n",
      "|       170.0|       180.0|      173.75|       695.0|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행 추가\n",
    "\n",
    "행을 추가하려면, DataFrame을 서로 합치는 방법으로 가능하다.\n",
    "추가할 행으로 DataFrame을 만들고, union() 함수로 합쳐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createFrame() 함수에는 리스트를 넣어주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toAppendDf = spark.createDataFrame([Row(4, 1, \"choi, js\", 177)])\n",
    "toAppendDf = spark.createDataFrame([Row(4, \"choi, js\", 177, \"CHOI, JS\", 177.0, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf = myDf.union(toAppendDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+------+\n",
      "|_c0|year|    name|height|\n",
      "+---+----+--------+------+\n",
      "|  0|   1| kim, js|   170|\n",
      "|  1|   1| lee, sm|   175|\n",
      "|  2|   2| lim, yg|   180|\n",
      "|  3|   2|     lee|   170|\n",
      "|  4|   1|choi, js|   177|\n",
      "+---+----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition\n",
    "\n",
    "repartition()은 partition의 개수를 늘리거나 줄이거나 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.repartition(4)\n",
    "print(_myDf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coalesce\n",
    "\n",
    "coalesce()는 partition을 **줄일 때** 사용한다.\n",
    "앞서 4개의 partition을 가진 ```_myDf```를 2로 줄여보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "_myDf2 = _myDf.coalesce(2)\n",
    "print(_myDf2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 통계 요약 describe\n",
    "\n",
    "column이 연산가능한 데이터타잎인 경우, 요약 값을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|            height|           heightD|             yearI|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                 4|                 4|                 4|\n",
      "|   mean|            173.75|            173.75|               1.5|\n",
      "| stddev|4.7871355387816905|4.7871355387816905|0.5773502691896257|\n",
      "|    min|               170|             170.0|                 1|\n",
      "|    max|               180|             180.0|                 2|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값\n",
    "\n",
    "결측값을 채우는 함수이다.\n",
    "* df.na.fill(0) 모든 컬럼의 na를 0으로 교체\n",
    "* df.fillna( { 'c0':0, 'c1':0 } ) 컬럼 c0, c1의 na를 0으로 교체\n",
    "\n",
    "결측값을 삭제할 수도 있다.\n",
    "* df.na.drop(subset=[\"c0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, year: int, name: string, height: int, nameUpper: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.where(F.col(\"height\").isNull())    # .count() -> this will show the number of isNull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+---------+\n",
      "|_c0|year|name|height|nameUpper|\n",
      "+---+----+----+------+---------+\n",
      "|  0|   0|   0|     0|        0|\n",
      "+---+----+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(isnan(c), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+---------+\n",
      "|_c0|year|name|height|nameUpper|\n",
      "+---+----+----+------+---------+\n",
      "|  0|   0|   0|     0|        0|\n",
      "+---+----+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(col(c).isNull(), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 년별 분기별 대여건수\n",
    "\n",
    "서울시 열린데이터광장의 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv```를 분석해보자 \n",
    "데이터는 일자별로, 대여건수이이고, 몇 줄만 출력해보면 다음과 같다.\n",
    "\n",
    "|      date| count|\n",
    "|----------|------|\n",
    "|2018-01-01|  4950|\n",
    "|2018-01-02|  7136|\n",
    "|2018-01-03|  7156|\n",
    "|2018-01-04|  7102|\n",
    "|2018-01-05|  7705|\n",
    "\n",
    "### 문제 1-1: 년도별 대여건수 합계\n",
    "데이터는 2018, 2019년 15개월 간의 대여건수이다. 년도별로 대여건수의 합계를 계산해서 출력하자.\n",
    "\n",
    "|year|sum(count)|\n",
    "|----|----------|\n",
    "|2018|  10124874|\n",
    "|2019|   1871935|\n",
    "\n",
    "\n",
    "### 문제 1-2: 년도별, 월별 대여건수 합계\n",
    "년별, 월별로 대여건수를 계산하여 합계를 계산하여 출력한다.\n",
    "\n",
    "### 문제 1-3: 년도별, 월별 대여건수 그래프\n",
    "문제 1-2의 출력을 선 그래프로 그려보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 읽기\n",
    "\n",
    "```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv``` 를 다운로드 받아서 저장한다 (https://github.com/smu405/s/blob/master/data/seoulBicycleDailyCount_2018_201903.csv)\n",
    "\n",
    " \n",
    "일자는 ```timestamp```로 건수는 ```integer```로 인식되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_bicycle = spark.read.format('csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: csv 형식으로 읽고,\n",
    "- L2: header=true로 읽고, inferschema=true로 schema는 자동 인식해서, data 폴더의 파일을 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |--  count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header=true로 읽고, header를 그대로 사용한다. 출력하면 count가 한 칸 밀려 있다 (헤더에 공백이 포함되어 있다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 건수는 455건, 5건의 데이터만 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_bicycle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date| count|\n",
      "+----------+------+\n",
      "|2018-01-01|  4950|\n",
      "|2018-01-02|  7136|\n",
      "|2018-01-03|  7156|\n",
      "|2018-01-04|  7102|\n",
      "|2018-01-05|  7705|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 변경\n",
    "\n",
    "앞서 보듯이 파일을 읽으면서 컬럼명이 인식되었는데 \" count\"가 맨 앞에 공백이 하나 있게 되어 변경해보자.\n",
    "일단 붙여진 컬럼의 명칭을 변경하려면 ```withColumnRenamed()```를 연결하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycle=_bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 만들기: substr\n",
    "\n",
    "```substr()``` 함수는 인자가 2개로서, 앞글자 '1'은 시작 '4'는 4글자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycle=bicycle.withColumn(\"year\", bicycle.Date.substr(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n",
      "|      Date|Count|year|month|\n",
      "+----------+-----+----+-----+\n",
      "|2018-01-01| 4950|2018|   01|\n",
      "|2018-01-02| 7136|2018|   01|\n",
      "|2018-01-03| 7156|2018|   01|\n",
      "|2018-01-04| 7102|2018|   01|\n",
      "|2018-01-05| 7705|2018|   01|\n",
      "+----------+-----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 만들기: F 함수\n",
    "\n",
    "함수를 이용해 년, 월, 일 등을 추출할 수 있다.\n",
    "먼저 앞서 생성된 column을 삭제하고 나서 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['year','month']\n",
    "df = bicycle.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2: 여러 컬럼을 삭제하기 위해서는 ```*```를 앞에 붙여 준다. 물론 하나씩 삭제할 수도 있고, 그러면 별표는 불필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year, month 컬럼이 삭제되고 Date, Count 컬럼만 남겨졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제공되는 함수를 이용하여 년, 월을 식별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "bicycle = bicycle\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .withColumn('month', F.month('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L3: pyspark.sql.functions의 year() 함수를 사용하여 년을 추출한다.\n",
    "- L4: pyspark.sql.functions의 month() 함수를 사용하여 월을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 substr() 결과와 비교해보자. year, month가 올바르게 추출되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n",
      "|      Date|Count|year|month|\n",
      "+----------+-----+----+-----+\n",
      "|2018-01-01| 4950|2018|   01|\n",
      "|2018-01-02| 7136|2018|   01|\n",
      "|2018-01-03| 7156|2018|   01|\n",
      "|2018-01-04| 7102|2018|   01|\n",
      "|2018-01-05| 7705|2018|   01|\n",
      "+----------+-----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show()는 앞 부분 보여주고 있다. 월이 1만 보여서, 다른 월의 결과를 보고 싶다면 filter()해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n",
      "|      Date|Count|year|month|\n",
      "+----------+-----+----+-----+\n",
      "|2018-02-01| 5821|2018|   02|\n",
      "|2018-02-02| 6557|2018|   02|\n",
      "|2018-02-03| 3499|2018|   02|\n",
      "+----------+-----+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.filter(bicycle.month == 2).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기\n",
    "\n",
    "1월 2월 3월은 1분기, 4 ~ 6은 2분기, 7 ~ 9는 3분기, 10 ~ 12는 4분기로 구분한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def classifyQuarter(s):\n",
    "    q=\"\"\n",
    "    s=int(s)\n",
    "    if 1<=s and s< 4:\n",
    "        q=\"Q1\"\n",
    "    elif 4<=s and s<7:\n",
    "        q=\"Q2\"\n",
    "    elif 7<=s and s<10:\n",
    "        q=\"Q3\"\n",
    "    elif 10<=s and s<=12:\n",
    "        q=\"Q4\"\n",
    "    else:\n",
    "        q=\"no\"\n",
    "    return q\n",
    "\n",
    "quarter_udf = udf(classifyQuarter, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L4~17: s월은 문자열 --> 정수로 형변환하고, 분기로 구분한다.\n",
    "- L18: 사용자정의 함수 udf()를 정의하고, 인자 2개 - 함수명과 반환타입을 적고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycle=bicycle.withColumn(\"quarter\", quarter_udf(bicycle.month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 분류되었는지 건수를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|quarter|count|\n",
      "+-------+-----+\n",
      "|     Q2|   91|\n",
      "|     Q1|  180|\n",
      "|     Q3|   92|\n",
      "|     Q4|   92|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+-------+\n",
      "|      Date|Count|year|month|quarter|\n",
      "+----------+-----+----+-----+-------+\n",
      "|2018-01-01| 4950|2018|    1|     Q1|\n",
      "|2018-01-02| 7136|2018|    1|     Q1|\n",
      "|2018-01-03| 7156|2018|    1|     Q1|\n",
      "|2018-01-04| 7102|2018|    1|     Q1|\n",
      "|2018-01-05| 7705|2018|    1|     Q1|\n",
      "+----------+-----+----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|year|sum(count)|\n",
      "+----+----------+\n",
      "|2019|   1871935|\n",
      "|2018|  10124874|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|quarter|sum(count)|\n",
      "+-------+----------+\n",
      "|     Q2|   2860617|\n",
      "|     Q1|   2667704|\n",
      "|     Q3|   3585513|\n",
      "|     Q4|   2882975|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|quarter|        avg(count)|\n",
      "+-------+------------------+\n",
      "|     Q2|31435.351648351647|\n",
      "|     Q1|14820.577777777778|\n",
      "|     Q3|38972.967391304344|\n",
      "|     Q4|31336.684782608696|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').agg({\"count\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별, 월별 (분기별) 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|year|    01|    02|    03|    04|    05|     06|     07|     08|     09|     10|    11|    12|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|2019|495573|471543|904819|  null|  null|   null|   null|   null|   null|   null|  null|  null|\n",
      "|2018|164367|168741|462661|687885|965609|1207123|1100015|1037505|1447993|1420621|961532|500822|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-------+-------+\n",
      "|year|     Q1|     Q2|     Q3|     Q4|\n",
      "+----+-------+-------+-------+-------+\n",
      "|2019|1871935|   null|   null|   null|\n",
      "|2018| 795769|2860617|3585513|2882975|\n",
      "+----+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "bicycleP = bicycle.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas의 info() 함수는 DataFrame의 컬럼, 데이터타입 dtypes를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 455 entries, 0 to 454\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Date     455 non-null    object\n",
      " 1   Count    455 non-null    int32 \n",
      " 2   year     455 non-null    object\n",
      " 3   month    455 non-null    object\n",
      " 4   quarter  455 non-null    object\n",
      "dtypes: int32(1), object(4)\n",
      "memory usage: 16.1+ KB\n"
     ]
    }
   ],
   "source": [
    "bicycleP.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>10124874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1871935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Count\n",
       "year          \n",
       "2018  10124874\n",
       "2019   1871935"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bicycleP.groupby('year').aggregate({'Count':np.sum})\n",
    "bicycleP.groupby('year').aggregate({'Count':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별, 월별 대여건수 합계\n",
    "\n",
    "index는 행, columns는 열 데이터를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>month</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>164367.0</td>\n",
       "      <td>168741.0</td>\n",
       "      <td>462661.0</td>\n",
       "      <td>687885.0</td>\n",
       "      <td>965609.0</td>\n",
       "      <td>1207123.0</td>\n",
       "      <td>1100015.0</td>\n",
       "      <td>1037505.0</td>\n",
       "      <td>1447993.0</td>\n",
       "      <td>1420621.0</td>\n",
       "      <td>961532.0</td>\n",
       "      <td>500822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>495573.0</td>\n",
       "      <td>471543.0</td>\n",
       "      <td>904819.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "month        01        02        03        04        05         06         07  \\\n",
       "year                                                                            \n",
       "2018   164367.0  168741.0  462661.0  687885.0  965609.0  1207123.0  1100015.0   \n",
       "2019   495573.0  471543.0  904819.0       NaN       NaN        NaN        NaN   \n",
       "\n",
       "month         08         09         10        11        12  \n",
       "year                                                        \n",
       "2018   1037505.0  1447993.0  1420621.0  961532.0  500822.0  \n",
       "2019         NaN        NaN        NaN       NaN       NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(bicycleP, values = 'Count', index = ['year'], columns = ['month'], aggfunc= 'sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018년만 선택해서 년도별 x 분기별 대여건수를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycleP2018=bicycleP[bicycleP['year']==2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycleP2018byQ = pd.pivot_table(bicycleP2018, values = 'Count', index = ['year'], columns = ['quarter'], aggfunc= 'sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iloc은 정수 인덱스의 위치, ```[:, 0:4]```는 즉 모든 행, 컬럼은 0~4까지 (4는 제외) 데이터를 조회한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicycleP2018byQ.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년별 월별 대여건수 그래프\n",
    "\n",
    "앞장 RDD에서 만들어진 단어빈도는 리스트에 저장되었다. 따라서 리스트에서 데이터를 추출하여 그래프를 그렸다.\n",
    "groupBy에서 생성된 월별 대여건수는 pandas로 변환하여 그려보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 년별 월별 대여건수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumMonthly=bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf=sumMonthly.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>495573</td>\n",
       "      <td>471543</td>\n",
       "      <td>904819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>164367</td>\n",
       "      <td>168741</td>\n",
       "      <td>462661</td>\n",
       "      <td>687885.0</td>\n",
       "      <td>965609.0</td>\n",
       "      <td>1207123.0</td>\n",
       "      <td>1100015.0</td>\n",
       "      <td>1037505.0</td>\n",
       "      <td>1447993.0</td>\n",
       "      <td>1420621.0</td>\n",
       "      <td>961532.0</td>\n",
       "      <td>500822.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year      01      02      03        04        05         06         07  \\\n",
       "0  2019  495573  471543  904819       NaN       NaN        NaN        NaN   \n",
       "1  2018  164367  168741  462661  687885.0  965609.0  1207123.0  1100015.0   \n",
       "\n",
       "          08         09         10        11        12  \n",
       "0        NaN        NaN        NaN       NaN       NaN  \n",
       "1  1037505.0  1447993.0  1420621.0  961532.0  500822.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "위 데이터에서 'year' 컬럼이 없어야 그래프를 그릴 수 있다.\n",
    "* drop() 명령어에 삭제할 컬럼명 'year'와 1을 적어준다.\n",
    "0은 행 (index), 1은 컬럼을 삭제한다는 의미이다.\n",
    "* transpose() 함수를 통해 열로 변환하여 (행 데이터는 plot을 할 수 없다), 그래프를 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsl\\AppData\\Local\\Temp\\ipykernel_8128\\3921547174.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  my=pdf.drop('year', 1).transpose()\n"
     ]
    }
   ],
   "source": [
    "my=pdf.drop('year', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 pdf를 변환한 my를 출력하면, 명령어가 적용되어 year 컬럼이 삭제되고, transpose되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>495573.0</td>\n",
       "      <td>164367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>471543.0</td>\n",
       "      <td>168741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>904819.0</td>\n",
       "      <td>462661.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>687885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>965609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1207123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1100015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1037505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1447993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1420621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>961532.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>500822.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1\n",
       "01  495573.0   164367.0\n",
       "02  471543.0   168741.0\n",
       "03  904819.0   462661.0\n",
       "04       NaN   687885.0\n",
       "05       NaN   965609.0\n",
       "06       NaN  1207123.0\n",
       "07       NaN  1100015.0\n",
       "08       NaN  1037505.0\n",
       "09       NaN  1447993.0\n",
       "10       NaN  1420621.0\n",
       "11       NaN   961532.0\n",
       "12       NaN   500822.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plot()``` 함수는 컬럼을 적지 않으면 모든 컬럼에 대해서 plot한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "my.columns=[2018, 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x132e5980b50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 일괄 실행\n",
    "\n",
    "공공자전거 대여건수를 분석하는 프로그램을 일괄 실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_bicycle.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_bicycle.py\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def classifyQuarter(s):\n",
    "    q=\"\"\n",
    "    s=int(s)\n",
    "    if 1<=s and s< 4:\n",
    "        q=\"Q1\"\n",
    "    elif 4<=s and s<7:\n",
    "        q=\"Q2\"\n",
    "    elif 7<=s and s<10:\n",
    "        q=\"Q3\"\n",
    "    elif 10<=s and s<=12:\n",
    "        q=\"Q4\"\n",
    "    else:\n",
    "        q=\"no\"\n",
    "    return q\n",
    "\n",
    "def doIt():\n",
    "    # create dataframe\n",
    "\t_bicycle = spark.read.format('com.databricks.spark.csv')\\\n",
    "\t\t.options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')\n",
    "    # columns Date, Count\n",
    "\tbicycle=_bicycle\\\n",
    "\t\t.withColumnRenamed(\"date\", \"Date\")\\\n",
    "\t\t.withColumnRenamed(\" count\", \"Count\")\n",
    "\t#bicycle=bicycle.withColumn(\"year\", bicycle.Date.substr(1, 4))\n",
    "\t#bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))\n",
    "\tbicycle = bicycle\\\n",
    "\t\t.withColumn('year', F.year('date'))\\\n",
    "\t\t.withColumn('month', F.month('date'))\n",
    "\n",
    "    # column quarter\n",
    "\tquarter_udf = udf(classifyQuarter, StringType())\n",
    "\tbicycle=bicycle.withColumn(\"quarter\", quarter_udf(bicycle.month))\n",
    "    \n",
    "    # groupBy\n",
    "\tbicycle.groupBy('quarter').count().show()\n",
    "\tbicycle.groupBy('year').agg({\"count\":\"sum\"}).show()\n",
    "\tbicycle.groupBy('quarter').agg({\"count\":\"sum\"}).show()\n",
    "\tbicycle.groupBy('quarter').agg({\"count\":\"avg\"}).show()\n",
    "\tbicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"}).show()\n",
    "\tbicycle.groupBy('year').pivot('quarter').agg({\"count\":\"sum\"}).show()\n",
    "\tsumMonthly=bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"})\n",
    "    \n",
    "    # graph\n",
    "\tpdf=sumMonthly.toPandas()\n",
    "\tmy=pdf.drop('year', axis=1).transpose()\n",
    "\tmy.columns=[2018, 2019]\n",
    "\tmy.plot(kind='line')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python3 src/ds_spark_bicycle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!spark-submit src/ds_spark_bicycle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 Spark SQL\n",
    "\n",
    "\n",
    "\n",
    "관계형 데이터베이스 RDB에서 사용하는 Sql을 사용하여 DataFrame으로부터 데이터를 조회할 수 있다. DataFrame과 달리, RDD는 비구조적인 경우에 사용하므로 테이블로 변환한 후 Sql을 사용하게 된다.\n",
    "\n",
    "* Spark SQL 구성\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "Language API | Python, Java, Scala, Hive QL API를 제공\n",
    "Schema RDD | RDD에 Schema를 적용해 임시 테이블로 변환한다.<br>createOrReplaceTempView<br>createGlobalTempView\n",
    "Data Sources | 다양한 형식 지원 - HDFS, Cassandra, HBase, RDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 만들어 놓은 World Cup 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이제 임시 테이블 ```wc```를 만들고, Sql문으로 데이터를 조회해보자.\n",
    "\n",
    "데이터프레임을 임시 뷰(temporary view)로 등록\n",
    "createOrReplaceTempView 메서드를 사용하는 것이 좋습니다. 이렇게 하면 데이터프레임을 임시 뷰로 등록할 수 있으며, 임시 뷰의 이름이 이미 존재하는 경우 해당 뷰를 덮어쓰게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----+\n",
      "|    FullName|                Club|     Team|Year|\n",
      "+------------+--------------------+---------+----+\n",
      "|Ãngel Bossio|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+------------+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")\n",
    "wcPlayers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='mySubway', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='wc', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```wcPlayers```를 RDD로 변환해서 이름만 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sql.functions and join\n",
    "\n",
    "리스트에 포함되어 있는 과일에 고유번호를 할당해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```truncate```는 행의 값을 잘라내지 않고 출력한다.\n",
    "```show(bucketDf.count(), truncate=False)```는 모든 행을 완전하게 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketDf.show(bucketDf.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* explode\n",
    "\n",
    "컬럼에 List 또는 배열이 포함된 경우 ```explode()``` 함수는 이를 flat해서 새로운 컬럼을 생성하게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId, explode(bucketDf.items).alias('item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또 다른 DataFrame을 생성해보자. 나중에 앞의 DataFrame과 join하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|          |    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* join\n",
    "\n",
    "join은 ```inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti``` 여러 종류가 있다. ```inner```기준으로 item이 일치하지 않는 것은 제외하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|itemId|      item|bucketId|\n",
      "+------+----------+--------+\n",
      "|    F5|   bananas|       2|\n",
      "|    F1|    orange|       1|\n",
      "|    F3| pineapple|       1|\n",
      "|    F4|watermelon|       2|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "\n",
    "### 문제\n",
    "\n",
    "네트워크에 불법적으로 침입하는 시도는 허용되어서는 안된다.\n",
    "1998년 MIT Lincoln Labs에서 DARPA Intrusion Detection Evaluation Program을 연구하였다.\n",
    "이 데이터의 일부가 1999년 KDD로 만들어져 배포되고 있다.\n",
    "https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "\n",
    "### 해결\n",
    "\n",
    "마지막 행에 attack의 유형이 구분되어 있다. 네트워크 침입 유형의 특징을 분석해 보자.\n",
    "탐지예방 모델을 구축할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KDD데이터는 41 항목으로 구성되어 있다.\n",
    "\n",
    "```python\n",
    "연결(초) | duration: continuous.\n",
    "프로토콜 (tcp,udp,etc) | protocol_type: symbolic.\n",
    "서비스 (http,telnet, etc) | service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous|.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일 내려받기\n",
    "\n",
    "KDD 파일은 **gz** 압축되어 있다. 파일 확장자 'gz'은 'gzip'이라는 압축 도구에서 생성된 파일이다. 지금은 WinZip에서 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 로컬 디렉토리 ```data```에 존재하면, 즉 이미 내려받았으므로 또 내려받지 않는다. 그렇지 않을 경우에만 ```urlretrieve()``` 함수로 내려받는다. 오류가 발생하면, (1) 파일이 없거나, (2) 파일을 모두 내려 받지 않았거나, (3) 파일이 깨져있을 수 있다. 내려받은 디렉토리로 가서 그 파일이 존재하는지, winzip같은 유틸리티로 해당 gz을 풀어보고 확인하든지, 적당한 에디터로 해당 파일에 내용이 있는지 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if(not os.path.exists(_fname)):\n",
    "    print (\"{} data does not exist! retrieving..\".format(_fname))\n",
    "    _f=urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "\n",
    "**RDD**는 gz와 같은 **압축파일에서 데이터를 읽어서** 생성할 수 있다.\n",
    "\n",
    "반면, DataFrame은 구조schema를 정의해야 하기 때문에 쉽지 않다. 여기서는 **오류**가 발생한다.\n",
    "따라서 RDD를 생성하고 난 후, 그로부터 DataFrame을 생성하고, Sql을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```textFile()``` 함수로 RDD를 생성한다. ```count()```는 행의 수를 돌려주는 action 함수이다. action 함수는 바로 실행되므로 시간이 좀 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```map()``` 함수를 사용하여 csv 형식으로 구성된 파일을 컴마(,)로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_allRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '181',\n",
       "  '5450',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8',\n",
       "  '8',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '9',\n",
       "  '9',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.11',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_allRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정상, 공격 건수\n",
    "\n",
    "데이터가 ```normal```인 경우와 아닌 경우로 구분하자.\n",
    "```filter()```는 41번째 행을 조건에 따라 데이터를 구분한다.\n",
    "```count()``` 함수로 건수를 계산하면 'normal' 97,278, 'attack'은 396,743 건이다.\n",
    "\n",
    "침입구분 | 건수\n",
    "-------|-------\n",
    "normal | 97278\n",
    "attack | 396743\n",
    "전체 | 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_normalRdd=_allRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_allRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97278"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_normalRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396743"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack별 건수\n",
    "\n",
    "**attack 종류**는 41번째 열에 구분되어 있다. 총 494,021건을 정상 'noraml'과 나머지는 'attack'으로 구분한다.\n",
    "'attack'은 크게 4종류로 나눈다. DOS는 서비스 거부, R2L 원격침입, U2R은 루트권한침입, probing은 탐지이다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | 서비스 거부 공격 denial-of-service, 과도한 트래픽을 발생하여 시스템을 서비스 거부 상태로 만든다 | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | Remote to Local의 약자, 원격에서 로컬로 침입을 시도하는 공격 | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | User to Root의 약자, 관리자 권한을 얻기 위해 침입하는 공격 | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | 보안 취약점을 탐색하는 사전 단계 | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "열41에 대해 건수를 세어보자.\n",
    "```reduceByKey()```는 인자로 '함수'가 필요. 키별로 '함수를 사용해서' 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 97278),\n",
       " ('buffer_overflow.', 30),\n",
       " ('loadmodule.', 9),\n",
       " ('perl.', 3),\n",
       " ('neptune.', 107201),\n",
       " ('smurf.', 280790),\n",
       " ('guess_passwd.', 53),\n",
       " ('pod.', 264),\n",
       " ('teardrop.', 979),\n",
       " ('portsweep.', 1040),\n",
       " ('ipsweep.', 1247),\n",
       " ('land.', 21),\n",
       " ('ftp_write.', 8),\n",
       " ('back.', 2203),\n",
       " ('imap.', 12),\n",
       " ('satan.', 1589),\n",
       " ('phf.', 4),\n",
       " ('nmap.', 231),\n",
       " ('multihop.', 7),\n",
       " ('warezmaster.', 20),\n",
       " ('warezclient.', 1020),\n",
       " ('spy.', 2),\n",
       " ('rootkit.', 10)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "_41.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```groupByKey()```는 키별로 group한다. 위 ```reduceByKey()```와 달리 ```mapValues()```를 사용해 값을 별도로 계산한다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "def f(x): return len(x)\n",
    "_41.groupByKey().mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe 생성\n",
    "\n",
    "열 0, 1, 2, 3, 4, 5, 41을 선별하여 스키마를 정해서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5]),\n",
    "        attack=p[41]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD를 Dataframe으로 변환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_df=spark.createDataFrame(_csvRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "| attack|dst_bytes|duration|flag|protocol|service|src_bytes|\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "|normal.|     5450|       0|  SF|     tcp|   http|      181|\n",
      "|normal.|      486|       0|  SF|     tcp|   http|      239|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      235|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      219|\n",
      "|normal.|     2032|       0|  SF|     tcp|   http|      217|\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack 분류\n",
    "\n",
    "네트워크 침입이 'attack' 또는 'normal'에 따라 구분해서 ```attackB``` 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "attack_udf = udf(lambda x: \"normal\" if x ==\"normal.\" else \"attack\", StringType())\n",
    "myDf=_df.withColumn(\"attackB\", attack_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "네트워크 침입 attack을 세분화하여 normal, dos, r2l, u2r, probling으로 **5종류**로 구분한다.\n",
    "구분 문자열이 **점('.')**으로 끝난다는 점에 주의하다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 표에 따라 ```udf()``` 함수를 사용해서 if문으로 'noraml' 및 'attack'을 총 5가지 종류로 구분한다.\n",
    "반환 값은 ```StringType()```이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def classify41(s):\n",
    "    _5=\"\"\n",
    "    if s==\"normal.\":\n",
    "        _5=\"normal\"\n",
    "    elif s==\"back.\" or s==\"land.\" or s==\"neptune.\" or s==\"pod.\" or s==\"smurf.\" or s==\"teardrop.\":\n",
    "        _5=\"dos\"\n",
    "    elif s==\"ftp_write.\" or s==\"guess_passwd.\" or s==\"imap.\" or s==\"multihop.\" or s==\"phf.\" or\\\n",
    "        s==\"spy.\" or s==\"warezclient.\" or s==\"warezmaster.\":\n",
    "        _5=\"r2l\"\n",
    "    elif s==\"buffer_overflow.\" or s==\"loadmodule.\" or s==\"perl.\" or s==\"rootkit.\":\n",
    "        _5=\"u2r\"\n",
    "    elif s==\"ipsweep.\" or s==\"nmap.\" or s==\"portsweep.\" or s==\"satan.\":\n",
    "        _5=\"probing\"\n",
    "    return _5\n",
    "\n",
    "attack5_udf = udf(classify41, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf=myDf.withColumn(\"attack5\", attack5_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      " |-- attack5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "잘 분류되었는지 일부 데이터를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+----+--------+-------+---------+-------+-------+\n",
      "| attack|dst_bytes|duration|flag|protocol|service|src_bytes|attackB|attack5|\n",
      "+-------+---------+--------+----+--------+-------+---------+-------+-------+\n",
      "|normal.|     5450|       0|  SF|     tcp|   http|      181| normal| normal|\n",
      "|normal.|      486|       0|  SF|     tcp|   http|      239| normal| normal|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      235| normal| normal|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      219| normal| normal|\n",
      "|normal.|     2032|       0|  SF|     tcp|   http|      217| normal| normal|\n",
      "+-------+---------+--------+----+--------+-------+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack, normal 특징 분석\n",
    "\n",
    "```attack5``` 별로 건수를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|attack5| count|\n",
      "+-------+------+\n",
      "|probing|  4107|\n",
      "|    u2r|    52|\n",
      "| normal| 97278|\n",
      "|    r2l|  1126|\n",
      "|    dos|391458|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "protocol은 (1) TCP (Transmission Control Protocol, 연결지향 프로토콜), (2) ICMP (Internet Control Message Protocol, 네트워크에서 제어 및 오류 메시지를 전달하는 데 사용되는 프로토콜), (3) UDP (User Datagram Protocol, 데이터그램(데이터 패킷)을 전달하는 데 사용되는 프로토콜)로 구분되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     tcp|190065|\n",
      "|     udp| 20354|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|attackB|protocol| count|\n",
      "+-------+--------+------+\n",
      "| normal|     udp| 19177|\n",
      "| normal|    icmp|  1288|\n",
      "| normal|     tcp| 76813|\n",
      "| attack|    icmp|282314|\n",
      "| attack|     tcp|113252|\n",
      "| attack|     udp|  1177|\n",
      "+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB','protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|attackB|  icmp|   tcp|  udp|\n",
      "+-------+------+------+-----+\n",
      "| normal|  1288| 76813|19177|\n",
      "| attack|282314|113252| 1177|\n",
      "+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB').pivot('protocol').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```attack5``` 별로 공격의 특징을 분석해보자. 어떤 ```protocol```, ```src_bytes```, ```duration```이 어떤지 계산할 수 있다.\n",
    "\n",
    "- src_bytes 출발지 (Source, 송신자)가 목적지(destination, 수신자)에게 보낸 데이터의 양\n",
    "- dst_bytes 수신자가 송신자로부터 받은 데이터의 양\n",
    "- duration 연결이 시작된 후 종료될 때까지의 시간 (초 또는 밀리초)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|attack5|              icmp|               tcp|               udp|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|probing|10.700793650793651| 261454.6003016591|25.235897435897435|\n",
      "|    u2r|              null| 960.8979591836735|13.333333333333334|\n",
      "| normal| 91.47049689440993|1439.3120305156679| 98.01220211711947|\n",
      "|    r2l|              null|271972.57460035523|              null|\n",
      "|    dos| 936.2672084368129| 1090.303422435458|              28.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').pivot('protocol').avg('src_bytes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|attack5|       avg(duration)|\n",
      "+-------+--------------------+\n",
      "|probing|   485.0299488677867|\n",
      "|    u2r|    80.9423076923077|\n",
      "| normal|  216.65732231336992|\n",
      "|    r2l|   559.7522202486679|\n",
      "|    dos|7.254929008986916E-4|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').avg('duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+---+\n",
      "|attackB|icmp|    tcp|udp|\n",
      "+-------+----+-------+---+\n",
      "| normal|   0|5134218|516|\n",
      "| attack|   0|5155468| 74|\n",
      "+-------+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.groupBy('attackB').pivot('protocol').agg(F.max('dst_bytes')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "좀 더 세밀한 조건으로 ```duration>1000)```, ```dst_bytes==0```인 경우의 건수를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|protocol|count|\n",
      "+--------+-----+\n",
      "|     tcp|  139|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "SQL을 사용해보자. 위에 사용했던 ```_df```에서 임시 테이블 ```_tab```을 생성한다.\n",
    "\n",
    "DataFrame을 Spark의 임시 테이블로 등록하고, sql문을 사용할 수 있다.\n",
    " \n",
    "- createOrReplaceTempView() 함수를 사용한다. registerTempTable() 함수는 2.0부터 더 이상 지원하지 않는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#_df.registerTempTable(\"_tab\")\n",
    "\n",
    "_df.createOrReplaceTempView(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcp_interactions = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions.rdd\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print (ti_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: Twitter JSON 데이터 읽기\n",
    "\n",
    "트윗 데이터에는 사용자가 작성한 글이 있는데, 이 부분만 추출하여 같이 발생하는 단어 또는 단어 빈도를 분석한다.\n",
    "\n",
    "트윗의 text 필드만 추출해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "json 파일을 읽어서 DataFrame을 생성해보자. \n",
    "아래와 같이 ```read.json()``` 하거나 또는 ```read.load.format(\"json\").load()``` 이라고 해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2df= spark.read.json(os.path.join(\"src\",\"ds_twitter_seoul_3.json\"))\n",
    "#t2df= spark.read.json(os.path.join(\"src\",\"ds_twitter_seoul_3.json\"), allowBackslashEscapingAnyCharacter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: Twitter JSON을 읽을 경우, 탈출문자가 혹시 존재하면 allowBackslashEscapingAnyCharacter=True로 설정한다.\n",
    "\n",
    "구분 | 예\n",
    "-------|-------\n",
    "unicode를 사용하면 backslash | \"{\\\"created_at\\\":\\\"Sun Nov 13 00:05:19 +0000 2016\\\"\n",
    "보통 | {\"created_at\":\"Sun Nov 13 00:05:19 +0000 2016\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "트윗의 'id','lang','text' 컬럼만을 선택해서 한 줄을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res=t2df.select('id','lang','text').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801657325836763136 en RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \n",
      "https://t.co/1XRSaRBbE0 https://t.co/fi…\n"
     ]
    }
   ],
   "source": [
    "for e in res:\n",
    "    print (e['id'],e['lang'],e['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitterDF= spark.read.json(os.path.join(\"src\",\"ds_twitter_1_noquote.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: boolean (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: boolean (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.select('text').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL 작업을 하기 위해서 임시 테이블(뷰)를 등록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#twitterDF.registerTempTable(\"twitter\")\n",
    "twitterDF.createOrReplaceTempView(\"twitter\")\n",
    "spark.sql(\"select text from twitter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "\n",
    "### 뉴욕에서 출생한 신생아가 년도별 성별에 차이가 있을까?\n",
    "\n",
    "뉴욕에서 2007년 출생한 유아의 기록이다.\n",
    "https://health.data.ny.gov/Health/Baby-Names-Beginning-2007/jxy9-yhdk\n",
    "\n",
    "Column Name | 설명\n",
    "-----|-----\n",
    "Year | Year data was collected.\n",
    "First Name | 이름\n",
    "County | Location where the baby’s mother resided as stated on their birth certificate.\n",
    "Sex | F= Female M= Male\n",
    "Count | Five (5) or more of the same baby name in a county outside of NYC; Ten (10) or more of the same baby name in a NYC borough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://catalog.data.gov/dataset\n",
    "\n",
    "data/dataGovbabyNames.json 은 메타데이터가 있어서 kaggle.com의 baby names를 사용해서 분석?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```requests.get()``` 함수를 사용해서 url로부터 데이터를 읽어 오면 string이다 (예: ```r.iter_lines()```하면 문자 1개씩 가져옴). response를 json으로 읽으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "_url=\"https://health.data.ny.gov/api/views/jxy9-yhdk/rows.json?accessType=DOWNLOAD\"\n",
    "_json=requests.get(_url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* json데이터는 meta, data로 구분해서 만들어져 있다.\n",
    "    * data는 Python List로 구성되어 있다 (앞서 Python dict에서 생성하는 경우와 비교해 본다.)\n",
    "    * data의 건수는 52,252건\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['meta', 'data'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baby Names: Beginning 2007'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json['meta']['view']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87899\n"
     ]
    }
   ],
   "source": [
    "_jsonList=_json['data']\n",
    "print (len(_jsonList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_jsonList=_json['data']\n",
    "print len(_jsonList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row-emfw_sfk5_5wtx',\n",
       " '00000000-0000-0000-3154-4394D27F2559',\n",
       " 0,\n",
       " 1682529128,\n",
       " None,\n",
       " 1682529128,\n",
       " None,\n",
       " '{ }',\n",
       " '2007',\n",
       " 'ZOEY',\n",
       " 'KINGS',\n",
       " 'F',\n",
       " '11']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list로부터 Spark Dataframe을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "_schema=StructType([\n",
    "    StructField(\"_1\", StringType(), True),\n",
    "    StructField(\"_2\", StringType(), True),\n",
    "    StructField(\"_3\", LongType(), True),\n",
    "    StructField(\"_4\", LongType(), True),\n",
    "    StructField(\"_5\", StringType(), True),\n",
    "    StructField(\"_6\", LongType(), True),\n",
    "    StructField(\"_7\", StringType(), True),\n",
    "    StructField(\"_8\", StringType(), True),\n",
    "    StructField(\"_9\", StringType(), True),\n",
    "    StructField(\"_10\", StringType(), True),\n",
    "    StructField(\"_11\", StringType(), True),\n",
    "    StructField(\"_12\", StringType(), True),\n",
    "    StructField(\"_13\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=spark.createDataFrame(_json['data'], schema=_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87899"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* schema를 정하지 않았으므로 임의로 생성된 속성을 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 컬럼명을 새로 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_myDf = _df.select('_9','_10','_11','_12','_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+----+-----+-------+---+-----+\n",
      "|year|fname| county|sex|count|\n",
      "+----+-----+-------+---+-----+\n",
      "|2007| ZOEY|  KINGS|  F|   11|\n",
      "|2007| ZOEY|SUFFOLK|  F|    6|\n",
      "|2007| ZOEY| MONROE|  F|    6|\n",
      "|2007| ZOEY|   ERIE|  F|    9|\n",
      "|2007|  ZOE| ULSTER|  F|    5|\n",
      "+----+-----+-------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf=_myDf.withColumn('count',_df['_13'].cast(\"integer\")).drop('_13')\n",
    "_myDf=_myDf.withColumnRenamed('_9','year')\n",
    "_myDf=_myDf.withColumnRenamed('_10','fname')\n",
    "_myDf=_myDf.withColumnRenamed('_11','county')\n",
    "_myDf=_myDf.withColumnRenamed('_12','sex')\n",
    "_myDf.printSchema()\n",
    "_myDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---+-----+\n",
      "|year|fname|    county|sex|count|\n",
      "+----+-----+----------+---+-----+\n",
      "|2007|GAVIN|CHAUTAUQUA|  M|    6|\n",
      "|2007|GAVIN|    ALBANY|  M|    9|\n",
      "+----+-----+----------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.filter(_myDf['fname'] == u'GAVIN').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Sql을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| fname|\n",
      "+------+\n",
      "|  JADE|\n",
      "|HUNTER|\n",
      "|  ANNA|\n",
      "|ANJALI|\n",
      "|  LEIB|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#_myDf.registerTempTable(\"babyNames\")\n",
    "_myDf.createOrReplaceTempView(\"babyNames\")\n",
    "spark.sql(\"select distinct(fname) from babyNames\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 년도별 성별 빈도수를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|year|   F|   M|\n",
      "+----+----+----+\n",
      "|2016|2843|3287|\n",
      "|2012|2872|3292|\n",
      "|2020|2557|3004|\n",
      "|2019|2739|3144|\n",
      "|2017|2786|3233|\n",
      "|2014|4121|4241|\n",
      "|2013|2836|3322|\n",
      "|2009|2917|3395|\n",
      "|2018|2750|3206|\n",
      "|2011|2918|3298|\n",
      "|2008|3039|3442|\n",
      "|2007|3002|3365|\n",
      "|2015|2857|3241|\n",
      "|2010|2925|3267|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.sort('year').groupBy('year').pivot('sex').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 우버 택시의 운행기록 분석\n",
    "\n",
    "가장 많은 운행을 한 차량기지(base)를 찾아보자 (참고: https://github.com/tmcgrath/spark-with-python-course/blob/master/Spark-SQL-CSV-with-Python.ipynb)\n",
    "\n",
    "앞서 서울시 열린데이터광장의 오픈데이터와 같은 뉴욕시 Open Data에서 제공하는 우버 운행기록이 있고, API를 사용해서 가져올 수 있다 (\"https://data.cityofnewyork.us/resource/gre9-vvjv.json\").\n",
    "\n",
    "API를 사용하기 위해서는 인증키를 받아야 하므로, 우버의 일간 운행기록 2015년 1,2월 데이터를 내려받아서 분석한다 (fivethirtyeight git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git)\n",
    "\n",
    "데이터는 차량기지, 일자, 차량, 운행으로 구성되어 있다.\n",
    "\n",
    "```\n",
    "차량기지 dispatching_base_number | 일자 date | 차량 active_vehicles | 운행 trips\n",
    "----------|----------|----------|----------\n",
    "B02512 | 1/1/2015 | 190 | 1132\n",
    "B02765 | 1/1/2015 | 225 | 1765\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_home=os.path.join(os.getcwd(),\"data\")\n",
    "uberCsv=os.path.join(data_home,\"Uber-Jan-Feb-FOIL.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2: 읽을 폴더를 지정한다.\n",
    "- L3: 내려받은 파일을 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: dispatching_base_number,date,active_vehicles,trips\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "_rdd = spark.sparkContext.textFile(uberCsv)\n",
    "\n",
    "header = _rdd.first() #extract header\n",
    "print (f\"Header: {header}\")\n",
    "\n",
    "_rdd = _rdd.filter(lambda x:x != header)\n",
    "print (_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: 파일을 읽어 RDD를 생성한다.\n",
    "- L3~4: 첫 줄은 header, 속성 명 dispatching_base_number,date,active_vehicles,trips을 출력한다.\n",
    "- L6: header를 제외하고 읽는다.\n",
    "- L7: rdd 데이터 개수를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD 데이터를 추출해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B02512', 'B02765', 'B02764', 'B02682', 'B02617', 'B02598']\n"
     ]
    }
   ],
   "source": [
    "_myRdd = _rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "_row0keys=_myRdd.map(lambda row: row[0]).distinct().collect()\n",
    "\n",
    "print (_row0keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1: csv는 컴마로 분리된 형식이므로, ','로 항목들을 분리\n",
    "- L3: 첫번째 열에서 key값을 추출한다 (header값 포함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "B02512인 경우, trips가 2000보다 큰 경우를 찾아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.filter(lambda row: \"B02512\" in row).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B02512', '1/30/2015', '256', '2016'],\n",
       " ['B02512', '2/5/2015', '264', '2022'],\n",
       " ['B02512', '2/12/2015', '269', '2092'],\n",
       " ['B02512', '2/13/2015', '281', '2408'],\n",
       " ['B02512', '2/14/2015', '236', '2055'],\n",
       " ['B02512', '2/19/2015', '250', '2120'],\n",
       " ['B02512', '2/20/2015', '272', '2380'],\n",
       " ['B02512', '2/21/2015', '238', '2149'],\n",
       " ['B02512', '2/27/2015', '272', '2056']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.filter(lambda row: \"B02512\" in row).filter(lambda row: int(row[3])>2000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 차량기지(dispatching_base_number)별로 운행기록(trips)의 합계를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B02512', 93786),\n",
       " ('B02765', 193670),\n",
       " ('B02764', 1914449),\n",
       " ('B02682', 662509),\n",
       " ('B02617', 725025),\n",
       " ('B02598', 540791)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.map(lambda x: (x[0], int(x[3]))).reduceByKey(lambda k,v: k + v).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "spark.createDataFrame()으로 읽으면, header가 ```_1, _2, _3, _4```로 주어진다.\n",
    "read() 함수의 option을 설정해서 header를 컬럼명으로 읽도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "uberDf = spark\\\n",
    "            .read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(uberCsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uberDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+---------------+-----+\n",
      "|dispatching_base_number|    date|active_vehicles|trips|\n",
      "+-----------------------+--------+---------------+-----+\n",
      "|                 B02512|1/1/2015|            190| 1132|\n",
      "|                 B02765|1/1/2015|            225| 1765|\n",
      "|                 B02764|1/1/2015|           3427|29421|\n",
      "+-----------------------+--------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uberDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- active_vehicles: string (nullable = true)\n",
      " |-- trips: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uberDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trips는 현재 문자열로 되어 있어서, 정수로 형변환을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_myDf=uberDf.withColumn('iTrips', uberDf['trips'].cast(\"integer\")).drop('trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_myDf=_myDf.withColumnRenamed('dispatching_base_number','baseNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- baseNum: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- active_vehicles: string (nullable = true)\n",
      " |-- iTrips: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+------+\n",
      "|baseNum|    date|active_vehicles|iTrips|\n",
      "+-------+--------+---------------+------+\n",
      "| B02512|1/1/2015|            190|  1132|\n",
      "| B02765|1/1/2015|            225|  1765|\n",
      "| B02764|1/1/2015|           3427| 29421|\n",
      "| B02682|1/1/2015|            945|  7679|\n",
      "| B02617|1/1/2015|           1228|  9537|\n",
      "+-------+--------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 차량기지(dispatching_base_number)별로 운행기록(trips)의 합계를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|baseNum|sum(iTrips)|\n",
      "+-------+-----------+\n",
      "| B02512|      93786|\n",
      "| B02598|     540791|\n",
      "| B02682|     662509|\n",
      "| B02765|     193670|\n",
      "| B02617|     725025|\n",
      "| B02764|    1914449|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#_myDf.groupBy('baseNum').sum('iTrips').show()\n",
    "_myDf.groupBy('baseNum').agg({\"iTrips\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "\n",
    "Spark는 자바를 통한 데이터베이스 연결 JDBC (Java DataBase Connectivity)를 지원한다.\n",
    "\n",
    "여기서는 매우 간단한 데이터베이스인 sqlite를 실습해보자.\n",
    "\n",
    "## SQLite 설치\n",
    "\n",
    "SQLite는 사용자가 설치할 필요 없이 소스 코드를 직접 빌드하거나 다운로드 받아 사용할 수 있다.\n",
    "\n",
    "https://www.sqlite.org 에서 운영체제에 맞는 압축파일을 다운로드 받아서 풀면된다. \n",
    "\n",
    "* 단말에서 실행하는 sqlite3가 필요하다. 단말이 불편하면 GUI 버전의 sqlstudio를 사용한다.\n",
    "* 필요한 버전의 파일 sqlite-jdbc-버전.jar을 내려받아 (https://github.com/xerial/sqlite-jdbc), classpath에 추가한다. \n",
    "- 설정하기 위해 ```.set(\"spark.driver.extraClassPath\", jar 파일)``` 이렇게 jar 라이브러리를 추가한다 (또는 설정파일 conf/spark-defaults.conf에 'spark.driver.extraClassPath'를 추가해도 된다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "myConf=pyspark.SparkConf().set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0,com.databricks:spark-csv_2.11:1.5.0\")\\\n",
    "                        .set(\"spark.driver.extraClassPath\", os.path.join(os.getcwd(),\"lib\",\"sqlite-jdbc-3.41.0.1.jar\"))\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설정을 넣고 spark를 생성하고 나면, 비로서 jar 파일을 사용할 수 있게 classpath에 추가된다. 출력되지 않으면 설정이 올바르게 되지 않았다는 의미다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jsl\\\\Code\\\\201711111\\\\lib\\\\sqlite-jdbc-3.41.0.1.jar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.driver.extraClassPath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON 데이터를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "info = json.loads( '''[\n",
    "    { \"id\" : \"001\", \"x\" : \"2\", \"name\" : \"Chuck\"},\n",
    "    { \"id\" : \"009\", \"x\" : \"7\", \"name\" : \"Brent\" }\n",
    "]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '001', 'x': '2', 'name': 'Chuck'},\n",
       " {'id': '009', 'x': '7', 'name': 'Brent'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON 문자열을 바로 DataFrame으로 올릴 수 없다. 우선 JSON 리스트 -> rdd -> DataFrame으로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf= spark.read.json(spark.sparkContext.parallelize(info)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|  x|\n",
      "+---+-----+---+\n",
      "|001|Chuck|  2|\n",
      "|009|Brent|  7|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jdbc를 연결하는 방식은 Java와 같이 'driver', 'url'을 설정하면 된다.\n",
    "\n",
    "```\n",
    ".format(\"jdbc\")\\\n",
    ".option(\"url\", connection_str)\\ # 연결 url을 적어준다. 형식은 jdbc:subprotocol:subname\n",
    ".option(\"dbtable\", table)\\\n",
    ".option(\"user\", username)\\\n",
    ".option(\"password\", password)\\\n",
    ".option(\"driver\", \"org.sqlite.JDBC\")\\\n",
    ".option(\"numPartitions\", num_partitions)\\\n",
    ".save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode는:\n",
    "\n",
    "- 'overwrite' 테이블을 덮어 쓰거나,\n",
    "- 'append' 테이블이 존재하지 않으면 생성하지만, 있으면 붙여넣는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf.write.format('jdbc')\\\n",
    "    .options(\n",
    "        url=\"jdbc:sqlite:\"+os.path.join(os.getcwd(),\"customer1.db\"),\n",
    "        dbtable=\"customers\",\n",
    "        driver=\"org.sqlite.JDBC\"\n",
    "    ).mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:sqlite:C:\\Users\\jsl\\Code\\201711111\\customer.db\n"
     ]
    }
   ],
   "source": [
    "print(\"jdbc:sqlite:\"+os.path.join(os.getcwd(),\"customer.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('jdbc')\\\n",
    "    .options(\n",
    "        url=\"jdbc:sqlite:\"+os.path.join(os.getcwd(),\"customer1.db\"),\n",
    "        dbtable=\"customers\",\n",
    "        driver=\"org.sqlite.JDBC\"\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|  x|\n",
      "+---+-----+---+\n",
      "|001|Chuck|  2|\n",
      "|009|Brent|  7|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MySql\n",
    "\n",
    "* Spark 패키지를 제공하지 않는다. jar를 'spark.driver.extraClassPath'에 추가하고 사용한다.\n",
    "\n",
    "* 읽기\n",
    "```python\n",
    "dfmysql = sqlContext.read.format('jdbc')\\\n",
    "        .options(\n",
    "          url='jdbc:mysql://localhost/database_name',\n",
    "          driver='com.mysql.jdbc.Driver',\n",
    "          dbtable='SourceTableName',\n",
    "          user='your_user_name',\n",
    "          password='your_password')\\\n",
    "        .load()\n",
    "```\n",
    "\n",
    "* 쓰기\n",
    "```python\n",
    "destination_df.write.format('jdbc')\\\n",
    "        .options(\n",
    "          url='jdbc:mysql://localhost/database_name',\n",
    "          driver='com.mysql.jdbc.Driver',\n",
    "          dbtable='DestinationTableName',\n",
    "          user='your_user_name',\n",
    "          password='your_password')\\\n",
    "        .mode('append')\\\n",
    "        .save()\n",
    "```\n",
    "\n",
    "```python\n",
    "bin/spark-submit --jars mysql-connector-java-5.1.40-bin.jar\n",
    "      /path_to_your_program/spark_database.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 MongoDB Spark connector\n",
    "\n",
    "앞서 pymongo를 사용하여 MongoDB에 연결하였다.\n",
    "\n",
    "또 다른 방법은 Spark에서 직접 다른 라이브러리를 사용하지 않고 MongoDB를 연결할 수 있다.\n",
    "\n",
    "그러면 MongoDB가 원격 서버가 되고, Spark가 클라이언트가 되게 된다.\n",
    "\n",
    "Spark에서 MongoDB에 연결하기 위해서는 반드시 라이브러리를 통해야 하므로, 이를 쓸 수 있도록 설정해 주어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.1 설정\n",
    "\n",
    "필요한 설정은 내부적으로 해도, 즉 spark 객체를 만들기 전 설정을 해줄 수 있다.\n",
    "\n",
    "또는 완전설치하고 나면 설치된 conf 디렉토리의 ```conf/spark-defaults.conf``` 설정파일을 수정해도 된다. 이 방식은 완전설치가 아니면 conf 디렉토리가 없으니, 수정할 파일도 없어서 해당하지 않는다.\n",
    "\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내부적으로 설정하기 위해 'spark.jars.packages'에 MongoDB Spark connector를 추가하고, 다음에 유의한다.\n",
    "\n",
    "* Spark 버전에 맞는 jar를 선택한다. 여러 버전을 시도하여 보니 2.12:10.2.0가 맞게 작동한다 (최신버전 2.13@10.0.2, 2.13@10.2.0 오류가 발생한다)\n",
    "* MongoDB<3.2인 경우, spark.mongodb.input.partitioner가 필요하다.\n",
    "* packages 여러 개를 넣을 경우에는 컴마로 분리한다.\n",
    "\n",
    "sparkSession을 생성한 후, 그러니까 spark를 생성하고 나서 \"spark.jars.packages\"를 설정하려고 하면 효과가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```winutils.exe```를 설정하지 않으면 오류가 발생하고 있다. 그래서 코드를 추가하고 재실행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME']=os.getcwd()\n",
    "os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf().set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\") \n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "커널을 재시작한다.\n",
    "[I 2023-10-18 05:50:01.254 ServerApp] Restoring connection for 22e13429-b47f-463f-9899-18837522a092:919ba119-189f-4689-b132-c2303671a507\n",
    ":: loading settings :: url = jar:file:/C:/Users/jsl/AppData/Roaming/Python/Python39/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
    "Ivy Default Cache set to: C:\\Users\\jsl\\.ivy2\\cache\n",
    "The jars for the packages stored in: C:\\Users\\jsl\\.ivy2\\jars\n",
    "org.mongodb.spark#mongo-spark-connector_2.13 added as a dependency\n",
    ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2a0c8e79-2a3c-4338-a9d6-fbd373635a84;1.0\n",
    "        confs: [default]\n",
    "        found org.mongodb.spark#mongo-spark-connector_2.13;10.1.1 in central\n",
    "        found org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
    "        [4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
    "        found org.mongodb#bson;4.8.2 in central\n",
    "        found org.mongodb#mongodb-driver-core;4.8.2 in central\n",
    "        found org.mongodb#bson-record-codec;4.8.2 in central\n",
    "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.13/10.1.1/mongo-spark-connector_2.13-10.1.1.jar ...\n",
    "        [SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.13;10.1.1!mongo-spark-connector_2.13.jar (653ms)\n",
    "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar ...\n",
    "        [SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.8.2!mongodb-driver-sync.jar (443ms)\n",
    "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar ...\n",
    "        [SUCCESSFUL ] org.mongodb#bson;4.8.2!bson.jar (604ms)\n",
    "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar ...\n",
    "        [SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.8.2!mongodb-driver-core.jar (499ms)\n",
    "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar ...\n",
    "        [SUCCESSFUL ] org.mongodb#bson-record-codec;4.8.2!bson-record-codec.jar (295ms)\n",
    ":: resolution report :: resolve 7884ms :: artifacts dl 2517ms\n",
    "        :: modules in use:\n",
    "        org.mongodb#bson;4.8.2 from central in [default]\n",
    "        org.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
    "        org.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
    "        org.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
    "        org.mongodb.spark#mongo-spark-connector_2.13;10.1.1 from central in [default]\n",
    "        ---------------------------------------------------------------------\n",
    "        |                  |            modules            ||   artifacts   |\n",
    "        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
    "        ---------------------------------------------------------------------\n",
    "        |      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
    "        ---------------------------------------------------------------------\n",
    ":: retrieving :: org.apache.spark#spark-submit-parent-2a0c8e79-2a3c-4338-a9d6-fbd373635a84\n",
    "        confs: [default]\n",
    "        5 artifacts copied, 0 already retrieved (2354kB/73ms)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'spark.jars.packages' 설정을 출력하면 포함된 것을 확인할 수 있다.\n",
    "\n",
    "전체 설정을 보려면 ```spark.sparkContext.getConf().getAll()``` 명령을 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'org.mongodb.spark:mongo-spark-connector_2.12:10.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### S.7.2 uri\n",
    "\n",
    "동일한 URI를 사용하고자 하는 경우, 연결을 위한 단일 URI로 사용할 수 있다. 단일 URI를 사용할 때에는 읽기 및 쓰기 작업을 모두 지원해야 합니다.\n",
    "\n",
    "또는 input.uri (MongoDB -> DataFrame으로 읽을 때 사용하는 URI), output.uri (DataFrame -> MongoDB에 쓸 때 사용하는 URI)를 별도로 설정할 수 있다.\n",
    "\n",
    "```\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .getOrCreate()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### S.7.3 MongoDB Python API\n",
    "\n",
    "MongoDB에 데이터에 대하여 CRUD (입력, 수정, 삭제, 조회)의 작업을 할 수 있다.\n",
    "\n",
    "* 저장할 때는 .write(), 읽을 때는 .read()를 사용한다.\n",
    "* mode()는 \"overwrite\", \"append\", \"error\" 또는 \"errorifexists\" (중복 입력의 오류 발생), \"ignore\" (무시)\n",
    "* format()의 인자는 \"mongodb\"를 적는다 (\"mongo\"라고 하면 오류). Mongo-Spark connector에 대한 전체 경로 \"com.mongodb.spark.sql.DefaultSource\"를 적을 수 있다.\n",
    "* 'option'을 사용해서 실행시점에 Database, Colleciton 명을 설정할 수 있다.\n",
    "\n",
    "구분 | 명령어 예\n",
    "-----|-----\n",
    "쓰기 | DataFrame.write.format(\"mongodb\")\\<br>.mode(\"overwrite\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\<br>.save()\n",
    "읽기 | spark.read.format(\"mongodb\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\<br>.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.4 연습으로 쓰기, 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb+srv://jslimit:smu405@cluster0.v0bapbt.mongodb.net/?retryWrites=true&w=majority\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src import mylib\n",
    "import pymongo\n",
    "\n",
    "keyPath=os.path.join(os.getcwd(), 'src', 'key.properties')\n",
    "key=mylib.getKey(keyPath)\n",
    "#uriLocal='mongodb://localhost:27017'  # 로컬에 연결하기 위한 uri\n",
    "uriCloud='mongodb+srv://jslimit:'+key['mongo']+'@cluster0.v0bapbt.mongodb.net/?retryWrites=true&w=majority' # DB를 생략해도 된다.\n",
    "print(uriCloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 열린데이터과장의 지하철 관련 데이터를 Mongodb에 저장해 놓은 바 있는데, 접속해서 읽어보자.\n",
    "(데이터베이스는 ds_open_subwayPassengersDb, 테이블은 db_open_subwayTable)\n",
    "\n",
    "설정\n",
    "\n",
    "- uri\n",
    "- database\n",
    "- collection\n",
    "- partitioner: 기본은 MongoDefaultPartitioner (MongoSamplePartitioner는 MongoDB 3.2 이상이 필요하다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf = spark.createDataFrame([(\"kim\",10),(\"lee\",20),(\"choi\",30),(\"park\",40)],[\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| kim| 10|\n",
      "| lee| 20|\n",
      "|choi| 30|\n",
      "|park| 40|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf.write.format(\"mongodb\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"connection.uri\", uriCloud)\\\n",
    "    .option(\"database\", \"myDB\")\\\n",
    "    .option(\"collection\", \"myDfCol\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "쓴 내용을 다시 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDf2=spark.read.format(\"mongodb\")\\\n",
    "    .option(\"connection.uri\", uriCloud)\\\n",
    "    .option(\"database\", \"myDB\")\\\n",
    "    .option(\"collection\", \"myDfCol\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+\n",
      "|                 _id|age|name|\n",
      "+--------------------+---+----+\n",
      "|65319e7bbd0ad213c...| 30|choi|\n",
      "|65319e7bbd0ad213c...| 10| kim|\n",
      "|65319e7bbd0ad213c...| 20| lee|\n",
      "|65319e7bbd0ad213c...| 40|park|\n",
      "+--------------------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf3=spark.read.format(\"mongodb\")\\\n",
    "    .option(\"connection.uri\", uriCloud)\\\n",
    "    .option(\"database\", \"myDB\")\\\n",
    "    .option(\"collection\", \"myPyCol\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-------+---+----+\n",
      "|                 _id|age|country| id|name|\n",
      "+--------------------+---+-------+---+----+\n",
      "|64fd3d7255f6d9c33...| 11|     ko|  1|  js|\n",
      "|64fd7b2b55f6d9c33...| 11|     ko|  1|  js|\n",
      "|64fd7b3855f6d9c33...| 22|     ko|  1|  js|\n",
      "+--------------------+---+-------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장할 때 데이터타잎을 정의하지 않았더라도, 아래에서 보듯이 적절하게 설정되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|  js|\n",
      "|  js|\n",
      "|  js|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf3.select('name').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.7.5 MongoDB에 저장한 열린데이터\n",
    "\n",
    "* MongoDB에 저장된 데이터 읽기\n",
    "\n",
    "구분 | 명\n",
    "-----|-----\n",
    "Database | ds_open_subwayPassengersDb\n",
    "Collection | db_open_subwayTable\n",
    "key | JSON 계층구조를 따라 읽는다. CardSubwayStatisticsService.row.RIDE_PASGR_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* MongoDB shell\n",
    "\n",
    "```python\n",
    "$ mongo\n",
    "> use ds_open_subwayPassengersDb\n",
    "switched to db ds_rest_subwayPassengers_mongo_db\n",
    "> show tables\n",
    "db_open_subwayTable\n",
    "system.indexes\n",
    "> db.db_open_subwayTable.find().limit(1)\n",
    "{ \"_id\" : ObjectId(\"57fa386ff5e6e94359c033e9\"), \"CardSubwayStatisticsService\" : { \"row\" : [ { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 111275, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"용문\", \"ALIGHT_PASGR_NUM\" : 108878, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 11495, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"원덕\", \"ALIGHT_PASGR_NUM\" : 10964, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 118103, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"양평\", \"ALIGHT_PASGR_NUM\" : 116604, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 10590, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"오빈\", \"ALIGHT_PASGR_NUM\" : 10020, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 26304, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"아신\", \"ALIGHT_PASGR_NUM\" : 26358, \"USE_MON\" : \"201306\" } ], \"RESULT\" : { \"MESSAGE\" : \"정상 처리되었습니다\", \"CODE\" : \"INFO-000\" }, \"list_total_count\" : 530 } }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_open_subwayPassengersDb.db_open_subwayTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subwayDf=spark.read.format(\"mongodb\")\\\n",
    "    .option(\"connection.uri\", uriCloud)\\\n",
    "    .option(\"database\", \"ds_open_subwayPassengersDb\")\\\n",
    "    .option(\"collection\", \"db_open_subwayTable\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CardSubwayTime: struct (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- FOUR_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- FOUR_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- FIVE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- FIVE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- SIX_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- SIX_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- SEVEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- SEVEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- EIGHT_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- EIGHT_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- NINE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- NINE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- ELEVEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- ELEVEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWELVE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWELVE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- THIRTEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- THIRTEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- FOURTEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- FOURTEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- FIFTEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- FIFTEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- SIXTEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- SIXTEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- SEVENTEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- SEVENTEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- EIGHTEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- EIGHTEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- NINETEEN_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- NINETEEN_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_ONE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_ONE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_TWO_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_TWO_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_THREE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWENTY_THREE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- MIDNIGHT_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- MIDNIGHT_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- ONE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- ONE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWO_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- TWO_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- THREE_RIDE_NUM: double (nullable = true)\n",
      " |    |    |    |-- THREE_ALIGHT_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |-- _id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subwayDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|                  LINE_NUM|\n",
      "+--------------------------+\n",
      "|[1호선, 1호선, 1호선, 1...|\n",
      "|[1호선, 1호선, 1호선, 1...|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subwayDf.select('CardSubwayTime.row.LINE_NUM').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------------------------------+--------------------+--------------------+\n",
      "|                  LINE_NUM|                    SUB_STA_NM|       FOUR_RIDE_NUM|     FOUR_ALIGHT_NUM|\n",
      "+--------------------------+------------------------------+--------------------+--------------------+\n",
      "|[1호선, 1호선, 1호선, 1...|[서울역, 동묘앞, 시청, 종각...|[654.0, 51.0, 37....|[17.0, 1.0, 0.0, ...|\n",
      "|[1호선, 1호선, 1호선, 1...| [종로5가, 동대문, 신설동, ...|[71.0, 715.0, 343...|[0.0, 14.0, 3.0, ...|\n",
      "+--------------------------+------------------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subwayDf.select('CardSubwayTime.row.LINE_NUM', 'CardSubwayTime.row.SUB_STA_NM',\n",
    "                'CardSubwayTime.row.FOUR_RIDE_NUM', 'CardSubwayTime.row.FOUR_ALIGHT_NUM')\\\n",
    "        .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       FOUR_RIDE_NUM|\n",
      "+--------------------+\n",
      "|[654.0, 51.0, 37....|\n",
      "|[71.0, 715.0, 343...|\n",
      "+--------------------+\n",
      "\n",
      "None\n",
      "Row(FOUR_RIDE_NUM=[654.0, 51.0, 37.0, 127.0, 191.0])\n",
      "Row(FOUR_RIDE_NUM=[654.0, 51.0, 37.0, 127.0, 191.0])\n"
     ]
    }
   ],
   "source": [
    "subwayDf.createOrReplaceTempView(\"mySubway\")\n",
    "myTab = spark.sql(\"SELECT CardSubwayTime.row.FOUR_RIDE_NUM FROM mySubway\")\n",
    "#print type(myTab)\n",
    "print (myTab.show())\n",
    "print (myTab.first())\n",
    "print (myTab.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## S.8 spark-submit\n",
    "\n",
    "* spark-submit는 일괄실행 (self-contained app in quick-start 참조)\n",
    "\n",
    "* MongoDB를 사용하려면, spark-defaults.conf에 jar를 추가한다 (앞서 미리 설정하였다.)\n",
    "\n",
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```python\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.8.1 간단한 작업\n",
    "\n",
    "* DataFrame 만들고, 출력하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_sql.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_sql.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    d = [{'name': 'Alice', 'age': 1}]\n",
    "    print(spark.createDataFrame(d).collect())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[Row(age=1, name='Alice')]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!spark-submit src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```[Row(age=1, name='Alice')]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.2 MongoDB spark-submit\n",
    "\n",
    "* Database, Collection 읽기, 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_mongo.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    import mylib\n",
    "    # uriCloud\n",
    "    keyPath=os.path.join(os.getcwd(), 'src', 'key.properties')\n",
    "    key=mylib.getKey(keyPath)\n",
    "    uriCloud='mongodb+srv://jslimit:'+key['mongo']+'@cluster0.v0bapbt.mongodb.net/?retryWrites=true&w=majority' # DB를 생략해도 된다.\n",
    "    #print(uriCloud)\n",
    "    \n",
    "    print(\"---------RESULT-----------\")\n",
    "    print(\"------mongodb write-------\")\n",
    "    myRdd = spark.sparkContext.parallelize([\n",
    "        (\"js\", 150),\n",
    "        (\"Gandalf\", 1000),\n",
    "        (\"Thorin\", 195),\n",
    "        (\"Balin\", 178),\n",
    "        (\"Kili\", 77),\n",
    "        (\"Dwalin\", 169),\n",
    "        (\"Oin\", 167),\n",
    "        (\"Gloin\", 158),\n",
    "        (\"Fili\", 82),\n",
    "        (\"Bombur\", None)\n",
    "    ])\n",
    "    myDf = spark.createDataFrame(myRdd, [\"name\", \"age\"])\n",
    "    print(myDf)\n",
    "    myDf.write.format(\"mongodb\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"connection.uri\", uriCloud)\\\n",
    "        .option(\"database\", \"myDB\")\\\n",
    "        .option(\"collection\", \"ds_spark_df_mongo\")\\\n",
    "        .save()\n",
    "\n",
    "    print(\"---------read-----------\")\n",
    "    df = spark.read.format(\"mongodb\")\\\n",
    "        .option(\"connection.uri\", uriCloud)\\\n",
    "        .option(\"database\", \"myDB\")\\\n",
    "        .option(\"collection\", \"ds_spark_df_mongo\")\\\n",
    "        .load()\n",
    "    print(df.printSchema())\n",
    "    df.createOrReplaceTempView(\"myTable\")\n",
    "    myTab = spark.sql(\"SELECT name, age FROM myTable WHERE age >= 100\")\n",
    "    myTab.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #os.environ['HADOOP_HOME']=os.getcwd()\n",
    "    #os.environ[\"PATH\"] += os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "\n",
    "    myConf=pyspark.SparkConf().set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\")\\\n",
    "                                .set(\"log4j.logger.org.apache.spark.util.ShutdownHookManager\", \"OFF\")\\\n",
    "                                .set(\"log4j.logger.org.apache.spark.SparkEnv\", \"ERROR\")\n",
    "    spark = pyspark.sql.SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel('ERROR')\n",
    "    #spark = pyspark.sql.SparkSession.builder\\\n",
    "    #    .master(\"local\")\\\n",
    "    #    .appName(\"myApp\")\\\n",
    "    #    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    #    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    #    .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작업이 끝나고 정리하면서 임시파일을 지우면서 오류가 발생하고 있다. 다음 설정을 해서, 로그를 출력하지 않도록 할 수 있다.\n",
    "\n",
    "```\n",
    "SparkConf().set(log4j.logger.org.apache.spark.util.ShutdownHookManager=OFF)\n",
    "log4j.logger.org.apache.spark.SparkEnv=ERROR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Warning: Ignoring non-Spark config property: log4j.logger.org.apache.spark.SparkEnv 설정이 되지 않고 있다.\n",
    "Warning: Ignoring non-Spark config property: log4j.logger.org.apache.spark.util.ShutdownHookManager\n",
    "---------RESULT-----------\n",
    "------mongodb write-------\n",
    "DataFrame[name: string, age: bigint]\n",
    "---------read-----------\n",
    "root\n",
    " |-- _id: string (nullable = true)\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "\n",
    "None\n",
    "+-------+----+\n",
    "|   name| age|\n",
    "+-------+----+\n",
    "|     js| 150|\n",
    "|Gandalf|1000|\n",
    "| Thorin| 195|\n",
    "|  Balin| 178|\n",
    "| Dwalin| 169|\n",
    "|    Oin| 167|\n",
    "|  Gloin| 158|\n",
    "+-------+----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "!spark-submit  --packages org.mongodb.spark:mongo-spark-connector_2.12:10.2.0  src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---------RESULT-----------\n",
    "------mongodb write-------\n",
    "DataFrame[name: string, age: bigint]\n",
    "---------read-----------\n",
    "root\n",
    " |-- _id: string (nullable = true)\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "\n",
    "None\n",
    "+-------+----+\n",
    "|   name| age|\n",
    "+-------+----+\n",
    "|     js| 150|\n",
    "|Gandalf|1000|\n",
    "| Thorin| 195|\n",
    "|  Balin| 178|\n",
    "| Dwalin| 169|\n",
    "|    Oin| 167|\n",
    "|  Gloin| 158|\n",
    "+-------+----+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
